{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Jt_5WwtQtz_i"},"outputs":[],"source":["import os\n","import os.path\n","import sys\n","import h5py\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Flatten, Dense, Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, AveragePooling1D, BatchNormalization, Activation, Add, add\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.applications.imagenet_utils import decode_predictions\n","from tensorflow.keras.applications.imagenet_utils import preprocess_input\n","from tensorflow.keras.optimizers import RMSprop, Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZJv1frDtwvL"},"outputs":[],"source":["gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    # Currently, memory growth needs to be the same across GPUs\n","    for gpu in gpus:\n","      tf.config.experimental.set_memory_growth(gpu, True)\n","  except RuntimeError as e:\n","    # Memory growth must be set before GPUs have been initialized\n","    print(e)"]},{"cell_type":"markdown","metadata":{"id":"N7MqOUPNt40r"},"source":["# ASCAD data loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WS_ErnMKt-F8"},"outputs":[],"source":["ascad_database = \"/content/drive/MyDrive/ASCAD_databases/ASCAD.h5\"\n","RANDOM_STATE = 42\n","VAL_SIZE = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGabjC-pTr31"},"outputs":[],"source":["def check_file_exists(file_path):\n","\tfile_path = os.path.normpath(file_path)\n","\tif os.path.exists(file_path) == False:\n","\t\tprint(\"Error: provided file path '%s' does not exist!\" % file_path)\n","\t\tsys.exit(-1)\n","\treturn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9PBNfd_t7CM"},"outputs":[],"source":["def load_ascad(ascad_database_file, load_metadata=False):\n","\tcheck_file_exists(ascad_database_file)\n","\t# Open the ASCAD database HDF5 for reading\n","\ttry:\n","\t\tin_file\t = h5py.File(ascad_database_file, \"r\")\n","\texcept:\n","\t\tprint(\"Error: can't open HDF5 file '%s' for reading (it might be malformed) ...\" % ascad_database_file)\n","\t\tsys.exit(-1)\n","\t# Load profiling traces\n","\tX_profiling = np.array(in_file['Profiling_traces/traces'], dtype=np.int8)\n","\t# Load profiling labels\n","\tY_profiling = np.array(in_file['Profiling_traces/labels'])\n","\t# Load attacking traces\n","\tX_attack = np.array(in_file['Attack_traces/traces'], dtype=np.int8)\n","\t# Load attacking labels\n","\tY_attack = np.array(in_file['Attack_traces/labels'])\n","\tif load_metadata == False:\n","\t\treturn (X_profiling, Y_profiling), (X_attack, Y_attack)\n","\telse:\n","\t\treturn (X_profiling, Y_profiling), (X_attack, Y_attack), (in_file['Profiling_traces/metadata'], in_file['Attack_traces/metadata'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtsH0LItuA3i"},"outputs":[],"source":["def create_tf_dataset(X, y, batch_size=64, shuffle=True):\n","  ds = tf.data.Dataset.from_tensor_slices((X, y))\n","  if shuffle:\n","    ds = ds.shuffle(buffer_size=len(X))\n","  ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","  return ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9C1bOWXqT7l6"},"outputs":[],"source":["# Tuning\n","from sklearn.model_selection import train_test_split\n","\n","(X_profiling, Y_profiling), (X_attack, Y_attack) = load_ascad(ascad_database)\n","\n","X_train, X_val, Y_train, Y_val = train_test_split(\n","    X_profiling, Y_profiling, test_size=VAL_SIZE, random_state=RANDOM_STATE\n",")\n","\n","Y_train_cat = to_categorical(Y_train, num_classes=256)\n","Y_val_cat = to_categorical(Y_val, num_classes=256)\n","\n","X_train = X_train.reshape((-1, 700, 1))\n","X_val = X_val.reshape((-1, 700, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RK-4GoEYuCsS"},"outputs":[],"source":["# Full training\n","from sklearn.model_selection import train_test_split\n","\n","(X_profiling, Y_profiling), (X_attack, Y_attack) = load_ascad(ascad_database)\n","\n","X_train_full = X_profiling\n","Y_train_cat = to_categorical(Y_profiling, num_classes=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OlpQI23nTuux"},"outputs":[],"source":["print(X_train.shape)\n","print(Y_train_cat.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKXRao8wuG1G"},"outputs":[],"source":["# Full train\n","ds_train = create_tf_dataset(X_train_full, Y_train_cat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A49P08lPUETU"},"outputs":[],"source":["# Tuning\n","ds_train = create_tf_dataset(X_train, Y_train_cat)\n","ds_val = create_tf_dataset(X_val, Y_val_cat, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1747287536434,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"emeCFkPxc9ED","outputId":"b0ddbf7f-1830-4858-a2f1-c5bb267f85fc"},"outputs":[{"data":{"text/plain":["(45000, 256)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["Y_train_cat.shape"]},{"cell_type":"markdown","metadata":{"id":"nmzu6MEzjA0E"},"source":["# Transformer baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oK6XKARrlJ7h"},"outputs":[],"source":["!pip install"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDFDlFByr0y2"},"outputs":[],"source":["from tensorflow.keras.layers import (Input, Dense, LayerNormalization, Dropout,\n","                                     MultiHeadAttention, GlobalAveragePooling1D,\n","                                     Embedding, Add, Conv1D, Reshape)\n","from tensorflow.keras.models import Model\n","\n","def transformer_sca_baseline(\n","    input_length=700,\n","    num_classes=256,\n","    d_model=128,\n","    num_heads=4,\n","    ff_dim=256,\n","    dropout=0.1,\n","    num_blocks=4):\n","\n","  inputs = Input(shape=(input_length,1))\n","\n","  x = Conv1D(d_model, kernel_size=5, padding='same', activation='relu')(inputs)\n","\n","  positions = tf.range(start=0, limit=input_length, delta=1)\n","  pos = Embedding(input_dim=700, output_dim=x.shape[-1])(positions)\n","  x = x + pos\n","\n","  for _ in range(num_blocks):\n","    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(x, x)\n","    attn_output = Dropout(dropout)(attn_output)\n","\n","    x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n","\n","    ffn_output = Dense(ff_dim, activation='relu')(x)\n","    ffn_output = Dense(d_model)(ffn_output)\n","    ffn_output = Dropout(dropout)(ffn_output)\n","\n","    x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n","\n","  x = GlobalAveragePooling1D()(x)\n","  outputs = Dense(num_classes, activation='softmax')(x)\n","\n","  model = Model(inputs=inputs, outputs=outputs, name=\"transformer_sca_baseline\")\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1747257077256,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"yd3mSkwElkwj","outputId":"5462de9d-1747-43d5-eda8-6330f6e5903e"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_sca_baseline\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"transformer_sca_baseline\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n","│                     │                   │            │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalizat… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalizat… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalizat… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_28          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ global_average_p… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m768\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_14 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_15 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n","│                     │                   │            │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_14 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,896\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_16 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ layer_normalizat… │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_17 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_16 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_17 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,896\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_18 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ layer_normalizat… │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_19 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_19 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,896\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_20 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ layer_normalizat… │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_28          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_21 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_20 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ layer_normalizat… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_21 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,896\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_22 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m700\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_22 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m33,024\u001b[0m │ global_average_p… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">563,712</span> (2.15 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m563,712\u001b[0m (2.15 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">563,712</span> (2.15 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m563,712\u001b[0m (2.15 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["tfmr_sca_base = transformer_sca_baseline()\n","tfmr_sca_base.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","tfmr_sca_base.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1367657,"status":"ok","timestamp":1747258746316,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"u4TW8pfBofEc","outputId":"3cd5237f-d751-4a50-ef5e-81953133383a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0042 - loss: 5.5519 - val_accuracy: 0.0044 - val_loss: 5.5551\n","Epoch 2/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0044 - loss: 5.5677 - val_accuracy: 0.0048 - val_loss: 5.5548\n","Epoch 3/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0039 - loss: 5.5551 - val_accuracy: 0.0044 - val_loss: 5.5496\n","Epoch 4/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0040 - loss: 5.5491 - val_accuracy: 0.0044 - val_loss: 5.5473\n","Epoch 5/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0037 - loss: 5.5462 - val_accuracy: 0.0036 - val_loss: 5.5477\n","Epoch 6/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0043 - loss: 5.5458 - val_accuracy: 0.0044 - val_loss: 5.5458\n","Epoch 7/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0043 - loss: 5.5447 - val_accuracy: 0.0044 - val_loss: 5.5461\n","Epoch 8/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0051 - loss: 5.5449 - val_accuracy: 0.0030 - val_loss: 5.5490\n","Epoch 9/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0048 - loss: 5.5448 - val_accuracy: 0.0042 - val_loss: 5.5459\n","Epoch 10/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0044 - loss: 5.5443 - val_accuracy: 0.0044 - val_loss: 5.5477\n","Epoch 11/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0046 - loss: 5.5442 - val_accuracy: 0.0044 - val_loss: 5.5466\n"]}],"source":["early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","tfmr_sca_base.fit(ds_train, batch_size=64, epochs=75, validation_data=ds_val, callbacks=[early_stop])\n","\n","tfmr_sca_base.save(\"/content/drive/MyDrive/trfmr_baseline.keras\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":807,"status":"ok","timestamp":1747260292493,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"6TFdDifm0v65","outputId":"d7d36c87-546c-4df7-e468-b3015fbcef8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","preds = tfmr_sca_base.predict(X_val[:100])\n","for pred in preds:\n","  plt.plot(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8886671,"status":"ok","timestamp":1747269937780,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"tuKjciQy12fp","outputId":"6cdffb0e-d7f0-4c23-ff06-70a4a7855276"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 205ms/step - accuracy: 0.0040 - loss: 5.5859 - val_accuracy: 0.0032 - val_loss: 5.5562\n","Epoch 2/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0041 - loss: 5.5580 - val_accuracy: 0.0044 - val_loss: 5.5556\n","Epoch 3/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5495 - val_accuracy: 0.0046 - val_loss: 5.5463\n","Epoch 4/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5478 - val_accuracy: 0.0044 - val_loss: 5.5471\n","Epoch 5/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0053 - loss: 5.5465 - val_accuracy: 0.0044 - val_loss: 5.5493\n","Epoch 6/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0048 - loss: 5.5465 - val_accuracy: 0.0046 - val_loss: 5.5483\n","Epoch 7/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0042 - loss: 5.5466 - val_accuracy: 0.0046 - val_loss: 5.5467\n","Epoch 8/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0043 - loss: 5.5461 - val_accuracy: 0.0042 - val_loss: 5.5468\n","Epoch 9/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0045 - loss: 5.5448 - val_accuracy: 0.0040 - val_loss: 5.5476\n","Epoch 10/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5450 - val_accuracy: 0.0044 - val_loss: 5.5489\n","Epoch 11/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0048 - loss: 5.5527 - val_accuracy: 0.0038 - val_loss: 5.5472\n","Epoch 12/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0045 - loss: 5.5454 - val_accuracy: 0.0036 - val_loss: 5.5460\n","Epoch 13/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0042 - loss: 5.5452 - val_accuracy: 0.0044 - val_loss: 5.5454\n","Epoch 14/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0053 - loss: 5.5437 - val_accuracy: 0.0044 - val_loss: 5.5438\n","Epoch 15/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5444 - val_accuracy: 0.0044 - val_loss: 5.5461\n","Epoch 16/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0041 - loss: 5.5440 - val_accuracy: 0.0036 - val_loss: 5.5470\n","Epoch 17/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 176ms/step - accuracy: 0.0040 - loss: 5.5437 - val_accuracy: 0.0044 - val_loss: 5.5469\n","Epoch 18/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0051 - loss: 5.5436 - val_accuracy: 0.0036 - val_loss: 5.5469\n","Epoch 19/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0042 - loss: 5.5440 - val_accuracy: 0.0044 - val_loss: 5.5468\n","Epoch 20/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5439 - val_accuracy: 0.0044 - val_loss: 5.5472\n","Epoch 21/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0046 - loss: 5.5446 - val_accuracy: 0.0036 - val_loss: 5.5459\n","Epoch 22/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0044 - loss: 5.5438 - val_accuracy: 0.0044 - val_loss: 5.5467\n","Epoch 23/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5435 - val_accuracy: 0.0044 - val_loss: 5.5453\n","Epoch 24/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0046 - loss: 5.5439 - val_accuracy: 0.0046 - val_loss: 5.5469\n","Epoch 25/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0052 - loss: 5.5444 - val_accuracy: 0.0042 - val_loss: 5.5461\n","Epoch 26/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0045 - loss: 5.5436 - val_accuracy: 0.0044 - val_loss: 5.5464\n","Epoch 27/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0046 - loss: 5.5434 - val_accuracy: 0.0044 - val_loss: 5.5460\n","Epoch 28/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0043 - loss: 5.5440 - val_accuracy: 0.0044 - val_loss: 5.5460\n","Epoch 29/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5435 - val_accuracy: 0.0044 - val_loss: 5.5471\n","Epoch 30/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5437 - val_accuracy: 0.0036 - val_loss: 5.5463\n","Epoch 31/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0042 - loss: 5.5440 - val_accuracy: 0.0044 - val_loss: 5.5464\n","Epoch 32/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0057 - loss: 5.5433 - val_accuracy: 0.0052 - val_loss: 5.5456\n","Epoch 33/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5437 - val_accuracy: 0.0044 - val_loss: 5.5460\n","Epoch 34/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0052 - loss: 5.5434 - val_accuracy: 0.0044 - val_loss: 5.5460\n","Epoch 35/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5436 - val_accuracy: 0.0052 - val_loss: 5.5474\n","Epoch 36/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0042 - loss: 5.5434 - val_accuracy: 0.0044 - val_loss: 5.5470\n","Epoch 37/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5434 - val_accuracy: 0.0046 - val_loss: 5.5456\n","Epoch 38/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0044 - loss: 5.5441 - val_accuracy: 0.0052 - val_loss: 5.5464\n","Epoch 39/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0054 - loss: 5.5439 - val_accuracy: 0.0044 - val_loss: 5.5468\n","Epoch 40/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0043 - loss: 5.5436 - val_accuracy: 0.0052 - val_loss: 5.5464\n","Epoch 41/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0044 - loss: 5.5442 - val_accuracy: 0.0044 - val_loss: 5.5459\n","Epoch 42/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5435 - val_accuracy: 0.0044 - val_loss: 5.5471\n","Epoch 43/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5436 - val_accuracy: 0.0044 - val_loss: 5.5467\n","Epoch 44/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0037 - loss: 5.5434 - val_accuracy: 0.0044 - val_loss: 5.5466\n","Epoch 45/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0055 - loss: 5.5431 - val_accuracy: 0.0044 - val_loss: 5.5455\n","Epoch 46/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0051 - loss: 5.5434 - val_accuracy: 0.0044 - val_loss: 5.5459\n","Epoch 47/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0044 - loss: 5.5432 - val_accuracy: 0.0044 - val_loss: 5.5467\n","Epoch 48/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0045 - loss: 5.5435 - val_accuracy: 0.0044 - val_loss: 5.5462\n","Epoch 49/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0041 - loss: 5.5435 - val_accuracy: 0.0044 - val_loss: 5.5479\n","Epoch 50/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0056 - loss: 5.5437 - val_accuracy: 0.0044 - val_loss: 5.5461\n","Epoch 51/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5436 - val_accuracy: 0.0044 - val_loss: 5.5465\n","Epoch 52/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0038 - loss: 5.5435 - val_accuracy: 0.0044 - val_loss: 5.5474\n","Epoch 53/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5437 - val_accuracy: 0.0044 - val_loss: 5.5483\n","Epoch 54/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5435 - val_accuracy: 0.0044 - val_loss: 5.5464\n","Epoch 55/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0046 - loss: 5.5433 - val_accuracy: 0.0044 - val_loss: 5.5464\n","Epoch 56/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0046 - loss: 5.5437 - val_accuracy: 0.0044 - val_loss: 5.5465\n","Epoch 57/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0045 - loss: 5.5434 - val_accuracy: 0.0048 - val_loss: 5.5460\n","Epoch 58/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0046 - loss: 5.5433 - val_accuracy: 0.0044 - val_loss: 5.5459\n","Epoch 59/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5440 - val_accuracy: 0.0034 - val_loss: 5.5465\n","Epoch 60/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5440 - val_accuracy: 0.0036 - val_loss: 5.5469\n","Epoch 61/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5434 - val_accuracy: 0.0044 - val_loss: 5.5459\n","Epoch 62/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5432 - val_accuracy: 0.0044 - val_loss: 5.5469\n","Epoch 63/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0046 - loss: 5.5439 - val_accuracy: 0.0042 - val_loss: 5.5459\n","Epoch 64/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5436 - val_accuracy: 0.0036 - val_loss: 5.5463\n","Epoch 65/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0048 - loss: 5.5433 - val_accuracy: 0.0044 - val_loss: 5.5462\n","Epoch 66/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5436 - val_accuracy: 0.0044 - val_loss: 5.5467\n","Epoch 67/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0051 - loss: 5.5438 - val_accuracy: 0.0044 - val_loss: 5.5461\n","Epoch 68/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0049 - loss: 5.5433 - val_accuracy: 0.0036 - val_loss: 5.5461\n","Epoch 69/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0051 - loss: 5.5434 - val_accuracy: 0.0036 - val_loss: 5.5474\n","Epoch 70/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0047 - loss: 5.5437 - val_accuracy: 0.0044 - val_loss: 5.5456\n","Epoch 71/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0041 - loss: 5.5433 - val_accuracy: 0.0044 - val_loss: 5.5469\n","Epoch 72/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.0052 - loss: 5.5436 - val_accuracy: 0.0044 - val_loss: 5.5451\n","Epoch 73/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0043 - loss: 5.5438 - val_accuracy: 0.0034 - val_loss: 5.5458\n","Epoch 74/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0050 - loss: 5.5434 - val_accuracy: 0.0044 - val_loss: 5.5460\n","Epoch 75/75\n","\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 177ms/step - accuracy: 0.0048 - loss: 5.5431 - val_accuracy: 0.0044 - val_loss: 5.5463\n"]}],"source":["tfmr_sca_base_2 = transformer_sca_baseline()\n","tfmr_sca_base_2.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","history_2 = tfmr_sca_base_2.fit(ds_train, batch_size=64, epochs=75, validation_data=ds_val)\n","\n","tfmr_sca_base_2.save(\"/content/drive/MyDrive/trfmr_baseline_2.keras\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1747269938900,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"FnRTaWcb_sWR","outputId":"8944c119-9f9c-4e06-eb50-55c7727a2acd"},"outputs":[{"name":"stdout","output_type":"stream","text":["<keras.src.callbacks.history.History object at 0x7b70781c8e50>\n"]}],"source":["print(history_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUYdZbiDclQ4"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    Input, Dense, LayerNormalization, Dropout,\n","    MultiHeadAttention, GlobalAveragePooling1D,\n","    Conv1D, Embedding\n",")\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","def transformer_sca_baseline(\n","    input_length=700,\n","    num_classes=256,\n","    d_model=64,\n","    num_heads=2,\n","    ff_dim=128,\n","    dropout=0.1,\n","    num_blocks=4\n","):\n","    # Input: traces of shape (700, 1)\n","    inputs = Input(shape=(input_length, 1))\n","\n","    # Initial projection via 1D conv\n","    x = Conv1D(filters=d_model, kernel_size=5, padding='same', activation='relu')(inputs)\n","\n","    # Fixed positional encoding using Embedding\n","    positions = tf.range(start=0, limit=input_length, delta=1)\n","    position_embedding = Embedding(input_dim=input_length, output_dim=d_model)\n","    pos_encoding = position_embedding(positions)  # shape: (700, d_model)\n","    pos_encoding = tf.expand_dims(pos_encoding, axis=0)  # shape: (1, 700, d_model)\n","    x = x + pos_encoding  # broadcasted across batch\n","\n","    # Transformer encoder blocks\n","    for _ in range(num_blocks):\n","        # Multi-head self-attention\n","        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x, x)\n","        attn_output = Dropout(dropout)(attn_output)\n","        x = Add()([x, attn_output])\n","        x = LayerNormalization(epsilon=1e-6)(x)\n","\n","        # Feed-forward network\n","        ffn_output = Dense(ff_dim, activation='relu')(x)\n","        ffn_output = Dense(d_model)(ffn_output)\n","        ffn_output = Dropout(dropout)(ffn_output)\n","        x = Add()([x, ffn_output])\n","        x = LayerNormalization(epsilon=1e-6)(x)\n","\n","    # Final global pooling + classification\n","    x = GlobalAveragePooling1D()(x)\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","\n","    return Model(inputs=inputs, outputs=outputs, name=\"transformer_sca_baseline\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"elapsed":47819,"status":"error","timestamp":1747288803431,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"ozObs6pjdNrc","outputId":"eb8a780b-305f-4375-8249-25fd81f7455e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/75\n","\u001b[1m310/704\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 82ms/step - accuracy: 0.0038 - loss: 5.5699"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-05e965e27fcc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = transformer_sca_baseline()\n","\n","# Use label smoothing to regularize softmax outputs\n","loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n","\n","model.compile(\n","    optimizer=Adam(learning_rate=1e-4),\n","    loss=loss_fn,\n","    metrics=['accuracy']\n",")\n","\n","# Add early stopping to prevent long flat training\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n","\n","history = model.fit(\n","    ds_train,\n","    validation_data=ds_val,\n","    batch_size=200,\n","    epochs=75,\n","    callbacks=[early_stop]\n",")\n","\n","model.save(\"/content/drive/MyDrive/trfmr_baseline_fixed.keras\")\n"]},{"cell_type":"markdown","metadata":{"id":"Ab6ZdUP-fBcV"},"source":["# pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-oCVGxL8fCxK"},"outputs":[],"source":["import os\n","import sys\n","import h5py\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","RANDOM_STATE = 42\n","VAL_SIZE = 0.1\n","\n","def check_file_exists(file_path):\n","    file_path = os.path.normpath(file_path)\n","    if not os.path.exists(file_path):\n","        print(f\"Error: provided file path '{file_path}' does not exist!\")\n","        sys.exit(-1)\n","\n","def load_ascad(ascad_database_file):\n","    check_file_exists(ascad_database_file)\n","    with h5py.File(ascad_database_file, \"r\") as f:\n","        X_profiling = np.array(f['Profiling_traces/traces'], dtype=np.float32)\n","        Y_profiling = np.array(f['Profiling_traces/labels'], dtype=np.int64)\n","        X_attack = np.array(f['Attack_traces/traces'], dtype=np.float32)\n","        Y_attack = np.array(f['Attack_traces/labels'], dtype=np.int64)\n","    return (X_profiling, Y_profiling), (X_attack, Y_attack)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCnxsLlTiQdy"},"outputs":[],"source":["class ASCADDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # (N, 1, 700)\n","        self.y = torch.tensor(y, dtype=torch.long)  # integer labels\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VkLtIQj-fGXE"},"outputs":[],"source":["ascad_path = \"/content/drive/MyDrive/ASCAD_databases/ASCAD.h5\"\n","(X_profiling, Y_profiling), (X_attack, Y_attack) = load_ascad(ascad_path)\n","\n","# Normalize if needed (standard in ASCAD)\n","X_profiling /= 255.0\n","X_attack /= 255.0\n","\n","# Train/val split\n","X_train, X_val, Y_train, Y_val = train_test_split(\n","    X_profiling, Y_profiling, test_size=VAL_SIZE, random_state=RANDOM_STATE\n",")\n","\n","# Dataset and loaders\n","train_ds = ASCADDataset(X_train, Y_train)\n","val_ds = ASCADDataset(X_val, Y_val)\n","\n","train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sw1zl9lMhK2I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=700):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1), :].to(x.device)\n","        return x\n","\n","class TransformerSCA(nn.Module):\n","    def __init__(self, input_length=700, num_classes=256, d_model=32,\n","                 num_heads=2, ff_dim=128, num_layers=2, dropout=0.1):\n","        super().__init__()\n","        self.input_proj = nn.Conv1d(1, d_model, kernel_size=5, padding=2)\n","        self.pos_enc = PositionalEncoding(d_model, max_len=input_length)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            dim_feedforward=ff_dim,\n","            dropout=dropout,\n","            batch_first=True  # Important for (B, T, D) input shape\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","        self.pool = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","        # x: (B, 1, L)\n","        x = self.input_proj(x)  # (B, d_model, L)\n","        x = x.permute(0, 2, 1)  # (B, L, d_model)\n","        x = self.pos_enc(x)\n","        x = self.transformer(x)  # (B, L, d_model)\n","        x = x.permute(0, 2, 1)  # (B, d_model, L)\n","        x = self.pool(x).squeeze(-1)  # (B, d_model)\n","        return self.fc(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1747342197976,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"PjsvO8Yatfy7","outputId":"ac137240-1ccf-422e-e081-877d18046eb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using:\", device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9458560,"status":"ok","timestamp":1747306101490,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"UHwe08xnhhtz","outputId":"cf2d766f-a53c-46da-ced4-8a964d5cd09e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using: cuda\n","Epoch 1/50 Train Loss: 5.5682 Acc: 0.0042 Val Loss: 5.5548 Acc: 0.0036\n","Epoch 2/50 Train Loss: 5.5536 Acc: 0.0037 Val Loss: 5.5502 Acc: 0.0046\n","Epoch 3/50 Train Loss: 5.5528 Acc: 0.0045 Val Loss: 5.5511 Acc: 0.0038\n","Epoch 4/50 Train Loss: 5.5515 Acc: 0.0040 Val Loss: 5.5506 Acc: 0.0046\n","Epoch 5/50 Train Loss: 5.5515 Acc: 0.0039 Val Loss: 5.5476 Acc: 0.0044\n","Epoch 6/50 Train Loss: 5.5507 Acc: 0.0041 Val Loss: 5.5514 Acc: 0.0042\n","Epoch 7/50 Train Loss: 5.5506 Acc: 0.0043 Val Loss: 5.5481 Acc: 0.0042\n","Epoch 8/50 Train Loss: 5.5500 Acc: 0.0040 Val Loss: 5.5498 Acc: 0.0046\n","Epoch 9/50 Train Loss: 5.5495 Acc: 0.0043 Val Loss: 5.5474 Acc: 0.0054\n","Epoch 10/50 Train Loss: 5.5497 Acc: 0.0039 Val Loss: 5.5472 Acc: 0.0048\n","Epoch 11/50 Train Loss: 5.5496 Acc: 0.0037 Val Loss: 5.5482 Acc: 0.0032\n","Epoch 12/50 Train Loss: 5.5493 Acc: 0.0044 Val Loss: 5.5481 Acc: 0.0042\n","Epoch 13/50 Train Loss: 5.5491 Acc: 0.0042 Val Loss: 5.5476 Acc: 0.0036\n","Epoch 14/50 Train Loss: 5.5485 Acc: 0.0039 Val Loss: 5.5494 Acc: 0.0040\n","Epoch 15/50 Train Loss: 5.5490 Acc: 0.0043 Val Loss: 5.5477 Acc: 0.0032\n","Epoch 16/50 Train Loss: 5.5488 Acc: 0.0041 Val Loss: 5.5489 Acc: 0.0044\n","Epoch 17/50 Train Loss: 5.5486 Acc: 0.0040 Val Loss: 5.5484 Acc: 0.0046\n","Epoch 18/50 Train Loss: 5.5487 Acc: 0.0041 Val Loss: 5.5460 Acc: 0.0036\n","Epoch 19/50 Train Loss: 5.5483 Acc: 0.0046 Val Loss: 5.5477 Acc: 0.0032\n","Epoch 20/50 Train Loss: 5.5482 Acc: 0.0038 Val Loss: 5.5486 Acc: 0.0040\n","Epoch 21/50 Train Loss: 5.5480 Acc: 0.0043 Val Loss: 5.5464 Acc: 0.0042\n","Epoch 22/50 Train Loss: 5.5479 Acc: 0.0041 Val Loss: 5.5469 Acc: 0.0062\n","Epoch 23/50 Train Loss: 5.5478 Acc: 0.0041 Val Loss: 5.5489 Acc: 0.0036\n","Epoch 24/50 Train Loss: 5.5479 Acc: 0.0039 Val Loss: 5.5482 Acc: 0.0044\n","Epoch 25/50 Train Loss: 5.5476 Acc: 0.0044 Val Loss: 5.5480 Acc: 0.0044\n","Epoch 26/50 Train Loss: 5.5476 Acc: 0.0042 Val Loss: 5.5469 Acc: 0.0044\n","Epoch 27/50 Train Loss: 5.5476 Acc: 0.0046 Val Loss: 5.5476 Acc: 0.0036\n","Epoch 28/50 Train Loss: 5.5476 Acc: 0.0044 Val Loss: 5.5474 Acc: 0.0044\n","Epoch 29/50 Train Loss: 5.5475 Acc: 0.0041 Val Loss: 5.5477 Acc: 0.0044\n","Epoch 30/50 Train Loss: 5.5475 Acc: 0.0040 Val Loss: 5.5474 Acc: 0.0044\n","Epoch 31/50 Train Loss: 5.5472 Acc: 0.0045 Val Loss: 5.5455 Acc: 0.0044\n","Epoch 32/50 Train Loss: 5.5475 Acc: 0.0042 Val Loss: 5.5473 Acc: 0.0046\n","Epoch 33/50 Train Loss: 5.5473 Acc: 0.0042 Val Loss: 5.5473 Acc: 0.0032\n","Epoch 34/50 Train Loss: 5.5473 Acc: 0.0041 Val Loss: 5.5467 Acc: 0.0042\n","Epoch 35/50 Train Loss: 5.5471 Acc: 0.0044 Val Loss: 5.5456 Acc: 0.0044\n","Epoch 36/50 Train Loss: 5.5470 Acc: 0.0044 Val Loss: 5.5471 Acc: 0.0030\n","Epoch 37/50 Train Loss: 5.5470 Acc: 0.0042 Val Loss: 5.5465 Acc: 0.0062\n","Epoch 38/50 Train Loss: 5.5471 Acc: 0.0043 Val Loss: 5.5474 Acc: 0.0044\n","Epoch 39/50 Train Loss: 5.5469 Acc: 0.0038 Val Loss: 5.5471 Acc: 0.0044\n","Epoch 40/50 Train Loss: 5.5468 Acc: 0.0042 Val Loss: 5.5471 Acc: 0.0044\n","Epoch 41/50 Train Loss: 5.5468 Acc: 0.0043 Val Loss: 5.5467 Acc: 0.0032\n","Epoch 42/50 Train Loss: 5.5468 Acc: 0.0040 Val Loss: 5.5460 Acc: 0.0046\n","Epoch 43/50 Train Loss: 5.5466 Acc: 0.0044 Val Loss: 5.5465 Acc: 0.0036\n","Epoch 44/50 Train Loss: 5.5469 Acc: 0.0039 Val Loss: 5.5468 Acc: 0.0044\n","Epoch 45/50 Train Loss: 5.5466 Acc: 0.0043 Val Loss: 5.5456 Acc: 0.0044\n","Epoch 46/50 Train Loss: 5.5466 Acc: 0.0043 Val Loss: 5.5460 Acc: 0.0036\n","Epoch 47/50 Train Loss: 5.5466 Acc: 0.0045 Val Loss: 5.5466 Acc: 0.0062\n","Epoch 48/50 Train Loss: 5.5465 Acc: 0.0041 Val Loss: 5.5458 Acc: 0.0044\n","Epoch 49/50 Train Loss: 5.5464 Acc: 0.0046 Val Loss: 5.5461 Acc: 0.0044\n","Epoch 50/50 Train Loss: 5.5465 Acc: 0.0042 Val Loss: 5.5471 Acc: 0.0062\n"]}],"source":["model = TransformerSCA().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n","\n","num_epochs = 50\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss, train_correct = 0.0, 0\n","\n","    for X_batch, y_batch in train_loader:\n","        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(X_batch)  # (B, num_classes)\n","        loss = criterion(outputs, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * X_batch.size(0)\n","        preds = torch.argmax(outputs, dim=1)\n","        train_correct += (preds == y_batch).sum().item()\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = train_correct / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss, val_correct = 0.0, 0\n","    with torch.no_grad():\n","        for X_val, y_val in val_loader:\n","            X_val, y_val = X_val.to(device), y_val.to(device)\n","            outputs = model(X_val)\n","            loss = criterion(outputs, y_val)\n","            val_loss += loss.item() * X_val.size(0)\n","            preds = torch.argmax(outputs, dim=1)\n","            val_correct += (preds == y_val).sum().item()\n","\n","    val_loss /= len(val_loader.dataset)\n","    val_acc = val_correct / len(val_loader.dataset)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} \"\n","          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} \"\n","          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0FZ6JJ-kUGa"},"outputs":[],"source":["torch.save(model, \"/content/drive/MyDrive/transformer_sca_full_model.pt\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHlSOEcOCBCF"},"outputs":[],"source":["torch.save(model, \"/content/drive/MyDrive/transformer_sca_full_model_2.pt\")"]},{"cell_type":"markdown","metadata":{"id":"i3ej7W-CtHhZ"},"source":["## Overfit sanity check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uanIcahStJNT"},"outputs":[],"source":["small_X = torch.tensor(X_train[:512], dtype=torch.float32).unsqueeze(1).to(device)\n","small_y = torch.tensor(Y_train[:512], dtype=torch.long).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1747346022268,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"l1c83YI08D1U","outputId":"69001f44-0f3e-4830-c814-bc635cbeae4d"},"outputs":[{"data":{"text/plain":["(220,)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","np.unique(small_y.cpu()).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dqg7yPho3XCz"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","debug_dataset = TensorDataset(small_X, small_y)\n","\n","debug_loader = DataLoader(debug_dataset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14885,"status":"ok","timestamp":1747346047953,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"nJwvbdo1tkaY","outputId":"d7404f0d-52df-4e33-debd-42934eb6efe1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: Loss = 0.7017, Acc = 0.0000\n","Epoch 0: Loss = 1.4093, Acc = 0.0000\n","Epoch 0: Loss = 2.1168, Acc = 0.0000\n","Epoch 0: Loss = 2.8117, Acc = 0.0000\n","Epoch 0: Loss = 3.5215, Acc = 0.0000\n","Epoch 0: Loss = 4.2195, Acc = 0.0000\n","Epoch 0: Loss = 4.9217, Acc = 0.0000\n","Epoch 0: Loss = 5.6101, Acc = 0.0000\n","Epoch 1: Loss = 0.6853, Acc = 0.0020\n","Epoch 1: Loss = 1.3764, Acc = 0.0020\n","Epoch 1: Loss = 2.0715, Acc = 0.0020\n","Epoch 1: Loss = 2.7673, Acc = 0.0020\n","Epoch 1: Loss = 3.4607, Acc = 0.0020\n","Epoch 1: Loss = 4.1534, Acc = 0.0020\n","Epoch 1: Loss = 4.8491, Acc = 0.0020\n","Epoch 1: Loss = 5.5406, Acc = 0.0020\n","Epoch 2: Loss = 0.6942, Acc = 0.0000\n","Epoch 2: Loss = 1.3796, Acc = 0.0000\n","Epoch 2: Loss = 2.0664, Acc = 0.0000\n","Epoch 2: Loss = 2.7637, Acc = 0.0000\n","Epoch 2: Loss = 3.4536, Acc = 0.0000\n","Epoch 2: Loss = 4.1339, Acc = 0.0020\n","Epoch 2: Loss = 4.8198, Acc = 0.0039\n","Epoch 2: Loss = 5.5104, Acc = 0.0039\n","Epoch 3: Loss = 0.6795, Acc = 0.0000\n","Epoch 3: Loss = 1.3607, Acc = 0.0000\n","Epoch 3: Loss = 2.0461, Acc = 0.0000\n","Epoch 3: Loss = 2.7287, Acc = 0.0000\n","Epoch 3: Loss = 3.4110, Acc = 0.0000\n","Epoch 3: Loss = 4.1033, Acc = 0.0000\n","Epoch 3: Loss = 4.7890, Acc = 0.0020\n","Epoch 3: Loss = 5.4812, Acc = 0.0039\n","Epoch 4: Loss = 0.6804, Acc = 0.0000\n","Epoch 4: Loss = 1.3640, Acc = 0.0020\n","Epoch 4: Loss = 2.0410, Acc = 0.0039\n","Epoch 4: Loss = 2.7222, Acc = 0.0039\n","Epoch 4: Loss = 3.4113, Acc = 0.0078\n","Epoch 4: Loss = 4.0901, Acc = 0.0078\n","Epoch 4: Loss = 4.7667, Acc = 0.0078\n","Epoch 4: Loss = 5.4453, Acc = 0.0098\n","Epoch 5: Loss = 0.6721, Acc = 0.0000\n","Epoch 5: Loss = 1.3504, Acc = 0.0020\n","Epoch 5: Loss = 2.0198, Acc = 0.0020\n","Epoch 5: Loss = 2.7009, Acc = 0.0039\n","Epoch 5: Loss = 3.3794, Acc = 0.0059\n","Epoch 5: Loss = 4.0512, Acc = 0.0078\n","Epoch 5: Loss = 4.7335, Acc = 0.0078\n","Epoch 5: Loss = 5.4091, Acc = 0.0098\n","Epoch 6: Loss = 0.6775, Acc = 0.0020\n","Epoch 6: Loss = 1.3524, Acc = 0.0020\n","Epoch 6: Loss = 2.0219, Acc = 0.0020\n","Epoch 6: Loss = 2.7014, Acc = 0.0039\n","Epoch 6: Loss = 3.3698, Acc = 0.0059\n","Epoch 6: Loss = 4.0432, Acc = 0.0078\n","Epoch 6: Loss = 4.7092, Acc = 0.0078\n","Epoch 6: Loss = 5.3747, Acc = 0.0098\n","Epoch 7: Loss = 0.6719, Acc = 0.0000\n","Epoch 7: Loss = 1.3403, Acc = 0.0039\n","Epoch 7: Loss = 2.0037, Acc = 0.0039\n","Epoch 7: Loss = 2.6639, Acc = 0.0039\n","Epoch 7: Loss = 3.3326, Acc = 0.0059\n","Epoch 7: Loss = 4.0069, Acc = 0.0078\n","Epoch 7: Loss = 4.6750, Acc = 0.0098\n","Epoch 7: Loss = 5.3469, Acc = 0.0098\n","Epoch 8: Loss = 0.6671, Acc = 0.0020\n","Epoch 8: Loss = 1.3287, Acc = 0.0059\n","Epoch 8: Loss = 1.9928, Acc = 0.0078\n","Epoch 8: Loss = 2.6615, Acc = 0.0078\n","Epoch 8: Loss = 3.3219, Acc = 0.0098\n","Epoch 8: Loss = 3.9927, Acc = 0.0098\n","Epoch 8: Loss = 4.6560, Acc = 0.0117\n","Epoch 8: Loss = 5.3224, Acc = 0.0137\n","Epoch 9: Loss = 0.6658, Acc = 0.0000\n","Epoch 9: Loss = 1.3245, Acc = 0.0059\n","Epoch 9: Loss = 1.9835, Acc = 0.0078\n","Epoch 9: Loss = 2.6480, Acc = 0.0078\n","Epoch 9: Loss = 3.2975, Acc = 0.0117\n","Epoch 9: Loss = 3.9740, Acc = 0.0117\n","Epoch 9: Loss = 4.6437, Acc = 0.0117\n","Epoch 9: Loss = 5.3044, Acc = 0.0137\n","Epoch 10: Loss = 0.6503, Acc = 0.0000\n","Epoch 10: Loss = 1.3058, Acc = 0.0000\n","Epoch 10: Loss = 1.9774, Acc = 0.0020\n","Epoch 10: Loss = 2.6417, Acc = 0.0078\n","Epoch 10: Loss = 3.3049, Acc = 0.0098\n","Epoch 10: Loss = 3.9704, Acc = 0.0137\n","Epoch 10: Loss = 4.6278, Acc = 0.0137\n","Epoch 10: Loss = 5.2910, Acc = 0.0137\n","Epoch 11: Loss = 0.6632, Acc = 0.0000\n","Epoch 11: Loss = 1.3268, Acc = 0.0000\n","Epoch 11: Loss = 1.9834, Acc = 0.0059\n","Epoch 11: Loss = 2.6499, Acc = 0.0078\n","Epoch 11: Loss = 3.3008, Acc = 0.0078\n","Epoch 11: Loss = 3.9560, Acc = 0.0078\n","Epoch 11: Loss = 4.6170, Acc = 0.0098\n","Epoch 11: Loss = 5.2821, Acc = 0.0137\n","Epoch 12: Loss = 0.6447, Acc = 0.0020\n","Epoch 12: Loss = 1.2991, Acc = 0.0059\n","Epoch 12: Loss = 1.9586, Acc = 0.0078\n","Epoch 12: Loss = 2.6209, Acc = 0.0078\n","Epoch 12: Loss = 3.2830, Acc = 0.0098\n","Epoch 12: Loss = 3.9410, Acc = 0.0137\n","Epoch 12: Loss = 4.6128, Acc = 0.0137\n","Epoch 12: Loss = 5.2767, Acc = 0.0137\n","Epoch 13: Loss = 0.6662, Acc = 0.0020\n","Epoch 13: Loss = 1.3324, Acc = 0.0020\n","Epoch 13: Loss = 1.9763, Acc = 0.0059\n","Epoch 13: Loss = 2.6356, Acc = 0.0078\n","Epoch 13: Loss = 3.2983, Acc = 0.0078\n","Epoch 13: Loss = 3.9604, Acc = 0.0117\n","Epoch 13: Loss = 4.6128, Acc = 0.0117\n","Epoch 13: Loss = 5.2693, Acc = 0.0137\n","Epoch 14: Loss = 0.6471, Acc = 0.0020\n","Epoch 14: Loss = 1.2958, Acc = 0.0039\n","Epoch 14: Loss = 1.9482, Acc = 0.0039\n","Epoch 14: Loss = 2.6245, Acc = 0.0078\n","Epoch 14: Loss = 3.2725, Acc = 0.0078\n","Epoch 14: Loss = 3.9271, Acc = 0.0078\n","Epoch 14: Loss = 4.5832, Acc = 0.0098\n","Epoch 14: Loss = 5.2680, Acc = 0.0098\n","Epoch 15: Loss = 0.6513, Acc = 0.0020\n","Epoch 15: Loss = 1.3024, Acc = 0.0039\n","Epoch 15: Loss = 1.9664, Acc = 0.0039\n","Epoch 15: Loss = 2.6267, Acc = 0.0059\n","Epoch 15: Loss = 3.2743, Acc = 0.0078\n","Epoch 15: Loss = 3.9314, Acc = 0.0098\n","Epoch 15: Loss = 4.5958, Acc = 0.0098\n","Epoch 15: Loss = 5.2638, Acc = 0.0098\n","Epoch 16: Loss = 0.6665, Acc = 0.0000\n","Epoch 16: Loss = 1.3201, Acc = 0.0020\n","Epoch 16: Loss = 1.9625, Acc = 0.0078\n","Epoch 16: Loss = 2.6201, Acc = 0.0098\n","Epoch 16: Loss = 3.2764, Acc = 0.0098\n","Epoch 16: Loss = 3.9333, Acc = 0.0098\n","Epoch 16: Loss = 4.5969, Acc = 0.0098\n","Epoch 16: Loss = 5.2626, Acc = 0.0098\n","Epoch 17: Loss = 0.6526, Acc = 0.0000\n","Epoch 17: Loss = 1.3059, Acc = 0.0000\n","Epoch 17: Loss = 1.9737, Acc = 0.0000\n","Epoch 17: Loss = 2.6335, Acc = 0.0000\n","Epoch 17: Loss = 3.2956, Acc = 0.0020\n","Epoch 17: Loss = 3.9560, Acc = 0.0039\n","Epoch 17: Loss = 4.6015, Acc = 0.0059\n","Epoch 17: Loss = 5.2609, Acc = 0.0078\n","Epoch 18: Loss = 0.6594, Acc = 0.0059\n","Epoch 18: Loss = 1.3007, Acc = 0.0098\n","Epoch 18: Loss = 1.9590, Acc = 0.0117\n","Epoch 18: Loss = 2.6032, Acc = 0.0117\n","Epoch 18: Loss = 3.2647, Acc = 0.0117\n","Epoch 18: Loss = 3.9397, Acc = 0.0137\n","Epoch 18: Loss = 4.6037, Acc = 0.0137\n","Epoch 18: Loss = 5.2597, Acc = 0.0137\n","Epoch 19: Loss = 0.6557, Acc = 0.0039\n","Epoch 19: Loss = 1.3194, Acc = 0.0039\n","Epoch 19: Loss = 1.9798, Acc = 0.0078\n","Epoch 19: Loss = 2.6245, Acc = 0.0078\n","Epoch 19: Loss = 3.2746, Acc = 0.0078\n","Epoch 19: Loss = 3.9374, Acc = 0.0117\n","Epoch 19: Loss = 4.5903, Acc = 0.0117\n","Epoch 19: Loss = 5.2594, Acc = 0.0137\n","Epoch 20: Loss = 0.6525, Acc = 0.0020\n","Epoch 20: Loss = 1.3042, Acc = 0.0039\n","Epoch 20: Loss = 1.9523, Acc = 0.0059\n","Epoch 20: Loss = 2.6078, Acc = 0.0117\n","Epoch 20: Loss = 3.2896, Acc = 0.0117\n","Epoch 20: Loss = 3.9514, Acc = 0.0137\n","Epoch 20: Loss = 4.6100, Acc = 0.0137\n","Epoch 20: Loss = 5.2594, Acc = 0.0137\n","Epoch 21: Loss = 0.6591, Acc = 0.0000\n","Epoch 21: Loss = 1.3148, Acc = 0.0020\n","Epoch 21: Loss = 1.9756, Acc = 0.0039\n","Epoch 21: Loss = 2.6300, Acc = 0.0039\n","Epoch 21: Loss = 3.2900, Acc = 0.0078\n","Epoch 21: Loss = 3.9361, Acc = 0.0078\n","Epoch 21: Loss = 4.5956, Acc = 0.0078\n","Epoch 21: Loss = 5.2578, Acc = 0.0078\n","Epoch 22: Loss = 0.6518, Acc = 0.0059\n","Epoch 22: Loss = 1.3072, Acc = 0.0098\n","Epoch 22: Loss = 1.9593, Acc = 0.0117\n","Epoch 22: Loss = 2.6185, Acc = 0.0137\n","Epoch 22: Loss = 3.2774, Acc = 0.0137\n","Epoch 22: Loss = 3.9335, Acc = 0.0137\n","Epoch 22: Loss = 4.5964, Acc = 0.0137\n","Epoch 22: Loss = 5.2579, Acc = 0.0137\n","Epoch 23: Loss = 0.6569, Acc = 0.0020\n","Epoch 23: Loss = 1.3197, Acc = 0.0020\n","Epoch 23: Loss = 1.9635, Acc = 0.0078\n","Epoch 23: Loss = 2.6054, Acc = 0.0098\n","Epoch 23: Loss = 3.2612, Acc = 0.0117\n","Epoch 23: Loss = 3.9336, Acc = 0.0117\n","Epoch 23: Loss = 4.5958, Acc = 0.0117\n","Epoch 23: Loss = 5.2564, Acc = 0.0137\n","Epoch 24: Loss = 0.6585, Acc = 0.0000\n","Epoch 24: Loss = 1.3304, Acc = 0.0000\n","Epoch 24: Loss = 1.9739, Acc = 0.0020\n","Epoch 24: Loss = 2.6196, Acc = 0.0039\n","Epoch 24: Loss = 3.2932, Acc = 0.0039\n","Epoch 24: Loss = 3.9513, Acc = 0.0059\n","Epoch 24: Loss = 4.6041, Acc = 0.0078\n","Epoch 24: Loss = 5.2550, Acc = 0.0098\n","Epoch 25: Loss = 0.6656, Acc = 0.0059\n","Epoch 25: Loss = 1.3272, Acc = 0.0059\n","Epoch 25: Loss = 1.9990, Acc = 0.0098\n","Epoch 25: Loss = 2.6478, Acc = 0.0137\n","Epoch 25: Loss = 3.2942, Acc = 0.0137\n","Epoch 25: Loss = 3.9667, Acc = 0.0137\n","Epoch 25: Loss = 4.6123, Acc = 0.0137\n","Epoch 25: Loss = 5.2566, Acc = 0.0137\n","Epoch 26: Loss = 0.6466, Acc = 0.0020\n","Epoch 26: Loss = 1.2963, Acc = 0.0020\n","Epoch 26: Loss = 1.9420, Acc = 0.0039\n","Epoch 26: Loss = 2.5911, Acc = 0.0039\n","Epoch 26: Loss = 3.2662, Acc = 0.0059\n","Epoch 26: Loss = 3.9248, Acc = 0.0059\n","Epoch 26: Loss = 4.5924, Acc = 0.0078\n","Epoch 26: Loss = 5.2539, Acc = 0.0078\n","Epoch 27: Loss = 0.6528, Acc = 0.0078\n","Epoch 27: Loss = 1.3094, Acc = 0.0098\n","Epoch 27: Loss = 1.9640, Acc = 0.0117\n","Epoch 27: Loss = 2.6244, Acc = 0.0137\n","Epoch 27: Loss = 3.2786, Acc = 0.0137\n","Epoch 27: Loss = 3.9434, Acc = 0.0137\n","Epoch 27: Loss = 4.6031, Acc = 0.0137\n","Epoch 27: Loss = 5.2554, Acc = 0.0137\n","Epoch 28: Loss = 0.6516, Acc = 0.0020\n","Epoch 28: Loss = 1.2977, Acc = 0.0059\n","Epoch 28: Loss = 1.9551, Acc = 0.0098\n","Epoch 28: Loss = 2.6104, Acc = 0.0117\n","Epoch 28: Loss = 3.2674, Acc = 0.0117\n","Epoch 28: Loss = 3.9185, Acc = 0.0117\n","Epoch 28: Loss = 4.5831, Acc = 0.0117\n","Epoch 28: Loss = 5.2552, Acc = 0.0137\n","Epoch 29: Loss = 0.6663, Acc = 0.0000\n","Epoch 29: Loss = 1.3349, Acc = 0.0000\n","Epoch 29: Loss = 1.9872, Acc = 0.0039\n","Epoch 29: Loss = 2.6403, Acc = 0.0059\n","Epoch 29: Loss = 3.2961, Acc = 0.0078\n","Epoch 29: Loss = 3.9424, Acc = 0.0098\n","Epoch 29: Loss = 4.6021, Acc = 0.0117\n","Epoch 29: Loss = 5.2537, Acc = 0.0137\n","Epoch 30: Loss = 0.6522, Acc = 0.0000\n","Epoch 30: Loss = 1.3150, Acc = 0.0039\n","Epoch 30: Loss = 1.9739, Acc = 0.0039\n","Epoch 30: Loss = 2.6354, Acc = 0.0039\n","Epoch 30: Loss = 3.2854, Acc = 0.0078\n","Epoch 30: Loss = 3.9446, Acc = 0.0098\n","Epoch 30: Loss = 4.5957, Acc = 0.0117\n","Epoch 30: Loss = 5.2536, Acc = 0.0117\n","Epoch 31: Loss = 0.6510, Acc = 0.0000\n","Epoch 31: Loss = 1.3207, Acc = 0.0039\n","Epoch 31: Loss = 1.9571, Acc = 0.0098\n","Epoch 31: Loss = 2.6176, Acc = 0.0098\n","Epoch 31: Loss = 3.2714, Acc = 0.0117\n","Epoch 31: Loss = 3.9353, Acc = 0.0137\n","Epoch 31: Loss = 4.5832, Acc = 0.0137\n","Epoch 31: Loss = 5.2552, Acc = 0.0137\n","Epoch 32: Loss = 0.6461, Acc = 0.0039\n","Epoch 32: Loss = 1.3142, Acc = 0.0059\n","Epoch 32: Loss = 1.9745, Acc = 0.0059\n","Epoch 32: Loss = 2.6358, Acc = 0.0059\n","Epoch 32: Loss = 3.2878, Acc = 0.0137\n","Epoch 32: Loss = 3.9517, Acc = 0.0137\n","Epoch 32: Loss = 4.6006, Acc = 0.0137\n","Epoch 32: Loss = 5.2532, Acc = 0.0137\n","Epoch 33: Loss = 0.6580, Acc = 0.0020\n","Epoch 33: Loss = 1.3139, Acc = 0.0020\n","Epoch 33: Loss = 1.9654, Acc = 0.0059\n","Epoch 33: Loss = 2.6216, Acc = 0.0059\n","Epoch 33: Loss = 3.2692, Acc = 0.0098\n","Epoch 33: Loss = 3.9399, Acc = 0.0137\n","Epoch 33: Loss = 4.6038, Acc = 0.0137\n","Epoch 33: Loss = 5.2529, Acc = 0.0176\n","Epoch 34: Loss = 0.6663, Acc = 0.0000\n","Epoch 34: Loss = 1.3223, Acc = 0.0020\n","Epoch 34: Loss = 1.9793, Acc = 0.0039\n","Epoch 34: Loss = 2.6324, Acc = 0.0039\n","Epoch 34: Loss = 3.2857, Acc = 0.0039\n","Epoch 34: Loss = 3.9319, Acc = 0.0039\n","Epoch 34: Loss = 4.5814, Acc = 0.0078\n","Epoch 34: Loss = 5.2528, Acc = 0.0078\n","Epoch 35: Loss = 0.6643, Acc = 0.0000\n","Epoch 35: Loss = 1.3194, Acc = 0.0020\n","Epoch 35: Loss = 1.9806, Acc = 0.0039\n","Epoch 35: Loss = 2.6389, Acc = 0.0059\n","Epoch 35: Loss = 3.2872, Acc = 0.0059\n","Epoch 35: Loss = 3.9352, Acc = 0.0059\n","Epoch 35: Loss = 4.5884, Acc = 0.0059\n","Epoch 35: Loss = 5.2529, Acc = 0.0059\n","Epoch 36: Loss = 0.6430, Acc = 0.0020\n","Epoch 36: Loss = 1.2960, Acc = 0.0039\n","Epoch 36: Loss = 1.9620, Acc = 0.0078\n","Epoch 36: Loss = 2.6251, Acc = 0.0078\n","Epoch 36: Loss = 3.2695, Acc = 0.0078\n","Epoch 36: Loss = 3.9205, Acc = 0.0078\n","Epoch 36: Loss = 4.5872, Acc = 0.0117\n","Epoch 36: Loss = 5.2528, Acc = 0.0137\n","Epoch 37: Loss = 0.6645, Acc = 0.0039\n","Epoch 37: Loss = 1.3125, Acc = 0.0059\n","Epoch 37: Loss = 1.9640, Acc = 0.0098\n","Epoch 37: Loss = 2.6155, Acc = 0.0117\n","Epoch 37: Loss = 3.2806, Acc = 0.0117\n","Epoch 37: Loss = 3.9308, Acc = 0.0137\n","Epoch 37: Loss = 4.5926, Acc = 0.0137\n","Epoch 37: Loss = 5.2531, Acc = 0.0137\n","Epoch 38: Loss = 0.6441, Acc = 0.0000\n","Epoch 38: Loss = 1.3042, Acc = 0.0000\n","Epoch 38: Loss = 1.9608, Acc = 0.0000\n","Epoch 38: Loss = 2.6110, Acc = 0.0039\n","Epoch 38: Loss = 3.2569, Acc = 0.0039\n","Epoch 38: Loss = 3.9203, Acc = 0.0078\n","Epoch 38: Loss = 4.5825, Acc = 0.0098\n","Epoch 38: Loss = 5.2529, Acc = 0.0137\n","Epoch 39: Loss = 0.6609, Acc = 0.0000\n","Epoch 39: Loss = 1.3249, Acc = 0.0000\n","Epoch 39: Loss = 1.9847, Acc = 0.0020\n","Epoch 39: Loss = 2.6372, Acc = 0.0039\n","Epoch 39: Loss = 3.2939, Acc = 0.0039\n","Epoch 39: Loss = 3.9476, Acc = 0.0098\n","Epoch 39: Loss = 4.5907, Acc = 0.0137\n","Epoch 39: Loss = 5.2530, Acc = 0.0137\n","Epoch 40: Loss = 0.6633, Acc = 0.0020\n","Epoch 40: Loss = 1.3133, Acc = 0.0020\n","Epoch 40: Loss = 1.9726, Acc = 0.0020\n","Epoch 40: Loss = 2.6182, Acc = 0.0039\n","Epoch 40: Loss = 3.2753, Acc = 0.0039\n","Epoch 40: Loss = 3.9249, Acc = 0.0098\n","Epoch 40: Loss = 4.5918, Acc = 0.0098\n","Epoch 40: Loss = 5.2517, Acc = 0.0098\n","Epoch 41: Loss = 0.6577, Acc = 0.0039\n","Epoch 41: Loss = 1.3028, Acc = 0.0059\n","Epoch 41: Loss = 1.9667, Acc = 0.0078\n","Epoch 41: Loss = 2.6154, Acc = 0.0098\n","Epoch 41: Loss = 3.2687, Acc = 0.0117\n","Epoch 41: Loss = 3.9370, Acc = 0.0117\n","Epoch 41: Loss = 4.5907, Acc = 0.0117\n","Epoch 41: Loss = 5.2517, Acc = 0.0117\n","Epoch 42: Loss = 0.6477, Acc = 0.0039\n","Epoch 42: Loss = 1.3159, Acc = 0.0039\n","Epoch 42: Loss = 1.9820, Acc = 0.0059\n","Epoch 42: Loss = 2.6324, Acc = 0.0078\n","Epoch 42: Loss = 3.2892, Acc = 0.0117\n","Epoch 42: Loss = 3.9493, Acc = 0.0137\n","Epoch 42: Loss = 4.6001, Acc = 0.0137\n","Epoch 42: Loss = 5.2519, Acc = 0.0137\n","Epoch 43: Loss = 0.6471, Acc = 0.0000\n","Epoch 43: Loss = 1.3032, Acc = 0.0000\n","Epoch 43: Loss = 1.9468, Acc = 0.0020\n","Epoch 43: Loss = 2.6112, Acc = 0.0020\n","Epoch 43: Loss = 3.2615, Acc = 0.0059\n","Epoch 43: Loss = 3.9114, Acc = 0.0059\n","Epoch 43: Loss = 4.5746, Acc = 0.0059\n","Epoch 43: Loss = 5.2529, Acc = 0.0059\n","Epoch 44: Loss = 0.6538, Acc = 0.0020\n","Epoch 44: Loss = 1.3039, Acc = 0.0020\n","Epoch 44: Loss = 1.9497, Acc = 0.0039\n","Epoch 44: Loss = 2.6183, Acc = 0.0039\n","Epoch 44: Loss = 3.2784, Acc = 0.0039\n","Epoch 44: Loss = 3.9406, Acc = 0.0059\n","Epoch 44: Loss = 4.5990, Acc = 0.0059\n","Epoch 44: Loss = 5.2509, Acc = 0.0059\n","Epoch 45: Loss = 0.6553, Acc = 0.0039\n","Epoch 45: Loss = 1.3107, Acc = 0.0039\n","Epoch 45: Loss = 1.9615, Acc = 0.0039\n","Epoch 45: Loss = 2.6291, Acc = 0.0039\n","Epoch 45: Loss = 3.2860, Acc = 0.0039\n","Epoch 45: Loss = 3.9285, Acc = 0.0078\n","Epoch 45: Loss = 4.5839, Acc = 0.0098\n","Epoch 45: Loss = 5.2521, Acc = 0.0098\n","Epoch 46: Loss = 0.6428, Acc = 0.0020\n","Epoch 46: Loss = 1.3056, Acc = 0.0078\n","Epoch 46: Loss = 1.9677, Acc = 0.0078\n","Epoch 46: Loss = 2.6197, Acc = 0.0098\n","Epoch 46: Loss = 3.2888, Acc = 0.0098\n","Epoch 46: Loss = 3.9378, Acc = 0.0098\n","Epoch 46: Loss = 4.5967, Acc = 0.0137\n","Epoch 46: Loss = 5.2532, Acc = 0.0137\n","Epoch 47: Loss = 0.6671, Acc = 0.0000\n","Epoch 47: Loss = 1.3281, Acc = 0.0020\n","Epoch 47: Loss = 1.9657, Acc = 0.0098\n","Epoch 47: Loss = 2.6138, Acc = 0.0137\n","Epoch 47: Loss = 3.2750, Acc = 0.0137\n","Epoch 47: Loss = 3.9381, Acc = 0.0137\n","Epoch 47: Loss = 4.5938, Acc = 0.0137\n","Epoch 47: Loss = 5.2527, Acc = 0.0137\n","Epoch 48: Loss = 0.6578, Acc = 0.0000\n","Epoch 48: Loss = 1.3183, Acc = 0.0020\n","Epoch 48: Loss = 1.9769, Acc = 0.0059\n","Epoch 48: Loss = 2.6354, Acc = 0.0078\n","Epoch 48: Loss = 3.2780, Acc = 0.0098\n","Epoch 48: Loss = 3.9365, Acc = 0.0117\n","Epoch 48: Loss = 4.5999, Acc = 0.0137\n","Epoch 48: Loss = 5.2512, Acc = 0.0137\n","Epoch 49: Loss = 0.6465, Acc = 0.0039\n","Epoch 49: Loss = 1.3027, Acc = 0.0039\n","Epoch 49: Loss = 1.9646, Acc = 0.0059\n","Epoch 49: Loss = 2.6191, Acc = 0.0059\n","Epoch 49: Loss = 3.2729, Acc = 0.0078\n","Epoch 49: Loss = 3.9348, Acc = 0.0098\n","Epoch 49: Loss = 4.5910, Acc = 0.0098\n","Epoch 49: Loss = 5.2509, Acc = 0.0137\n","Epoch 50: Loss = 0.6622, Acc = 0.0020\n","Epoch 50: Loss = 1.3075, Acc = 0.0020\n","Epoch 50: Loss = 1.9719, Acc = 0.0039\n","Epoch 50: Loss = 2.6167, Acc = 0.0039\n","Epoch 50: Loss = 3.2842, Acc = 0.0039\n","Epoch 50: Loss = 3.9502, Acc = 0.0059\n","Epoch 50: Loss = 4.6097, Acc = 0.0078\n","Epoch 50: Loss = 5.2512, Acc = 0.0117\n","Epoch 51: Loss = 0.6482, Acc = 0.0000\n","Epoch 51: Loss = 1.3070, Acc = 0.0000\n","Epoch 51: Loss = 1.9595, Acc = 0.0039\n","Epoch 51: Loss = 2.6182, Acc = 0.0039\n","Epoch 51: Loss = 3.2799, Acc = 0.0039\n","Epoch 51: Loss = 3.9232, Acc = 0.0039\n","Epoch 51: Loss = 4.5948, Acc = 0.0059\n","Epoch 51: Loss = 5.2514, Acc = 0.0078\n","Epoch 52: Loss = 0.6573, Acc = 0.0000\n","Epoch 52: Loss = 1.3091, Acc = 0.0039\n","Epoch 52: Loss = 1.9760, Acc = 0.0039\n","Epoch 52: Loss = 2.6253, Acc = 0.0059\n","Epoch 52: Loss = 3.2881, Acc = 0.0059\n","Epoch 52: Loss = 3.9384, Acc = 0.0078\n","Epoch 52: Loss = 4.5878, Acc = 0.0117\n","Epoch 52: Loss = 5.2518, Acc = 0.0117\n","Epoch 53: Loss = 0.6626, Acc = 0.0020\n","Epoch 53: Loss = 1.3126, Acc = 0.0039\n","Epoch 53: Loss = 1.9677, Acc = 0.0039\n","Epoch 53: Loss = 2.6221, Acc = 0.0078\n","Epoch 53: Loss = 3.2681, Acc = 0.0098\n","Epoch 53: Loss = 3.9134, Acc = 0.0117\n","Epoch 53: Loss = 4.5747, Acc = 0.0137\n","Epoch 53: Loss = 5.2523, Acc = 0.0137\n","Epoch 54: Loss = 0.6550, Acc = 0.0000\n","Epoch 54: Loss = 1.3033, Acc = 0.0020\n","Epoch 54: Loss = 1.9642, Acc = 0.0020\n","Epoch 54: Loss = 2.6338, Acc = 0.0020\n","Epoch 54: Loss = 3.2887, Acc = 0.0059\n","Epoch 54: Loss = 3.9489, Acc = 0.0059\n","Epoch 54: Loss = 4.6030, Acc = 0.0059\n","Epoch 54: Loss = 5.2508, Acc = 0.0098\n","Epoch 55: Loss = 0.6479, Acc = 0.0059\n","Epoch 55: Loss = 1.3092, Acc = 0.0098\n","Epoch 55: Loss = 1.9644, Acc = 0.0098\n","Epoch 55: Loss = 2.6160, Acc = 0.0137\n","Epoch 55: Loss = 3.2883, Acc = 0.0137\n","Epoch 55: Loss = 3.9329, Acc = 0.0137\n","Epoch 55: Loss = 4.5809, Acc = 0.0137\n","Epoch 55: Loss = 5.2512, Acc = 0.0137\n","Epoch 56: Loss = 0.6552, Acc = 0.0039\n","Epoch 56: Loss = 1.3160, Acc = 0.0059\n","Epoch 56: Loss = 1.9588, Acc = 0.0078\n","Epoch 56: Loss = 2.6145, Acc = 0.0098\n","Epoch 56: Loss = 3.2787, Acc = 0.0117\n","Epoch 56: Loss = 3.9248, Acc = 0.0137\n","Epoch 56: Loss = 4.5838, Acc = 0.0137\n","Epoch 56: Loss = 5.2524, Acc = 0.0137\n","Epoch 57: Loss = 0.6558, Acc = 0.0000\n","Epoch 57: Loss = 1.3079, Acc = 0.0039\n","Epoch 57: Loss = 1.9492, Acc = 0.0059\n","Epoch 57: Loss = 2.5979, Acc = 0.0059\n","Epoch 57: Loss = 3.2533, Acc = 0.0059\n","Epoch 57: Loss = 3.9113, Acc = 0.0078\n","Epoch 57: Loss = 4.5830, Acc = 0.0078\n","Epoch 57: Loss = 5.2506, Acc = 0.0098\n","Epoch 58: Loss = 0.6592, Acc = 0.0020\n","Epoch 58: Loss = 1.3158, Acc = 0.0020\n","Epoch 58: Loss = 1.9732, Acc = 0.0020\n","Epoch 58: Loss = 2.6257, Acc = 0.0020\n","Epoch 58: Loss = 3.2642, Acc = 0.0059\n","Epoch 58: Loss = 3.9356, Acc = 0.0059\n","Epoch 58: Loss = 4.6043, Acc = 0.0059\n","Epoch 58: Loss = 5.2516, Acc = 0.0059\n","Epoch 59: Loss = 0.6544, Acc = 0.0020\n","Epoch 59: Loss = 1.3114, Acc = 0.0039\n","Epoch 59: Loss = 1.9566, Acc = 0.0059\n","Epoch 59: Loss = 2.6048, Acc = 0.0059\n","Epoch 59: Loss = 3.2626, Acc = 0.0059\n","Epoch 59: Loss = 3.9244, Acc = 0.0059\n","Epoch 59: Loss = 4.5901, Acc = 0.0078\n","Epoch 59: Loss = 5.2509, Acc = 0.0098\n","Epoch 60: Loss = 0.6523, Acc = 0.0039\n","Epoch 60: Loss = 1.3085, Acc = 0.0039\n","Epoch 60: Loss = 1.9644, Acc = 0.0039\n","Epoch 60: Loss = 2.6008, Acc = 0.0098\n","Epoch 60: Loss = 3.2682, Acc = 0.0098\n","Epoch 60: Loss = 3.9270, Acc = 0.0098\n","Epoch 60: Loss = 4.5964, Acc = 0.0137\n","Epoch 60: Loss = 5.2505, Acc = 0.0137\n","Epoch 61: Loss = 0.6657, Acc = 0.0000\n","Epoch 61: Loss = 1.3206, Acc = 0.0000\n","Epoch 61: Loss = 1.9852, Acc = 0.0020\n","Epoch 61: Loss = 2.6390, Acc = 0.0020\n","Epoch 61: Loss = 3.2897, Acc = 0.0039\n","Epoch 61: Loss = 3.9281, Acc = 0.0078\n","Epoch 61: Loss = 4.6043, Acc = 0.0078\n","Epoch 61: Loss = 5.2514, Acc = 0.0117\n","Epoch 62: Loss = 0.6634, Acc = 0.0020\n","Epoch 62: Loss = 1.3159, Acc = 0.0020\n","Epoch 62: Loss = 1.9640, Acc = 0.0059\n","Epoch 62: Loss = 2.6230, Acc = 0.0059\n","Epoch 62: Loss = 3.2774, Acc = 0.0059\n","Epoch 62: Loss = 3.9325, Acc = 0.0078\n","Epoch 62: Loss = 4.5948, Acc = 0.0117\n","Epoch 62: Loss = 5.2497, Acc = 0.0137\n","Epoch 63: Loss = 0.6473, Acc = 0.0039\n","Epoch 63: Loss = 1.2986, Acc = 0.0078\n","Epoch 63: Loss = 1.9453, Acc = 0.0098\n","Epoch 63: Loss = 2.6076, Acc = 0.0098\n","Epoch 63: Loss = 3.2601, Acc = 0.0098\n","Epoch 63: Loss = 3.9237, Acc = 0.0117\n","Epoch 63: Loss = 4.5668, Acc = 0.0117\n","Epoch 63: Loss = 5.2506, Acc = 0.0117\n","Epoch 64: Loss = 0.6439, Acc = 0.0020\n","Epoch 64: Loss = 1.2961, Acc = 0.0059\n","Epoch 64: Loss = 1.9505, Acc = 0.0078\n","Epoch 64: Loss = 2.6097, Acc = 0.0078\n","Epoch 64: Loss = 3.2611, Acc = 0.0117\n","Epoch 64: Loss = 3.9265, Acc = 0.0117\n","Epoch 64: Loss = 4.5819, Acc = 0.0117\n","Epoch 64: Loss = 5.2507, Acc = 0.0117\n","Epoch 65: Loss = 0.6393, Acc = 0.0020\n","Epoch 65: Loss = 1.3012, Acc = 0.0020\n","Epoch 65: Loss = 1.9488, Acc = 0.0020\n","Epoch 65: Loss = 2.6014, Acc = 0.0020\n","Epoch 65: Loss = 3.2521, Acc = 0.0039\n","Epoch 65: Loss = 3.9121, Acc = 0.0039\n","Epoch 65: Loss = 4.5861, Acc = 0.0039\n","Epoch 65: Loss = 5.2495, Acc = 0.0059\n","Epoch 66: Loss = 0.6583, Acc = 0.0000\n","Epoch 66: Loss = 1.3025, Acc = 0.0020\n","Epoch 66: Loss = 1.9563, Acc = 0.0059\n","Epoch 66: Loss = 2.6215, Acc = 0.0059\n","Epoch 66: Loss = 3.2723, Acc = 0.0078\n","Epoch 66: Loss = 3.9327, Acc = 0.0078\n","Epoch 66: Loss = 4.5819, Acc = 0.0078\n","Epoch 66: Loss = 5.2500, Acc = 0.0117\n","Epoch 67: Loss = 0.6401, Acc = 0.0078\n","Epoch 67: Loss = 1.2966, Acc = 0.0098\n","Epoch 67: Loss = 1.9460, Acc = 0.0098\n","Epoch 67: Loss = 2.6119, Acc = 0.0098\n","Epoch 67: Loss = 3.2669, Acc = 0.0117\n","Epoch 67: Loss = 3.9255, Acc = 0.0117\n","Epoch 67: Loss = 4.5959, Acc = 0.0117\n","Epoch 67: Loss = 5.2513, Acc = 0.0137\n","Epoch 68: Loss = 0.6570, Acc = 0.0039\n","Epoch 68: Loss = 1.3182, Acc = 0.0039\n","Epoch 68: Loss = 1.9704, Acc = 0.0078\n","Epoch 68: Loss = 2.6234, Acc = 0.0098\n","Epoch 68: Loss = 3.2840, Acc = 0.0098\n","Epoch 68: Loss = 3.9425, Acc = 0.0098\n","Epoch 68: Loss = 4.5867, Acc = 0.0137\n","Epoch 68: Loss = 5.2498, Acc = 0.0137\n","Epoch 69: Loss = 0.6625, Acc = 0.0000\n","Epoch 69: Loss = 1.3217, Acc = 0.0039\n","Epoch 69: Loss = 1.9827, Acc = 0.0059\n","Epoch 69: Loss = 2.6322, Acc = 0.0078\n","Epoch 69: Loss = 3.2820, Acc = 0.0078\n","Epoch 69: Loss = 3.9325, Acc = 0.0098\n","Epoch 69: Loss = 4.5858, Acc = 0.0098\n","Epoch 69: Loss = 5.2507, Acc = 0.0137\n","Epoch 70: Loss = 0.6563, Acc = 0.0039\n","Epoch 70: Loss = 1.3078, Acc = 0.0078\n","Epoch 70: Loss = 1.9638, Acc = 0.0098\n","Epoch 70: Loss = 2.6135, Acc = 0.0098\n","Epoch 70: Loss = 3.2793, Acc = 0.0098\n","Epoch 70: Loss = 3.9416, Acc = 0.0098\n","Epoch 70: Loss = 4.5989, Acc = 0.0117\n","Epoch 70: Loss = 5.2495, Acc = 0.0137\n","Epoch 71: Loss = 0.6576, Acc = 0.0000\n","Epoch 71: Loss = 1.3193, Acc = 0.0020\n","Epoch 71: Loss = 1.9730, Acc = 0.0039\n","Epoch 71: Loss = 2.6354, Acc = 0.0039\n","Epoch 71: Loss = 3.2865, Acc = 0.0078\n","Epoch 71: Loss = 3.9426, Acc = 0.0078\n","Epoch 71: Loss = 4.5968, Acc = 0.0078\n","Epoch 71: Loss = 5.2501, Acc = 0.0098\n","Epoch 72: Loss = 0.6465, Acc = 0.0039\n","Epoch 72: Loss = 1.3116, Acc = 0.0039\n","Epoch 72: Loss = 1.9596, Acc = 0.0078\n","Epoch 72: Loss = 2.6121, Acc = 0.0078\n","Epoch 72: Loss = 3.2756, Acc = 0.0078\n","Epoch 72: Loss = 3.9258, Acc = 0.0098\n","Epoch 72: Loss = 4.5878, Acc = 0.0098\n","Epoch 72: Loss = 5.2501, Acc = 0.0137\n","Epoch 73: Loss = 0.6465, Acc = 0.0039\n","Epoch 73: Loss = 1.3137, Acc = 0.0039\n","Epoch 73: Loss = 1.9665, Acc = 0.0059\n","Epoch 73: Loss = 2.6262, Acc = 0.0059\n","Epoch 73: Loss = 3.2820, Acc = 0.0059\n","Epoch 73: Loss = 3.9421, Acc = 0.0059\n","Epoch 73: Loss = 4.5922, Acc = 0.0078\n","Epoch 73: Loss = 5.2505, Acc = 0.0078\n","Epoch 74: Loss = 0.6586, Acc = 0.0000\n","Epoch 74: Loss = 1.2939, Acc = 0.0039\n","Epoch 74: Loss = 1.9532, Acc = 0.0059\n","Epoch 74: Loss = 2.6164, Acc = 0.0078\n","Epoch 74: Loss = 3.2722, Acc = 0.0078\n","Epoch 74: Loss = 3.9342, Acc = 0.0078\n","Epoch 74: Loss = 4.5875, Acc = 0.0078\n","Epoch 74: Loss = 5.2506, Acc = 0.0078\n","Epoch 75: Loss = 0.6526, Acc = 0.0000\n","Epoch 75: Loss = 1.3077, Acc = 0.0039\n","Epoch 75: Loss = 1.9609, Acc = 0.0098\n","Epoch 75: Loss = 2.6193, Acc = 0.0117\n","Epoch 75: Loss = 3.2774, Acc = 0.0117\n","Epoch 75: Loss = 3.9426, Acc = 0.0117\n","Epoch 75: Loss = 4.5992, Acc = 0.0117\n","Epoch 75: Loss = 5.2498, Acc = 0.0137\n","Epoch 76: Loss = 0.6552, Acc = 0.0020\n","Epoch 76: Loss = 1.3185, Acc = 0.0039\n","Epoch 76: Loss = 1.9730, Acc = 0.0059\n","Epoch 76: Loss = 2.6268, Acc = 0.0059\n","Epoch 76: Loss = 3.2754, Acc = 0.0059\n","Epoch 76: Loss = 3.9389, Acc = 0.0059\n","Epoch 76: Loss = 4.6010, Acc = 0.0098\n","Epoch 76: Loss = 5.2489, Acc = 0.0098\n","Epoch 77: Loss = 0.6560, Acc = 0.0000\n","Epoch 77: Loss = 1.3137, Acc = 0.0000\n","Epoch 77: Loss = 1.9568, Acc = 0.0000\n","Epoch 77: Loss = 2.6178, Acc = 0.0000\n","Epoch 77: Loss = 3.2694, Acc = 0.0039\n","Epoch 77: Loss = 3.9282, Acc = 0.0059\n","Epoch 77: Loss = 4.5811, Acc = 0.0078\n","Epoch 77: Loss = 5.2499, Acc = 0.0078\n","Epoch 78: Loss = 0.6620, Acc = 0.0000\n","Epoch 78: Loss = 1.3078, Acc = 0.0020\n","Epoch 78: Loss = 1.9576, Acc = 0.0039\n","Epoch 78: Loss = 2.6083, Acc = 0.0039\n","Epoch 78: Loss = 3.2785, Acc = 0.0059\n","Epoch 78: Loss = 3.9365, Acc = 0.0098\n","Epoch 78: Loss = 4.5794, Acc = 0.0098\n","Epoch 78: Loss = 5.2499, Acc = 0.0098\n","Epoch 79: Loss = 0.6450, Acc = 0.0039\n","Epoch 79: Loss = 1.3094, Acc = 0.0059\n","Epoch 79: Loss = 1.9625, Acc = 0.0059\n","Epoch 79: Loss = 2.6197, Acc = 0.0059\n","Epoch 79: Loss = 3.2779, Acc = 0.0078\n","Epoch 79: Loss = 3.9353, Acc = 0.0098\n","Epoch 79: Loss = 4.5876, Acc = 0.0137\n","Epoch 79: Loss = 5.2493, Acc = 0.0137\n","Epoch 80: Loss = 0.6471, Acc = 0.0000\n","Epoch 80: Loss = 1.3089, Acc = 0.0000\n","Epoch 80: Loss = 1.9641, Acc = 0.0000\n","Epoch 80: Loss = 2.6296, Acc = 0.0039\n","Epoch 80: Loss = 3.2683, Acc = 0.0039\n","Epoch 80: Loss = 3.9253, Acc = 0.0059\n","Epoch 80: Loss = 4.5881, Acc = 0.0059\n","Epoch 80: Loss = 5.2502, Acc = 0.0059\n","Epoch 81: Loss = 0.6438, Acc = 0.0020\n","Epoch 81: Loss = 1.3038, Acc = 0.0020\n","Epoch 81: Loss = 1.9543, Acc = 0.0020\n","Epoch 81: Loss = 2.6121, Acc = 0.0020\n","Epoch 81: Loss = 3.2679, Acc = 0.0020\n","Epoch 81: Loss = 3.9212, Acc = 0.0059\n","Epoch 81: Loss = 4.5853, Acc = 0.0059\n","Epoch 81: Loss = 5.2500, Acc = 0.0059\n","Epoch 82: Loss = 0.6616, Acc = 0.0000\n","Epoch 82: Loss = 1.3097, Acc = 0.0020\n","Epoch 82: Loss = 1.9703, Acc = 0.0039\n","Epoch 82: Loss = 2.6073, Acc = 0.0059\n","Epoch 82: Loss = 3.2618, Acc = 0.0059\n","Epoch 82: Loss = 3.9167, Acc = 0.0078\n","Epoch 82: Loss = 4.5824, Acc = 0.0078\n","Epoch 82: Loss = 5.2504, Acc = 0.0078\n","Epoch 83: Loss = 0.6445, Acc = 0.0039\n","Epoch 83: Loss = 1.2972, Acc = 0.0039\n","Epoch 83: Loss = 1.9471, Acc = 0.0059\n","Epoch 83: Loss = 2.6191, Acc = 0.0059\n","Epoch 83: Loss = 3.2615, Acc = 0.0059\n","Epoch 83: Loss = 3.9117, Acc = 0.0078\n","Epoch 83: Loss = 4.5775, Acc = 0.0098\n","Epoch 83: Loss = 5.2502, Acc = 0.0098\n","Epoch 84: Loss = 0.6648, Acc = 0.0000\n","Epoch 84: Loss = 1.3226, Acc = 0.0020\n","Epoch 84: Loss = 1.9739, Acc = 0.0059\n","Epoch 84: Loss = 2.6338, Acc = 0.0078\n","Epoch 84: Loss = 3.2764, Acc = 0.0098\n","Epoch 84: Loss = 3.9358, Acc = 0.0098\n","Epoch 84: Loss = 4.5967, Acc = 0.0098\n","Epoch 84: Loss = 5.2492, Acc = 0.0117\n","Epoch 85: Loss = 0.6465, Acc = 0.0000\n","Epoch 85: Loss = 1.2993, Acc = 0.0039\n","Epoch 85: Loss = 1.9559, Acc = 0.0078\n","Epoch 85: Loss = 2.6186, Acc = 0.0078\n","Epoch 85: Loss = 3.2834, Acc = 0.0117\n","Epoch 85: Loss = 3.9345, Acc = 0.0117\n","Epoch 85: Loss = 4.5927, Acc = 0.0117\n","Epoch 85: Loss = 5.2494, Acc = 0.0137\n","Epoch 86: Loss = 0.6615, Acc = 0.0039\n","Epoch 86: Loss = 1.3140, Acc = 0.0059\n","Epoch 86: Loss = 1.9722, Acc = 0.0078\n","Epoch 86: Loss = 2.6205, Acc = 0.0078\n","Epoch 86: Loss = 3.2763, Acc = 0.0098\n","Epoch 86: Loss = 3.9354, Acc = 0.0117\n","Epoch 86: Loss = 4.5915, Acc = 0.0137\n","Epoch 86: Loss = 5.2502, Acc = 0.0137\n","Epoch 87: Loss = 0.6536, Acc = 0.0039\n","Epoch 87: Loss = 1.3172, Acc = 0.0039\n","Epoch 87: Loss = 1.9710, Acc = 0.0039\n","Epoch 87: Loss = 2.6173, Acc = 0.0039\n","Epoch 87: Loss = 3.2817, Acc = 0.0059\n","Epoch 87: Loss = 3.9301, Acc = 0.0059\n","Epoch 87: Loss = 4.5901, Acc = 0.0059\n","Epoch 87: Loss = 5.2504, Acc = 0.0059\n","Epoch 88: Loss = 0.6517, Acc = 0.0020\n","Epoch 88: Loss = 1.3120, Acc = 0.0059\n","Epoch 88: Loss = 1.9637, Acc = 0.0098\n","Epoch 88: Loss = 2.6039, Acc = 0.0117\n","Epoch 88: Loss = 3.2584, Acc = 0.0137\n","Epoch 88: Loss = 3.9181, Acc = 0.0137\n","Epoch 88: Loss = 4.5870, Acc = 0.0137\n","Epoch 88: Loss = 5.2502, Acc = 0.0137\n","Epoch 89: Loss = 0.6578, Acc = 0.0039\n","Epoch 89: Loss = 1.2975, Acc = 0.0039\n","Epoch 89: Loss = 1.9413, Acc = 0.0039\n","Epoch 89: Loss = 2.6043, Acc = 0.0059\n","Epoch 89: Loss = 3.2664, Acc = 0.0098\n","Epoch 89: Loss = 3.9184, Acc = 0.0098\n","Epoch 89: Loss = 4.5832, Acc = 0.0098\n","Epoch 89: Loss = 5.2512, Acc = 0.0098\n","Epoch 90: Loss = 0.6455, Acc = 0.0000\n","Epoch 90: Loss = 1.2943, Acc = 0.0039\n","Epoch 90: Loss = 1.9550, Acc = 0.0059\n","Epoch 90: Loss = 2.6103, Acc = 0.0098\n","Epoch 90: Loss = 3.2779, Acc = 0.0098\n","Epoch 90: Loss = 3.9306, Acc = 0.0117\n","Epoch 90: Loss = 4.5788, Acc = 0.0137\n","Epoch 90: Loss = 5.2495, Acc = 0.0137\n","Epoch 91: Loss = 0.6498, Acc = 0.0000\n","Epoch 91: Loss = 1.3140, Acc = 0.0000\n","Epoch 91: Loss = 1.9790, Acc = 0.0000\n","Epoch 91: Loss = 2.6305, Acc = 0.0000\n","Epoch 91: Loss = 3.2890, Acc = 0.0000\n","Epoch 91: Loss = 3.9487, Acc = 0.0020\n","Epoch 91: Loss = 4.5975, Acc = 0.0020\n","Epoch 91: Loss = 5.2510, Acc = 0.0020\n","Epoch 92: Loss = 0.6613, Acc = 0.0020\n","Epoch 92: Loss = 1.3077, Acc = 0.0059\n","Epoch 92: Loss = 1.9581, Acc = 0.0098\n","Epoch 92: Loss = 2.6209, Acc = 0.0098\n","Epoch 92: Loss = 3.2832, Acc = 0.0098\n","Epoch 92: Loss = 3.9310, Acc = 0.0117\n","Epoch 92: Loss = 4.5875, Acc = 0.0137\n","Epoch 92: Loss = 5.2489, Acc = 0.0137\n","Epoch 93: Loss = 0.6588, Acc = 0.0020\n","Epoch 93: Loss = 1.3139, Acc = 0.0020\n","Epoch 93: Loss = 1.9680, Acc = 0.0059\n","Epoch 93: Loss = 2.6183, Acc = 0.0059\n","Epoch 93: Loss = 3.2800, Acc = 0.0059\n","Epoch 93: Loss = 3.9379, Acc = 0.0078\n","Epoch 93: Loss = 4.5905, Acc = 0.0117\n","Epoch 93: Loss = 5.2504, Acc = 0.0117\n","Epoch 94: Loss = 0.6593, Acc = 0.0000\n","Epoch 94: Loss = 1.3220, Acc = 0.0000\n","Epoch 94: Loss = 1.9794, Acc = 0.0000\n","Epoch 94: Loss = 2.6256, Acc = 0.0020\n","Epoch 94: Loss = 3.2699, Acc = 0.0059\n","Epoch 94: Loss = 3.9300, Acc = 0.0059\n","Epoch 94: Loss = 4.5987, Acc = 0.0059\n","Epoch 94: Loss = 5.2501, Acc = 0.0078\n","Epoch 95: Loss = 0.6461, Acc = 0.0000\n","Epoch 95: Loss = 1.3081, Acc = 0.0020\n","Epoch 95: Loss = 1.9701, Acc = 0.0020\n","Epoch 95: Loss = 2.6213, Acc = 0.0020\n","Epoch 95: Loss = 3.2814, Acc = 0.0059\n","Epoch 95: Loss = 3.9325, Acc = 0.0078\n","Epoch 95: Loss = 4.5821, Acc = 0.0078\n","Epoch 95: Loss = 5.2494, Acc = 0.0098\n","Epoch 96: Loss = 0.6551, Acc = 0.0020\n","Epoch 96: Loss = 1.3123, Acc = 0.0059\n","Epoch 96: Loss = 1.9583, Acc = 0.0078\n","Epoch 96: Loss = 2.6304, Acc = 0.0078\n","Epoch 96: Loss = 3.2956, Acc = 0.0098\n","Epoch 96: Loss = 3.9471, Acc = 0.0098\n","Epoch 96: Loss = 4.5961, Acc = 0.0117\n","Epoch 96: Loss = 5.2491, Acc = 0.0117\n","Epoch 97: Loss = 0.6627, Acc = 0.0039\n","Epoch 97: Loss = 1.3216, Acc = 0.0059\n","Epoch 97: Loss = 1.9740, Acc = 0.0078\n","Epoch 97: Loss = 2.6422, Acc = 0.0078\n","Epoch 97: Loss = 3.2912, Acc = 0.0078\n","Epoch 97: Loss = 3.9320, Acc = 0.0078\n","Epoch 97: Loss = 4.5960, Acc = 0.0098\n","Epoch 97: Loss = 5.2484, Acc = 0.0098\n","Epoch 98: Loss = 0.6506, Acc = 0.0020\n","Epoch 98: Loss = 1.3241, Acc = 0.0039\n","Epoch 98: Loss = 1.9750, Acc = 0.0078\n","Epoch 98: Loss = 2.6352, Acc = 0.0078\n","Epoch 98: Loss = 3.2971, Acc = 0.0078\n","Epoch 98: Loss = 3.9272, Acc = 0.0098\n","Epoch 98: Loss = 4.5899, Acc = 0.0137\n","Epoch 98: Loss = 5.2500, Acc = 0.0137\n","Epoch 99: Loss = 0.6515, Acc = 0.0020\n","Epoch 99: Loss = 1.3001, Acc = 0.0020\n","Epoch 99: Loss = 1.9588, Acc = 0.0020\n","Epoch 99: Loss = 2.5955, Acc = 0.0039\n","Epoch 99: Loss = 3.2564, Acc = 0.0078\n","Epoch 99: Loss = 3.9173, Acc = 0.0078\n","Epoch 99: Loss = 4.5761, Acc = 0.0078\n","Epoch 99: Loss = 5.2489, Acc = 0.0098\n"]}],"source":["model = TransformerSCA(d_model=32, num_heads=2, ff_dim=64, num_layers=2, dropout=0.0).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","\n","  running_loss = 0\n","  correct = 0\n","\n","  for xb, yb in debug_loader:\n","    xb, yb = xb.to(device), yb.to(device)\n","\n","    optimizer.zero_grad()\n","    output = model(xb)\n","    loss = criterion(output, yb)\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item() * xb.size(0)\n","    correct += (output.argmax(1) == yb).sum().item()\n","\n","    acc = correct / len(debug_loader.dataset)\n","    print(f\"Epoch {epoch}: Loss = {running_loss/len(debug_loader.dataset):.4f}, Acc = {acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOHGrevc48p_"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class TinyCNN(nn.Module):\n","    def __init__(self, input_len=700, num_classes=2):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv1d(1, 32, kernel_size=11, padding=5),  # (B, 32, 700)\n","            nn.ReLU(),\n","            nn.Conv1d(32, 64, kernel_size=5, padding=2),  # (B, 64, 700)\n","            nn.ReLU(),\n","            nn.AdaptiveAvgPool1d(1),                      # (B, 64, 1)\n","            nn.Flatten(),                                 # (B, 64)\n","            nn.Linear(64, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaiBJAIG5BOX"},"outputs":[],"source":["model = TinyCNN().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2186,"status":"ok","timestamp":1747346065618,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"RkDhZL6X5W_k","outputId":"6adc4759-f79d-41b4-d257-ee29ccab9430"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: Loss = 0.6944, Acc = 0.0000\n","Epoch 0: Loss = 1.3890, Acc = 0.0000\n","Epoch 0: Loss = 2.0840, Acc = 0.0039\n","Epoch 0: Loss = 2.7772, Acc = 0.0059\n","Epoch 0: Loss = 3.4708, Acc = 0.0059\n","Epoch 0: Loss = 4.1654, Acc = 0.0059\n","Epoch 0: Loss = 4.8606, Acc = 0.0059\n","Epoch 0: Loss = 5.5552, Acc = 0.0078\n","Epoch 1: Loss = 0.6927, Acc = 0.0000\n","Epoch 1: Loss = 1.3870, Acc = 0.0020\n","Epoch 1: Loss = 2.0782, Acc = 0.0039\n","Epoch 1: Loss = 2.7723, Acc = 0.0039\n","Epoch 1: Loss = 3.4644, Acc = 0.0039\n","Epoch 1: Loss = 4.1558, Acc = 0.0039\n","Epoch 1: Loss = 4.8462, Acc = 0.0078\n","Epoch 1: Loss = 5.5408, Acc = 0.0078\n","Epoch 2: Loss = 0.6919, Acc = 0.0020\n","Epoch 2: Loss = 1.3844, Acc = 0.0020\n","Epoch 2: Loss = 2.0732, Acc = 0.0020\n","Epoch 2: Loss = 2.7628, Acc = 0.0059\n","Epoch 2: Loss = 3.4541, Acc = 0.0078\n","Epoch 2: Loss = 4.1432, Acc = 0.0078\n","Epoch 2: Loss = 4.8321, Acc = 0.0078\n","Epoch 2: Loss = 5.5245, Acc = 0.0078\n","Epoch 3: Loss = 0.6888, Acc = 0.0000\n","Epoch 3: Loss = 1.3765, Acc = 0.0039\n","Epoch 3: Loss = 2.0656, Acc = 0.0039\n","Epoch 3: Loss = 2.7521, Acc = 0.0039\n","Epoch 3: Loss = 3.4370, Acc = 0.0039\n","Epoch 3: Loss = 4.1210, Acc = 0.0059\n","Epoch 3: Loss = 4.8103, Acc = 0.0078\n","Epoch 3: Loss = 5.4972, Acc = 0.0098\n","Epoch 4: Loss = 0.6858, Acc = 0.0020\n","Epoch 4: Loss = 1.3696, Acc = 0.0020\n","Epoch 4: Loss = 2.0572, Acc = 0.0020\n","Epoch 4: Loss = 2.7394, Acc = 0.0039\n","Epoch 4: Loss = 3.4212, Acc = 0.0039\n","Epoch 4: Loss = 4.0989, Acc = 0.0059\n","Epoch 4: Loss = 4.7793, Acc = 0.0059\n","Epoch 4: Loss = 5.4557, Acc = 0.0078\n","Epoch 5: Loss = 0.6804, Acc = 0.0000\n","Epoch 5: Loss = 1.3531, Acc = 0.0000\n","Epoch 5: Loss = 2.0283, Acc = 0.0039\n","Epoch 5: Loss = 2.7033, Acc = 0.0039\n","Epoch 5: Loss = 3.3748, Acc = 0.0078\n","Epoch 5: Loss = 4.0554, Acc = 0.0078\n","Epoch 5: Loss = 4.7323, Acc = 0.0078\n","Epoch 5: Loss = 5.4063, Acc = 0.0078\n","Epoch 6: Loss = 0.6651, Acc = 0.0039\n","Epoch 6: Loss = 1.3314, Acc = 0.0059\n","Epoch 6: Loss = 2.0005, Acc = 0.0059\n","Epoch 6: Loss = 2.6704, Acc = 0.0059\n","Epoch 6: Loss = 3.3411, Acc = 0.0078\n","Epoch 6: Loss = 4.0073, Acc = 0.0098\n","Epoch 6: Loss = 4.6814, Acc = 0.0117\n","Epoch 6: Loss = 5.3550, Acc = 0.0137\n","Epoch 7: Loss = 0.6644, Acc = 0.0000\n","Epoch 7: Loss = 1.3374, Acc = 0.0000\n","Epoch 7: Loss = 2.0021, Acc = 0.0000\n","Epoch 7: Loss = 2.6683, Acc = 0.0020\n","Epoch 7: Loss = 3.3366, Acc = 0.0059\n","Epoch 7: Loss = 3.9984, Acc = 0.0117\n","Epoch 7: Loss = 4.6629, Acc = 0.0117\n","Epoch 7: Loss = 5.3155, Acc = 0.0137\n","Epoch 8: Loss = 0.6571, Acc = 0.0000\n","Epoch 8: Loss = 1.3211, Acc = 0.0020\n","Epoch 8: Loss = 1.9753, Acc = 0.0020\n","Epoch 8: Loss = 2.6370, Acc = 0.0059\n","Epoch 8: Loss = 3.2957, Acc = 0.0078\n","Epoch 8: Loss = 3.9665, Acc = 0.0098\n","Epoch 8: Loss = 4.6237, Acc = 0.0137\n","Epoch 8: Loss = 5.2970, Acc = 0.0137\n","Epoch 9: Loss = 0.6522, Acc = 0.0039\n","Epoch 9: Loss = 1.3240, Acc = 0.0039\n","Epoch 9: Loss = 1.9724, Acc = 0.0117\n","Epoch 9: Loss = 2.6410, Acc = 0.0117\n","Epoch 9: Loss = 3.3084, Acc = 0.0117\n","Epoch 9: Loss = 3.9703, Acc = 0.0117\n","Epoch 9: Loss = 4.6305, Acc = 0.0137\n","Epoch 9: Loss = 5.2836, Acc = 0.0137\n","Epoch 10: Loss = 0.6622, Acc = 0.0020\n","Epoch 10: Loss = 1.3169, Acc = 0.0039\n","Epoch 10: Loss = 1.9780, Acc = 0.0039\n","Epoch 10: Loss = 2.6346, Acc = 0.0059\n","Epoch 10: Loss = 3.2894, Acc = 0.0098\n","Epoch 10: Loss = 3.9552, Acc = 0.0117\n","Epoch 10: Loss = 4.6137, Acc = 0.0137\n","Epoch 10: Loss = 5.2736, Acc = 0.0137\n","Epoch 11: Loss = 0.6639, Acc = 0.0020\n","Epoch 11: Loss = 1.3117, Acc = 0.0039\n","Epoch 11: Loss = 1.9617, Acc = 0.0098\n","Epoch 11: Loss = 2.6190, Acc = 0.0098\n","Epoch 11: Loss = 3.2697, Acc = 0.0117\n","Epoch 11: Loss = 3.9282, Acc = 0.0117\n","Epoch 11: Loss = 4.5944, Acc = 0.0137\n","Epoch 11: Loss = 5.2670, Acc = 0.0137\n","Epoch 12: Loss = 0.6393, Acc = 0.0000\n","Epoch 12: Loss = 1.2861, Acc = 0.0039\n","Epoch 12: Loss = 1.9537, Acc = 0.0039\n","Epoch 12: Loss = 2.6106, Acc = 0.0059\n","Epoch 12: Loss = 3.2557, Acc = 0.0098\n","Epoch 12: Loss = 3.9112, Acc = 0.0098\n","Epoch 12: Loss = 4.5738, Acc = 0.0137\n","Epoch 12: Loss = 5.2624, Acc = 0.0137\n","Epoch 13: Loss = 0.6502, Acc = 0.0000\n","Epoch 13: Loss = 1.3073, Acc = 0.0039\n","Epoch 13: Loss = 1.9684, Acc = 0.0039\n","Epoch 13: Loss = 2.6232, Acc = 0.0039\n","Epoch 13: Loss = 3.2725, Acc = 0.0059\n","Epoch 13: Loss = 3.9374, Acc = 0.0098\n","Epoch 13: Loss = 4.5868, Acc = 0.0137\n","Epoch 13: Loss = 5.2601, Acc = 0.0137\n","Epoch 14: Loss = 0.6556, Acc = 0.0020\n","Epoch 14: Loss = 1.3156, Acc = 0.0020\n","Epoch 14: Loss = 1.9737, Acc = 0.0039\n","Epoch 14: Loss = 2.6313, Acc = 0.0078\n","Epoch 14: Loss = 3.2972, Acc = 0.0078\n","Epoch 14: Loss = 3.9437, Acc = 0.0098\n","Epoch 14: Loss = 4.6193, Acc = 0.0098\n","Epoch 14: Loss = 5.2574, Acc = 0.0156\n","Epoch 15: Loss = 0.6519, Acc = 0.0000\n","Epoch 15: Loss = 1.3062, Acc = 0.0039\n","Epoch 15: Loss = 1.9755, Acc = 0.0039\n","Epoch 15: Loss = 2.6328, Acc = 0.0039\n","Epoch 15: Loss = 3.2983, Acc = 0.0039\n","Epoch 15: Loss = 3.9521, Acc = 0.0039\n","Epoch 15: Loss = 4.6112, Acc = 0.0039\n","Epoch 15: Loss = 5.2567, Acc = 0.0039\n","Epoch 16: Loss = 0.6595, Acc = 0.0000\n","Epoch 16: Loss = 1.3120, Acc = 0.0020\n","Epoch 16: Loss = 1.9657, Acc = 0.0059\n","Epoch 16: Loss = 2.6163, Acc = 0.0078\n","Epoch 16: Loss = 3.2644, Acc = 0.0078\n","Epoch 16: Loss = 3.9306, Acc = 0.0078\n","Epoch 16: Loss = 4.5896, Acc = 0.0078\n","Epoch 16: Loss = 5.2557, Acc = 0.0098\n","Epoch 17: Loss = 0.6432, Acc = 0.0020\n","Epoch 17: Loss = 1.2847, Acc = 0.0078\n","Epoch 17: Loss = 1.9431, Acc = 0.0078\n","Epoch 17: Loss = 2.6173, Acc = 0.0078\n","Epoch 17: Loss = 3.2747, Acc = 0.0098\n","Epoch 17: Loss = 3.9391, Acc = 0.0117\n","Epoch 17: Loss = 4.6038, Acc = 0.0117\n","Epoch 17: Loss = 5.2544, Acc = 0.0117\n","Epoch 18: Loss = 0.6608, Acc = 0.0020\n","Epoch 18: Loss = 1.3197, Acc = 0.0039\n","Epoch 18: Loss = 1.9800, Acc = 0.0059\n","Epoch 18: Loss = 2.6338, Acc = 0.0059\n","Epoch 18: Loss = 3.2849, Acc = 0.0059\n","Epoch 18: Loss = 3.9455, Acc = 0.0078\n","Epoch 18: Loss = 4.6026, Acc = 0.0078\n","Epoch 18: Loss = 5.2542, Acc = 0.0098\n","Epoch 19: Loss = 0.6670, Acc = 0.0000\n","Epoch 19: Loss = 1.3279, Acc = 0.0020\n","Epoch 19: Loss = 1.9842, Acc = 0.0039\n","Epoch 19: Loss = 2.6516, Acc = 0.0078\n","Epoch 19: Loss = 3.3041, Acc = 0.0098\n","Epoch 19: Loss = 3.9560, Acc = 0.0098\n","Epoch 19: Loss = 4.6068, Acc = 0.0098\n","Epoch 19: Loss = 5.2536, Acc = 0.0117\n","Epoch 20: Loss = 0.6588, Acc = 0.0059\n","Epoch 20: Loss = 1.3062, Acc = 0.0059\n","Epoch 20: Loss = 1.9490, Acc = 0.0078\n","Epoch 20: Loss = 2.6091, Acc = 0.0078\n","Epoch 20: Loss = 3.2752, Acc = 0.0078\n","Epoch 20: Loss = 3.9381, Acc = 0.0078\n","Epoch 20: Loss = 4.5969, Acc = 0.0098\n","Epoch 20: Loss = 5.2535, Acc = 0.0137\n","Epoch 21: Loss = 0.6519, Acc = 0.0000\n","Epoch 21: Loss = 1.3055, Acc = 0.0039\n","Epoch 21: Loss = 1.9758, Acc = 0.0039\n","Epoch 21: Loss = 2.6332, Acc = 0.0059\n","Epoch 21: Loss = 3.3083, Acc = 0.0078\n","Epoch 21: Loss = 3.9497, Acc = 0.0078\n","Epoch 21: Loss = 4.5925, Acc = 0.0078\n","Epoch 21: Loss = 5.2525, Acc = 0.0098\n","Epoch 22: Loss = 0.6409, Acc = 0.0039\n","Epoch 22: Loss = 1.3125, Acc = 0.0039\n","Epoch 22: Loss = 1.9846, Acc = 0.0039\n","Epoch 22: Loss = 2.6318, Acc = 0.0039\n","Epoch 22: Loss = 3.2831, Acc = 0.0039\n","Epoch 22: Loss = 3.9278, Acc = 0.0059\n","Epoch 22: Loss = 4.5795, Acc = 0.0059\n","Epoch 22: Loss = 5.2529, Acc = 0.0059\n","Epoch 23: Loss = 0.6504, Acc = 0.0000\n","Epoch 23: Loss = 1.3044, Acc = 0.0039\n","Epoch 23: Loss = 1.9621, Acc = 0.0039\n","Epoch 23: Loss = 2.6217, Acc = 0.0059\n","Epoch 23: Loss = 3.2718, Acc = 0.0059\n","Epoch 23: Loss = 3.9238, Acc = 0.0078\n","Epoch 23: Loss = 4.5866, Acc = 0.0078\n","Epoch 23: Loss = 5.2516, Acc = 0.0098\n","Epoch 24: Loss = 0.6600, Acc = 0.0020\n","Epoch 24: Loss = 1.3137, Acc = 0.0078\n","Epoch 24: Loss = 1.9693, Acc = 0.0098\n","Epoch 24: Loss = 2.6186, Acc = 0.0117\n","Epoch 24: Loss = 3.2733, Acc = 0.0137\n","Epoch 24: Loss = 3.9351, Acc = 0.0137\n","Epoch 24: Loss = 4.6026, Acc = 0.0137\n","Epoch 24: Loss = 5.2510, Acc = 0.0137\n","Epoch 25: Loss = 0.6645, Acc = 0.0020\n","Epoch 25: Loss = 1.3108, Acc = 0.0020\n","Epoch 25: Loss = 1.9578, Acc = 0.0039\n","Epoch 25: Loss = 2.6285, Acc = 0.0039\n","Epoch 25: Loss = 3.2890, Acc = 0.0078\n","Epoch 25: Loss = 3.9560, Acc = 0.0078\n","Epoch 25: Loss = 4.5965, Acc = 0.0078\n","Epoch 25: Loss = 5.2516, Acc = 0.0098\n","Epoch 26: Loss = 0.6469, Acc = 0.0020\n","Epoch 26: Loss = 1.3017, Acc = 0.0039\n","Epoch 26: Loss = 1.9631, Acc = 0.0039\n","Epoch 26: Loss = 2.6050, Acc = 0.0059\n","Epoch 26: Loss = 3.2533, Acc = 0.0098\n","Epoch 26: Loss = 3.9297, Acc = 0.0098\n","Epoch 26: Loss = 4.5883, Acc = 0.0098\n","Epoch 26: Loss = 5.2510, Acc = 0.0098\n","Epoch 27: Loss = 0.6556, Acc = 0.0039\n","Epoch 27: Loss = 1.3086, Acc = 0.0039\n","Epoch 27: Loss = 1.9633, Acc = 0.0059\n","Epoch 27: Loss = 2.6330, Acc = 0.0078\n","Epoch 27: Loss = 3.2859, Acc = 0.0117\n","Epoch 27: Loss = 3.9396, Acc = 0.0117\n","Epoch 27: Loss = 4.5924, Acc = 0.0117\n","Epoch 27: Loss = 5.2500, Acc = 0.0137\n","Epoch 28: Loss = 0.6577, Acc = 0.0000\n","Epoch 28: Loss = 1.3291, Acc = 0.0020\n","Epoch 28: Loss = 1.9805, Acc = 0.0039\n","Epoch 28: Loss = 2.6371, Acc = 0.0039\n","Epoch 28: Loss = 3.3038, Acc = 0.0039\n","Epoch 28: Loss = 3.9583, Acc = 0.0059\n","Epoch 28: Loss = 4.6071, Acc = 0.0117\n","Epoch 28: Loss = 5.2510, Acc = 0.0117\n","Epoch 29: Loss = 0.6587, Acc = 0.0000\n","Epoch 29: Loss = 1.3178, Acc = 0.0059\n","Epoch 29: Loss = 1.9844, Acc = 0.0059\n","Epoch 29: Loss = 2.6459, Acc = 0.0078\n","Epoch 29: Loss = 3.3010, Acc = 0.0078\n","Epoch 29: Loss = 3.9491, Acc = 0.0137\n","Epoch 29: Loss = 4.5986, Acc = 0.0137\n","Epoch 29: Loss = 5.2502, Acc = 0.0137\n","Epoch 30: Loss = 0.6501, Acc = 0.0039\n","Epoch 30: Loss = 1.3009, Acc = 0.0039\n","Epoch 30: Loss = 1.9544, Acc = 0.0039\n","Epoch 30: Loss = 2.6165, Acc = 0.0078\n","Epoch 30: Loss = 3.2781, Acc = 0.0078\n","Epoch 30: Loss = 3.9437, Acc = 0.0078\n","Epoch 30: Loss = 4.6075, Acc = 0.0078\n","Epoch 30: Loss = 5.2510, Acc = 0.0078\n","Epoch 31: Loss = 0.6707, Acc = 0.0000\n","Epoch 31: Loss = 1.3217, Acc = 0.0000\n","Epoch 31: Loss = 1.9731, Acc = 0.0020\n","Epoch 31: Loss = 2.6263, Acc = 0.0020\n","Epoch 31: Loss = 3.2774, Acc = 0.0020\n","Epoch 31: Loss = 3.9269, Acc = 0.0039\n","Epoch 31: Loss = 4.5843, Acc = 0.0039\n","Epoch 31: Loss = 5.2495, Acc = 0.0059\n","Epoch 32: Loss = 0.6590, Acc = 0.0020\n","Epoch 32: Loss = 1.3148, Acc = 0.0020\n","Epoch 32: Loss = 1.9723, Acc = 0.0039\n","Epoch 32: Loss = 2.6303, Acc = 0.0078\n","Epoch 32: Loss = 3.2826, Acc = 0.0078\n","Epoch 32: Loss = 3.9354, Acc = 0.0137\n","Epoch 32: Loss = 4.5810, Acc = 0.0137\n","Epoch 32: Loss = 5.2497, Acc = 0.0137\n","Epoch 33: Loss = 0.6448, Acc = 0.0000\n","Epoch 33: Loss = 1.2974, Acc = 0.0020\n","Epoch 33: Loss = 1.9531, Acc = 0.0039\n","Epoch 33: Loss = 2.6034, Acc = 0.0059\n","Epoch 33: Loss = 3.2635, Acc = 0.0059\n","Epoch 33: Loss = 3.9153, Acc = 0.0059\n","Epoch 33: Loss = 4.5840, Acc = 0.0078\n","Epoch 33: Loss = 5.2499, Acc = 0.0078\n","Epoch 34: Loss = 0.6570, Acc = 0.0020\n","Epoch 34: Loss = 1.3138, Acc = 0.0039\n","Epoch 34: Loss = 1.9663, Acc = 0.0039\n","Epoch 34: Loss = 2.6229, Acc = 0.0078\n","Epoch 34: Loss = 3.2844, Acc = 0.0078\n","Epoch 34: Loss = 3.9450, Acc = 0.0078\n","Epoch 34: Loss = 4.6042, Acc = 0.0098\n","Epoch 34: Loss = 5.2499, Acc = 0.0137\n","Epoch 35: Loss = 0.6452, Acc = 0.0020\n","Epoch 35: Loss = 1.2952, Acc = 0.0020\n","Epoch 35: Loss = 1.9582, Acc = 0.0020\n","Epoch 35: Loss = 2.6127, Acc = 0.0039\n","Epoch 35: Loss = 3.2738, Acc = 0.0059\n","Epoch 35: Loss = 3.9330, Acc = 0.0059\n","Epoch 35: Loss = 4.5933, Acc = 0.0059\n","Epoch 35: Loss = 5.2485, Acc = 0.0078\n","Epoch 36: Loss = 0.6513, Acc = 0.0039\n","Epoch 36: Loss = 1.2998, Acc = 0.0039\n","Epoch 36: Loss = 1.9610, Acc = 0.0098\n","Epoch 36: Loss = 2.6050, Acc = 0.0117\n","Epoch 36: Loss = 3.2687, Acc = 0.0137\n","Epoch 36: Loss = 3.9268, Acc = 0.0137\n","Epoch 36: Loss = 4.5914, Acc = 0.0137\n","Epoch 36: Loss = 5.2482, Acc = 0.0137\n","Epoch 37: Loss = 0.6866, Acc = 0.0020\n","Epoch 37: Loss = 1.3231, Acc = 0.0039\n","Epoch 37: Loss = 1.9782, Acc = 0.0059\n","Epoch 37: Loss = 2.6298, Acc = 0.0059\n","Epoch 37: Loss = 3.2878, Acc = 0.0078\n","Epoch 37: Loss = 3.9390, Acc = 0.0137\n","Epoch 37: Loss = 4.5937, Acc = 0.0137\n","Epoch 37: Loss = 5.2493, Acc = 0.0137\n","Epoch 38: Loss = 0.6432, Acc = 0.0020\n","Epoch 38: Loss = 1.2973, Acc = 0.0059\n","Epoch 38: Loss = 1.9643, Acc = 0.0059\n","Epoch 38: Loss = 2.6248, Acc = 0.0059\n","Epoch 38: Loss = 3.2665, Acc = 0.0098\n","Epoch 38: Loss = 3.9342, Acc = 0.0098\n","Epoch 38: Loss = 4.5946, Acc = 0.0098\n","Epoch 38: Loss = 5.2487, Acc = 0.0098\n","Epoch 39: Loss = 0.6513, Acc = 0.0000\n","Epoch 39: Loss = 1.3190, Acc = 0.0020\n","Epoch 39: Loss = 1.9673, Acc = 0.0020\n","Epoch 39: Loss = 2.6163, Acc = 0.0020\n","Epoch 39: Loss = 3.2680, Acc = 0.0039\n","Epoch 39: Loss = 3.9256, Acc = 0.0039\n","Epoch 39: Loss = 4.5899, Acc = 0.0039\n","Epoch 39: Loss = 5.2497, Acc = 0.0039\n","Epoch 40: Loss = 0.6548, Acc = 0.0020\n","Epoch 40: Loss = 1.3111, Acc = 0.0020\n","Epoch 40: Loss = 1.9644, Acc = 0.0059\n","Epoch 40: Loss = 2.6408, Acc = 0.0059\n","Epoch 40: Loss = 3.2882, Acc = 0.0059\n","Epoch 40: Loss = 3.9455, Acc = 0.0059\n","Epoch 40: Loss = 4.5940, Acc = 0.0098\n","Epoch 40: Loss = 5.2489, Acc = 0.0117\n","Epoch 41: Loss = 0.6703, Acc = 0.0000\n","Epoch 41: Loss = 1.3206, Acc = 0.0020\n","Epoch 41: Loss = 1.9742, Acc = 0.0078\n","Epoch 41: Loss = 2.6261, Acc = 0.0078\n","Epoch 41: Loss = 3.2791, Acc = 0.0137\n","Epoch 41: Loss = 3.9365, Acc = 0.0137\n","Epoch 41: Loss = 4.5990, Acc = 0.0137\n","Epoch 41: Loss = 5.2491, Acc = 0.0137\n","Epoch 42: Loss = 0.6545, Acc = 0.0000\n","Epoch 42: Loss = 1.3021, Acc = 0.0059\n","Epoch 42: Loss = 1.9514, Acc = 0.0059\n","Epoch 42: Loss = 2.6020, Acc = 0.0059\n","Epoch 42: Loss = 3.2570, Acc = 0.0059\n","Epoch 42: Loss = 3.9270, Acc = 0.0059\n","Epoch 42: Loss = 4.5813, Acc = 0.0078\n","Epoch 42: Loss = 5.2489, Acc = 0.0078\n","Epoch 43: Loss = 0.6463, Acc = 0.0000\n","Epoch 43: Loss = 1.3108, Acc = 0.0078\n","Epoch 43: Loss = 1.9665, Acc = 0.0117\n","Epoch 43: Loss = 2.6171, Acc = 0.0137\n","Epoch 43: Loss = 3.2721, Acc = 0.0137\n","Epoch 43: Loss = 3.9265, Acc = 0.0137\n","Epoch 43: Loss = 4.5881, Acc = 0.0137\n","Epoch 43: Loss = 5.2487, Acc = 0.0137\n","Epoch 44: Loss = 0.6455, Acc = 0.0020\n","Epoch 44: Loss = 1.3022, Acc = 0.0059\n","Epoch 44: Loss = 1.9710, Acc = 0.0078\n","Epoch 44: Loss = 2.6277, Acc = 0.0098\n","Epoch 44: Loss = 3.2799, Acc = 0.0117\n","Epoch 44: Loss = 3.9407, Acc = 0.0117\n","Epoch 44: Loss = 4.5902, Acc = 0.0117\n","Epoch 44: Loss = 5.2483, Acc = 0.0137\n","Epoch 45: Loss = 0.6528, Acc = 0.0000\n","Epoch 45: Loss = 1.2941, Acc = 0.0020\n","Epoch 45: Loss = 1.9434, Acc = 0.0059\n","Epoch 45: Loss = 2.5933, Acc = 0.0059\n","Epoch 45: Loss = 3.2344, Acc = 0.0117\n","Epoch 45: Loss = 3.9011, Acc = 0.0137\n","Epoch 45: Loss = 4.5710, Acc = 0.0137\n","Epoch 45: Loss = 5.2487, Acc = 0.0137\n","Epoch 46: Loss = 0.6573, Acc = 0.0020\n","Epoch 46: Loss = 1.3093, Acc = 0.0020\n","Epoch 46: Loss = 1.9607, Acc = 0.0039\n","Epoch 46: Loss = 2.6143, Acc = 0.0078\n","Epoch 46: Loss = 3.2751, Acc = 0.0098\n","Epoch 46: Loss = 3.9368, Acc = 0.0098\n","Epoch 46: Loss = 4.5933, Acc = 0.0098\n","Epoch 46: Loss = 5.2487, Acc = 0.0098\n","Epoch 47: Loss = 0.6707, Acc = 0.0000\n","Epoch 47: Loss = 1.3332, Acc = 0.0000\n","Epoch 47: Loss = 1.9869, Acc = 0.0020\n","Epoch 47: Loss = 2.6370, Acc = 0.0039\n","Epoch 47: Loss = 3.2933, Acc = 0.0039\n","Epoch 47: Loss = 3.9342, Acc = 0.0078\n","Epoch 47: Loss = 4.5982, Acc = 0.0078\n","Epoch 47: Loss = 5.2481, Acc = 0.0098\n","Epoch 48: Loss = 0.6447, Acc = 0.0020\n","Epoch 48: Loss = 1.3110, Acc = 0.0039\n","Epoch 48: Loss = 1.9703, Acc = 0.0039\n","Epoch 48: Loss = 2.6287, Acc = 0.0059\n","Epoch 48: Loss = 3.2853, Acc = 0.0078\n","Epoch 48: Loss = 3.9440, Acc = 0.0078\n","Epoch 48: Loss = 4.5869, Acc = 0.0098\n","Epoch 48: Loss = 5.2478, Acc = 0.0117\n","Epoch 49: Loss = 0.6507, Acc = 0.0000\n","Epoch 49: Loss = 1.3111, Acc = 0.0039\n","Epoch 49: Loss = 1.9618, Acc = 0.0059\n","Epoch 49: Loss = 2.6220, Acc = 0.0059\n","Epoch 49: Loss = 3.2847, Acc = 0.0078\n","Epoch 49: Loss = 3.9368, Acc = 0.0117\n","Epoch 49: Loss = 4.5885, Acc = 0.0137\n","Epoch 49: Loss = 5.2477, Acc = 0.0137\n","Epoch 50: Loss = 0.6614, Acc = 0.0000\n","Epoch 50: Loss = 1.3093, Acc = 0.0020\n","Epoch 50: Loss = 1.9683, Acc = 0.0020\n","Epoch 50: Loss = 2.6077, Acc = 0.0039\n","Epoch 50: Loss = 3.2654, Acc = 0.0059\n","Epoch 50: Loss = 3.9225, Acc = 0.0059\n","Epoch 50: Loss = 4.5963, Acc = 0.0059\n","Epoch 50: Loss = 5.2479, Acc = 0.0078\n","Epoch 51: Loss = 0.6469, Acc = 0.0000\n","Epoch 51: Loss = 1.2996, Acc = 0.0000\n","Epoch 51: Loss = 1.9426, Acc = 0.0000\n","Epoch 51: Loss = 2.6078, Acc = 0.0020\n","Epoch 51: Loss = 3.2493, Acc = 0.0039\n","Epoch 51: Loss = 3.9153, Acc = 0.0039\n","Epoch 51: Loss = 4.5797, Acc = 0.0059\n","Epoch 51: Loss = 5.2487, Acc = 0.0078\n","Epoch 52: Loss = 0.6528, Acc = 0.0020\n","Epoch 52: Loss = 1.3129, Acc = 0.0020\n","Epoch 52: Loss = 1.9624, Acc = 0.0020\n","Epoch 52: Loss = 2.6180, Acc = 0.0039\n","Epoch 52: Loss = 3.2749, Acc = 0.0039\n","Epoch 52: Loss = 3.9354, Acc = 0.0059\n","Epoch 52: Loss = 4.5806, Acc = 0.0078\n","Epoch 52: Loss = 5.2483, Acc = 0.0078\n","Epoch 53: Loss = 0.6578, Acc = 0.0039\n","Epoch 53: Loss = 1.3024, Acc = 0.0039\n","Epoch 53: Loss = 1.9523, Acc = 0.0039\n","Epoch 53: Loss = 2.5952, Acc = 0.0039\n","Epoch 53: Loss = 3.2535, Acc = 0.0039\n","Epoch 53: Loss = 3.9333, Acc = 0.0039\n","Epoch 53: Loss = 4.6003, Acc = 0.0078\n","Epoch 53: Loss = 5.2480, Acc = 0.0098\n","Epoch 54: Loss = 0.6473, Acc = 0.0000\n","Epoch 54: Loss = 1.3113, Acc = 0.0000\n","Epoch 54: Loss = 1.9714, Acc = 0.0000\n","Epoch 54: Loss = 2.6158, Acc = 0.0000\n","Epoch 54: Loss = 3.2874, Acc = 0.0000\n","Epoch 54: Loss = 3.9390, Acc = 0.0020\n","Epoch 54: Loss = 4.6015, Acc = 0.0020\n","Epoch 54: Loss = 5.2483, Acc = 0.0039\n","Epoch 55: Loss = 0.6465, Acc = 0.0020\n","Epoch 55: Loss = 1.3077, Acc = 0.0020\n","Epoch 55: Loss = 1.9530, Acc = 0.0059\n","Epoch 55: Loss = 2.5978, Acc = 0.0059\n","Epoch 55: Loss = 3.2557, Acc = 0.0078\n","Epoch 55: Loss = 3.9128, Acc = 0.0078\n","Epoch 55: Loss = 4.5757, Acc = 0.0078\n","Epoch 55: Loss = 5.2484, Acc = 0.0098\n","Epoch 56: Loss = 0.6689, Acc = 0.0000\n","Epoch 56: Loss = 1.3154, Acc = 0.0059\n","Epoch 56: Loss = 1.9663, Acc = 0.0098\n","Epoch 56: Loss = 2.6265, Acc = 0.0137\n","Epoch 56: Loss = 3.2895, Acc = 0.0137\n","Epoch 56: Loss = 3.9442, Acc = 0.0137\n","Epoch 56: Loss = 4.5968, Acc = 0.0137\n","Epoch 56: Loss = 5.2482, Acc = 0.0137\n","Epoch 57: Loss = 0.6500, Acc = 0.0020\n","Epoch 57: Loss = 1.3118, Acc = 0.0059\n","Epoch 57: Loss = 1.9750, Acc = 0.0059\n","Epoch 57: Loss = 2.6161, Acc = 0.0078\n","Epoch 57: Loss = 3.2715, Acc = 0.0078\n","Epoch 57: Loss = 3.9292, Acc = 0.0117\n","Epoch 57: Loss = 4.5910, Acc = 0.0137\n","Epoch 57: Loss = 5.2488, Acc = 0.0137\n","Epoch 58: Loss = 0.6645, Acc = 0.0020\n","Epoch 58: Loss = 1.3238, Acc = 0.0039\n","Epoch 58: Loss = 1.9696, Acc = 0.0059\n","Epoch 58: Loss = 2.6167, Acc = 0.0078\n","Epoch 58: Loss = 3.2663, Acc = 0.0078\n","Epoch 58: Loss = 3.9185, Acc = 0.0098\n","Epoch 58: Loss = 4.5788, Acc = 0.0098\n","Epoch 58: Loss = 5.2476, Acc = 0.0117\n","Epoch 59: Loss = 0.6777, Acc = 0.0020\n","Epoch 59: Loss = 1.3209, Acc = 0.0020\n","Epoch 59: Loss = 1.9780, Acc = 0.0020\n","Epoch 59: Loss = 2.6267, Acc = 0.0039\n","Epoch 59: Loss = 3.2763, Acc = 0.0078\n","Epoch 59: Loss = 3.9314, Acc = 0.0078\n","Epoch 59: Loss = 4.5781, Acc = 0.0098\n","Epoch 59: Loss = 5.2477, Acc = 0.0098\n","Epoch 60: Loss = 0.6596, Acc = 0.0000\n","Epoch 60: Loss = 1.3118, Acc = 0.0000\n","Epoch 60: Loss = 1.9695, Acc = 0.0000\n","Epoch 60: Loss = 2.6124, Acc = 0.0039\n","Epoch 60: Loss = 3.2630, Acc = 0.0039\n","Epoch 60: Loss = 3.9239, Acc = 0.0039\n","Epoch 60: Loss = 4.5945, Acc = 0.0059\n","Epoch 60: Loss = 5.2482, Acc = 0.0059\n","Epoch 61: Loss = 0.6488, Acc = 0.0039\n","Epoch 61: Loss = 1.3082, Acc = 0.0059\n","Epoch 61: Loss = 1.9751, Acc = 0.0059\n","Epoch 61: Loss = 2.6279, Acc = 0.0059\n","Epoch 61: Loss = 3.2781, Acc = 0.0078\n","Epoch 61: Loss = 3.9434, Acc = 0.0078\n","Epoch 61: Loss = 4.5986, Acc = 0.0078\n","Epoch 61: Loss = 5.2477, Acc = 0.0078\n","Epoch 62: Loss = 0.6486, Acc = 0.0000\n","Epoch 62: Loss = 1.3086, Acc = 0.0020\n","Epoch 62: Loss = 1.9489, Acc = 0.0039\n","Epoch 62: Loss = 2.6233, Acc = 0.0039\n","Epoch 62: Loss = 3.2777, Acc = 0.0039\n","Epoch 62: Loss = 3.9455, Acc = 0.0039\n","Epoch 62: Loss = 4.5993, Acc = 0.0059\n","Epoch 62: Loss = 5.2483, Acc = 0.0059\n","Epoch 63: Loss = 0.6569, Acc = 0.0020\n","Epoch 63: Loss = 1.3185, Acc = 0.0020\n","Epoch 63: Loss = 1.9744, Acc = 0.0059\n","Epoch 63: Loss = 2.6258, Acc = 0.0078\n","Epoch 63: Loss = 3.2777, Acc = 0.0078\n","Epoch 63: Loss = 3.9219, Acc = 0.0098\n","Epoch 63: Loss = 4.5850, Acc = 0.0098\n","Epoch 63: Loss = 5.2479, Acc = 0.0098\n","Epoch 64: Loss = 0.6488, Acc = 0.0000\n","Epoch 64: Loss = 1.3167, Acc = 0.0020\n","Epoch 64: Loss = 1.9826, Acc = 0.0020\n","Epoch 64: Loss = 2.6382, Acc = 0.0059\n","Epoch 64: Loss = 3.2775, Acc = 0.0059\n","Epoch 64: Loss = 3.9253, Acc = 0.0078\n","Epoch 64: Loss = 4.5782, Acc = 0.0078\n","Epoch 64: Loss = 5.2475, Acc = 0.0117\n","Epoch 65: Loss = 0.6492, Acc = 0.0000\n","Epoch 65: Loss = 1.3003, Acc = 0.0020\n","Epoch 65: Loss = 1.9644, Acc = 0.0039\n","Epoch 65: Loss = 2.6183, Acc = 0.0059\n","Epoch 65: Loss = 3.2747, Acc = 0.0059\n","Epoch 65: Loss = 3.9340, Acc = 0.0059\n","Epoch 65: Loss = 4.5903, Acc = 0.0078\n","Epoch 65: Loss = 5.2477, Acc = 0.0078\n","Epoch 66: Loss = 0.6542, Acc = 0.0039\n","Epoch 66: Loss = 1.3109, Acc = 0.0039\n","Epoch 66: Loss = 1.9555, Acc = 0.0039\n","Epoch 66: Loss = 2.6150, Acc = 0.0059\n","Epoch 66: Loss = 3.2767, Acc = 0.0098\n","Epoch 66: Loss = 3.9339, Acc = 0.0117\n","Epoch 66: Loss = 4.5985, Acc = 0.0137\n","Epoch 66: Loss = 5.2475, Acc = 0.0137\n","Epoch 67: Loss = 0.6548, Acc = 0.0000\n","Epoch 67: Loss = 1.3123, Acc = 0.0020\n","Epoch 67: Loss = 1.9669, Acc = 0.0039\n","Epoch 67: Loss = 2.6299, Acc = 0.0078\n","Epoch 67: Loss = 3.2979, Acc = 0.0078\n","Epoch 67: Loss = 3.9349, Acc = 0.0117\n","Epoch 67: Loss = 4.5959, Acc = 0.0117\n","Epoch 67: Loss = 5.2469, Acc = 0.0117\n","Epoch 68: Loss = 0.6403, Acc = 0.0039\n","Epoch 68: Loss = 1.3101, Acc = 0.0078\n","Epoch 68: Loss = 1.9642, Acc = 0.0098\n","Epoch 68: Loss = 2.6108, Acc = 0.0117\n","Epoch 68: Loss = 3.2668, Acc = 0.0137\n","Epoch 68: Loss = 3.9193, Acc = 0.0137\n","Epoch 68: Loss = 4.5851, Acc = 0.0137\n","Epoch 68: Loss = 5.2484, Acc = 0.0137\n","Epoch 69: Loss = 0.6540, Acc = 0.0000\n","Epoch 69: Loss = 1.3160, Acc = 0.0000\n","Epoch 69: Loss = 1.9640, Acc = 0.0039\n","Epoch 69: Loss = 2.6207, Acc = 0.0039\n","Epoch 69: Loss = 3.2668, Acc = 0.0078\n","Epoch 69: Loss = 3.9235, Acc = 0.0078\n","Epoch 69: Loss = 4.5915, Acc = 0.0078\n","Epoch 69: Loss = 5.2471, Acc = 0.0098\n","Epoch 70: Loss = 0.6517, Acc = 0.0020\n","Epoch 70: Loss = 1.3061, Acc = 0.0020\n","Epoch 70: Loss = 1.9530, Acc = 0.0039\n","Epoch 70: Loss = 2.6157, Acc = 0.0039\n","Epoch 70: Loss = 3.2697, Acc = 0.0039\n","Epoch 70: Loss = 3.9166, Acc = 0.0039\n","Epoch 70: Loss = 4.5850, Acc = 0.0039\n","Epoch 70: Loss = 5.2475, Acc = 0.0059\n","Epoch 71: Loss = 0.6530, Acc = 0.0020\n","Epoch 71: Loss = 1.3058, Acc = 0.0039\n","Epoch 71: Loss = 1.9718, Acc = 0.0059\n","Epoch 71: Loss = 2.6133, Acc = 0.0078\n","Epoch 71: Loss = 3.2640, Acc = 0.0098\n","Epoch 71: Loss = 3.9357, Acc = 0.0117\n","Epoch 71: Loss = 4.5934, Acc = 0.0137\n","Epoch 71: Loss = 5.2474, Acc = 0.0137\n","Epoch 72: Loss = 0.6652, Acc = 0.0000\n","Epoch 72: Loss = 1.3258, Acc = 0.0000\n","Epoch 72: Loss = 1.9761, Acc = 0.0020\n","Epoch 72: Loss = 2.6376, Acc = 0.0020\n","Epoch 72: Loss = 3.2850, Acc = 0.0039\n","Epoch 72: Loss = 3.9468, Acc = 0.0078\n","Epoch 72: Loss = 4.5992, Acc = 0.0098\n","Epoch 72: Loss = 5.2472, Acc = 0.0117\n","Epoch 73: Loss = 0.6465, Acc = 0.0000\n","Epoch 73: Loss = 1.2979, Acc = 0.0000\n","Epoch 73: Loss = 1.9605, Acc = 0.0000\n","Epoch 73: Loss = 2.6176, Acc = 0.0020\n","Epoch 73: Loss = 3.2760, Acc = 0.0020\n","Epoch 73: Loss = 3.9223, Acc = 0.0059\n","Epoch 73: Loss = 4.5804, Acc = 0.0078\n","Epoch 73: Loss = 5.2469, Acc = 0.0098\n","Epoch 74: Loss = 0.6573, Acc = 0.0020\n","Epoch 74: Loss = 1.3117, Acc = 0.0039\n","Epoch 74: Loss = 1.9681, Acc = 0.0039\n","Epoch 74: Loss = 2.6129, Acc = 0.0078\n","Epoch 74: Loss = 3.2664, Acc = 0.0078\n","Epoch 74: Loss = 3.9250, Acc = 0.0098\n","Epoch 74: Loss = 4.5901, Acc = 0.0117\n","Epoch 74: Loss = 5.2473, Acc = 0.0137\n","Epoch 75: Loss = 0.6696, Acc = 0.0020\n","Epoch 75: Loss = 1.3188, Acc = 0.0020\n","Epoch 75: Loss = 1.9767, Acc = 0.0039\n","Epoch 75: Loss = 2.6352, Acc = 0.0059\n","Epoch 75: Loss = 3.2878, Acc = 0.0078\n","Epoch 75: Loss = 3.9425, Acc = 0.0098\n","Epoch 75: Loss = 4.5961, Acc = 0.0117\n","Epoch 75: Loss = 5.2475, Acc = 0.0137\n","Epoch 76: Loss = 0.6595, Acc = 0.0000\n","Epoch 76: Loss = 1.3175, Acc = 0.0000\n","Epoch 76: Loss = 1.9677, Acc = 0.0020\n","Epoch 76: Loss = 2.6214, Acc = 0.0059\n","Epoch 76: Loss = 3.2812, Acc = 0.0078\n","Epoch 76: Loss = 3.9401, Acc = 0.0078\n","Epoch 76: Loss = 4.5931, Acc = 0.0078\n","Epoch 76: Loss = 5.2479, Acc = 0.0098\n","Epoch 77: Loss = 0.6454, Acc = 0.0020\n","Epoch 77: Loss = 1.3135, Acc = 0.0020\n","Epoch 77: Loss = 1.9690, Acc = 0.0020\n","Epoch 77: Loss = 2.6340, Acc = 0.0020\n","Epoch 77: Loss = 3.2782, Acc = 0.0020\n","Epoch 77: Loss = 3.9462, Acc = 0.0078\n","Epoch 77: Loss = 4.5905, Acc = 0.0098\n","Epoch 77: Loss = 5.2473, Acc = 0.0098\n","Epoch 78: Loss = 0.6468, Acc = 0.0000\n","Epoch 78: Loss = 1.2892, Acc = 0.0039\n","Epoch 78: Loss = 1.9380, Acc = 0.0039\n","Epoch 78: Loss = 2.5980, Acc = 0.0078\n","Epoch 78: Loss = 3.2558, Acc = 0.0098\n","Epoch 78: Loss = 3.9169, Acc = 0.0098\n","Epoch 78: Loss = 4.5760, Acc = 0.0137\n","Epoch 78: Loss = 5.2478, Acc = 0.0137\n","Epoch 79: Loss = 0.6550, Acc = 0.0020\n","Epoch 79: Loss = 1.3119, Acc = 0.0039\n","Epoch 79: Loss = 1.9648, Acc = 0.0039\n","Epoch 79: Loss = 2.6223, Acc = 0.0039\n","Epoch 79: Loss = 3.2679, Acc = 0.0039\n","Epoch 79: Loss = 3.9413, Acc = 0.0039\n","Epoch 79: Loss = 4.5970, Acc = 0.0039\n","Epoch 79: Loss = 5.2477, Acc = 0.0059\n","Epoch 80: Loss = 0.6511, Acc = 0.0039\n","Epoch 80: Loss = 1.3130, Acc = 0.0039\n","Epoch 80: Loss = 1.9631, Acc = 0.0059\n","Epoch 80: Loss = 2.6233, Acc = 0.0098\n","Epoch 80: Loss = 3.2742, Acc = 0.0098\n","Epoch 80: Loss = 3.9326, Acc = 0.0117\n","Epoch 80: Loss = 4.5892, Acc = 0.0117\n","Epoch 80: Loss = 5.2468, Acc = 0.0137\n","Epoch 81: Loss = 0.6489, Acc = 0.0039\n","Epoch 81: Loss = 1.3044, Acc = 0.0059\n","Epoch 81: Loss = 1.9569, Acc = 0.0078\n","Epoch 81: Loss = 2.6233, Acc = 0.0078\n","Epoch 81: Loss = 3.2835, Acc = 0.0117\n","Epoch 81: Loss = 3.9284, Acc = 0.0137\n","Epoch 81: Loss = 4.5925, Acc = 0.0137\n","Epoch 81: Loss = 5.2471, Acc = 0.0137\n","Epoch 82: Loss = 0.6636, Acc = 0.0039\n","Epoch 82: Loss = 1.3155, Acc = 0.0039\n","Epoch 82: Loss = 1.9573, Acc = 0.0059\n","Epoch 82: Loss = 2.6130, Acc = 0.0059\n","Epoch 82: Loss = 3.2681, Acc = 0.0059\n","Epoch 82: Loss = 3.9220, Acc = 0.0078\n","Epoch 82: Loss = 4.5735, Acc = 0.0078\n","Epoch 82: Loss = 5.2469, Acc = 0.0117\n","Epoch 83: Loss = 0.6598, Acc = 0.0000\n","Epoch 83: Loss = 1.2993, Acc = 0.0020\n","Epoch 83: Loss = 1.9591, Acc = 0.0039\n","Epoch 83: Loss = 2.5996, Acc = 0.0059\n","Epoch 83: Loss = 3.2529, Acc = 0.0098\n","Epoch 83: Loss = 3.9079, Acc = 0.0098\n","Epoch 83: Loss = 4.5711, Acc = 0.0098\n","Epoch 83: Loss = 5.2475, Acc = 0.0098\n","Epoch 84: Loss = 0.6366, Acc = 0.0020\n","Epoch 84: Loss = 1.2952, Acc = 0.0020\n","Epoch 84: Loss = 1.9542, Acc = 0.0059\n","Epoch 84: Loss = 2.6034, Acc = 0.0078\n","Epoch 84: Loss = 3.2637, Acc = 0.0078\n","Epoch 84: Loss = 3.9218, Acc = 0.0098\n","Epoch 84: Loss = 4.5881, Acc = 0.0098\n","Epoch 84: Loss = 5.2469, Acc = 0.0137\n","Epoch 85: Loss = 0.6408, Acc = 0.0039\n","Epoch 85: Loss = 1.3003, Acc = 0.0039\n","Epoch 85: Loss = 1.9449, Acc = 0.0039\n","Epoch 85: Loss = 2.6027, Acc = 0.0039\n","Epoch 85: Loss = 3.2720, Acc = 0.0039\n","Epoch 85: Loss = 3.9387, Acc = 0.0039\n","Epoch 85: Loss = 4.5945, Acc = 0.0078\n","Epoch 85: Loss = 5.2470, Acc = 0.0098\n","Epoch 86: Loss = 0.6566, Acc = 0.0020\n","Epoch 86: Loss = 1.3194, Acc = 0.0020\n","Epoch 86: Loss = 1.9946, Acc = 0.0039\n","Epoch 86: Loss = 2.6415, Acc = 0.0078\n","Epoch 86: Loss = 3.2920, Acc = 0.0078\n","Epoch 86: Loss = 3.9339, Acc = 0.0078\n","Epoch 86: Loss = 4.5831, Acc = 0.0078\n","Epoch 86: Loss = 5.2471, Acc = 0.0098\n","Epoch 87: Loss = 0.6527, Acc = 0.0020\n","Epoch 87: Loss = 1.3126, Acc = 0.0020\n","Epoch 87: Loss = 1.9726, Acc = 0.0039\n","Epoch 87: Loss = 2.6227, Acc = 0.0039\n","Epoch 87: Loss = 3.2699, Acc = 0.0059\n","Epoch 87: Loss = 3.9341, Acc = 0.0059\n","Epoch 87: Loss = 4.5768, Acc = 0.0098\n","Epoch 87: Loss = 5.2468, Acc = 0.0098\n","Epoch 88: Loss = 0.6590, Acc = 0.0000\n","Epoch 88: Loss = 1.3106, Acc = 0.0000\n","Epoch 88: Loss = 1.9855, Acc = 0.0000\n","Epoch 88: Loss = 2.6381, Acc = 0.0000\n","Epoch 88: Loss = 3.2948, Acc = 0.0039\n","Epoch 88: Loss = 3.9485, Acc = 0.0059\n","Epoch 88: Loss = 4.5900, Acc = 0.0098\n","Epoch 88: Loss = 5.2470, Acc = 0.0098\n","Epoch 89: Loss = 0.6596, Acc = 0.0000\n","Epoch 89: Loss = 1.3120, Acc = 0.0020\n","Epoch 89: Loss = 1.9771, Acc = 0.0039\n","Epoch 89: Loss = 2.6233, Acc = 0.0059\n","Epoch 89: Loss = 3.2763, Acc = 0.0059\n","Epoch 89: Loss = 3.9456, Acc = 0.0078\n","Epoch 89: Loss = 4.5945, Acc = 0.0117\n","Epoch 89: Loss = 5.2467, Acc = 0.0117\n","Epoch 90: Loss = 0.6572, Acc = 0.0000\n","Epoch 90: Loss = 1.3130, Acc = 0.0020\n","Epoch 90: Loss = 1.9577, Acc = 0.0059\n","Epoch 90: Loss = 2.6246, Acc = 0.0078\n","Epoch 90: Loss = 3.2769, Acc = 0.0078\n","Epoch 90: Loss = 3.9290, Acc = 0.0117\n","Epoch 90: Loss = 4.5897, Acc = 0.0117\n","Epoch 90: Loss = 5.2465, Acc = 0.0117\n","Epoch 91: Loss = 0.6570, Acc = 0.0000\n","Epoch 91: Loss = 1.3015, Acc = 0.0020\n","Epoch 91: Loss = 1.9494, Acc = 0.0059\n","Epoch 91: Loss = 2.6122, Acc = 0.0059\n","Epoch 91: Loss = 3.2597, Acc = 0.0059\n","Epoch 91: Loss = 3.9109, Acc = 0.0078\n","Epoch 91: Loss = 4.5835, Acc = 0.0078\n","Epoch 91: Loss = 5.2470, Acc = 0.0098\n","Epoch 92: Loss = 0.6559, Acc = 0.0000\n","Epoch 92: Loss = 1.3159, Acc = 0.0000\n","Epoch 92: Loss = 1.9635, Acc = 0.0020\n","Epoch 92: Loss = 2.6131, Acc = 0.0020\n","Epoch 92: Loss = 3.2630, Acc = 0.0039\n","Epoch 92: Loss = 3.9158, Acc = 0.0059\n","Epoch 92: Loss = 4.5765, Acc = 0.0059\n","Epoch 92: Loss = 5.2470, Acc = 0.0059\n","Epoch 93: Loss = 0.6715, Acc = 0.0000\n","Epoch 93: Loss = 1.3206, Acc = 0.0039\n","Epoch 93: Loss = 1.9752, Acc = 0.0039\n","Epoch 93: Loss = 2.6365, Acc = 0.0059\n","Epoch 93: Loss = 3.2793, Acc = 0.0059\n","Epoch 93: Loss = 3.9314, Acc = 0.0059\n","Epoch 93: Loss = 4.5912, Acc = 0.0059\n","Epoch 93: Loss = 5.2469, Acc = 0.0059\n","Epoch 94: Loss = 0.6618, Acc = 0.0000\n","Epoch 94: Loss = 1.3146, Acc = 0.0000\n","Epoch 94: Loss = 1.9623, Acc = 0.0020\n","Epoch 94: Loss = 2.6093, Acc = 0.0078\n","Epoch 94: Loss = 3.2677, Acc = 0.0098\n","Epoch 94: Loss = 3.9187, Acc = 0.0098\n","Epoch 94: Loss = 4.5827, Acc = 0.0098\n","Epoch 94: Loss = 5.2465, Acc = 0.0098\n","Epoch 95: Loss = 0.6560, Acc = 0.0020\n","Epoch 95: Loss = 1.3175, Acc = 0.0078\n","Epoch 95: Loss = 1.9735, Acc = 0.0078\n","Epoch 95: Loss = 2.6187, Acc = 0.0098\n","Epoch 95: Loss = 3.2762, Acc = 0.0098\n","Epoch 95: Loss = 3.9271, Acc = 0.0098\n","Epoch 95: Loss = 4.5854, Acc = 0.0098\n","Epoch 95: Loss = 5.2468, Acc = 0.0117\n","Epoch 96: Loss = 0.6495, Acc = 0.0000\n","Epoch 96: Loss = 1.3118, Acc = 0.0059\n","Epoch 96: Loss = 1.9725, Acc = 0.0078\n","Epoch 96: Loss = 2.6212, Acc = 0.0098\n","Epoch 96: Loss = 3.2918, Acc = 0.0098\n","Epoch 96: Loss = 3.9335, Acc = 0.0117\n","Epoch 96: Loss = 4.5844, Acc = 0.0117\n","Epoch 96: Loss = 5.2469, Acc = 0.0137\n","Epoch 97: Loss = 0.6465, Acc = 0.0020\n","Epoch 97: Loss = 1.3136, Acc = 0.0020\n","Epoch 97: Loss = 1.9581, Acc = 0.0039\n","Epoch 97: Loss = 2.6073, Acc = 0.0059\n","Epoch 97: Loss = 3.2695, Acc = 0.0117\n","Epoch 97: Loss = 3.9284, Acc = 0.0117\n","Epoch 97: Loss = 4.5996, Acc = 0.0117\n","Epoch 97: Loss = 5.2461, Acc = 0.0137\n","Epoch 98: Loss = 0.6623, Acc = 0.0039\n","Epoch 98: Loss = 1.3063, Acc = 0.0039\n","Epoch 98: Loss = 1.9620, Acc = 0.0059\n","Epoch 98: Loss = 2.6125, Acc = 0.0059\n","Epoch 98: Loss = 3.2728, Acc = 0.0059\n","Epoch 98: Loss = 3.9374, Acc = 0.0059\n","Epoch 98: Loss = 4.5965, Acc = 0.0059\n","Epoch 98: Loss = 5.2468, Acc = 0.0078\n","Epoch 99: Loss = 0.6596, Acc = 0.0000\n","Epoch 99: Loss = 1.3087, Acc = 0.0039\n","Epoch 99: Loss = 1.9714, Acc = 0.0078\n","Epoch 99: Loss = 2.6385, Acc = 0.0098\n","Epoch 99: Loss = 3.2945, Acc = 0.0117\n","Epoch 99: Loss = 3.9547, Acc = 0.0117\n","Epoch 99: Loss = 4.5983, Acc = 0.0117\n","Epoch 99: Loss = 5.2465, Acc = 0.0117\n"]}],"source":["num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","\n","  running_loss = 0\n","  correct = 0\n","\n","  for xb, yb in debug_loader:\n","    xb, yb = xb.to(device), yb.to(device)\n","\n","    optimizer.zero_grad()\n","    output = model(xb)\n","    loss = criterion(output, yb)\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item() * xb.size(0)\n","    correct += (output.argmax(1) == yb).sum().item()\n","\n","    acc = correct / len(debug_loader.dataset)\n","    print(f\"Epoch {epoch}: Loss = {running_loss/len(debug_loader.dataset):.4f}, Acc = {acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjRHmglV7xap"},"outputs":[],"source":["X_rand = torch.rand(200, 1, 700).to(device)\n","y_rand = torch.randint(0, 2, (200,))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiNe5LEVA0XR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zitAXa2L82MB"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","rand_dataset = TensorDataset(small_X, small_y)\n","\n","rand_loader = DataLoader(rand_dataset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PD0hPyQ7AmC3"},"outputs":[],"source":["model = TinyCNN(num_classes=2) #.to(device)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"elapsed":90,"status":"error","timestamp":1747347309979,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"RZqBUx2J8ywk","outputId":"d775f691-7731-4213-c7a4-d9f9881f66d5"},"outputs":[{"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-e6b2599a60be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrand_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# xb, yb = xb.to(device), yb.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","\n","  running_loss = 0\n","  correct = 0\n","\n","  for xb, yb in rand_loader:\n","    # xb, yb = xb.to(device), yb.to(device)\n","\n","    optimizer.zero_grad()\n","    output = model(xb)\n","\n","    print(\"out shape:\", output.shape)   # should be (64, 256)\n","    print(\"yb shape:\", yb.shape)        # should be (64,)\n","    print(\"yb dtype:\", yb.dtype)        # should be torch.int64\n","    print(\"preds:\", output.argmax(1)[:5])\n","    print(\"true :\", yb[:5])\n","    # break\n","\n","    loss = criterion(output, yb)\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item() * xb.size(0)\n","    correct += (output.argmax(1) == yb).sum().item()\n","\n","    acc = correct / len(rand_loader.dataset)\n","    print(f\"Epoch {epoch}: Loss = {running_loss/len(rand_loader.dataset):.4f}, Acc = {acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":365184,"status":"error","timestamp":1747348122120,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"1lB0KkrWBP-P","outputId":"49b7fc93-e368-4b9c-d612-4aa83ac6d3cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 841ms/step - accuracy: 0.4680 - loss: 0.7258\n","Epoch 2/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 572ms/step - accuracy: 0.4835 - loss: 0.7035\n","Epoch 3/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 732ms/step - accuracy: 0.5028 - loss: 0.7129\n","Epoch 4/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 657ms/step - accuracy: 0.5294 - loss: 0.6997\n","Epoch 5/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 665ms/step - accuracy: 0.5284 - loss: 0.6957\n","Epoch 6/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 572ms/step - accuracy: 0.4203 - loss: 0.7067\n","Epoch 7/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 687ms/step - accuracy: 0.5032 - loss: 0.6990\n","Epoch 8/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 643ms/step - accuracy: 0.4563 - loss: 0.7120\n","Epoch 9/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 579ms/step - accuracy: 0.5353 - loss: 0.6902\n","Epoch 10/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 695ms/step - accuracy: 0.5276 - loss: 0.6995\n","Epoch 11/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 677ms/step - accuracy: 0.5358 - loss: 0.6932\n","Epoch 12/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 768ms/step - accuracy: 0.4701 - loss: 0.7003\n","Epoch 13/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 661ms/step - accuracy: 0.5521 - loss: 0.6870\n","Epoch 14/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 878ms/step - accuracy: 0.5265 - loss: 0.6885\n","Epoch 15/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 575ms/step - accuracy: 0.4708 - loss: 0.6939\n","Epoch 16/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 867ms/step - accuracy: 0.5768 - loss: 0.6920\n","Epoch 17/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 594ms/step - accuracy: 0.4808 - loss: 0.7063\n","Epoch 18/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 573ms/step - accuracy: 0.4780 - loss: 0.6991\n","Epoch 19/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 714ms/step - accuracy: 0.5290 - loss: 0.6969\n","Epoch 20/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 720ms/step - accuracy: 0.5249 - loss: 0.6930\n","Epoch 21/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 673ms/step - accuracy: 0.4095 - loss: 0.7013\n","Epoch 22/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 811ms/step - accuracy: 0.5276 - loss: 0.6877\n","Epoch 23/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 578ms/step - accuracy: 0.4963 - loss: 0.6937\n","Epoch 24/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 846ms/step - accuracy: 0.4848 - loss: 0.7029\n","Epoch 25/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 579ms/step - accuracy: 0.5755 - loss: 0.6882\n","Epoch 26/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 888ms/step - accuracy: 0.4680 - loss: 0.7065\n","Epoch 27/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 592ms/step - accuracy: 0.5447 - loss: 0.6939\n","Epoch 28/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 894ms/step - accuracy: 0.5483 - loss: 0.6910\n","Epoch 29/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - accuracy: 0.4789 - loss: 0.6953\n","Epoch 30/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 691ms/step - accuracy: 0.5449 - loss: 0.6897\n","Epoch 31/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 900ms/step - accuracy: 0.5323 - loss: 0.6993\n","Epoch 32/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 599ms/step - accuracy: 0.5243 - loss: 0.6904\n","Epoch 33/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 862ms/step - accuracy: 0.5417 - loss: 0.6904\n","Epoch 34/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 580ms/step - accuracy: 0.5271 - loss: 0.6897\n","Epoch 35/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 588ms/step - accuracy: 0.5350 - loss: 0.6901\n","Epoch 36/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 741ms/step - accuracy: 0.5525 - loss: 0.6923\n","Epoch 37/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 585ms/step - accuracy: 0.5147 - loss: 0.6937\n","Epoch 38/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 717ms/step - accuracy: 0.5269 - loss: 0.6910\n","Epoch 39/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 654ms/step - accuracy: 0.5142 - loss: 0.7001\n","Epoch 40/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 579ms/step - accuracy: 0.5227 - loss: 0.6938\n","Epoch 41/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 699ms/step - accuracy: 0.5625 - loss: 0.6890\n","Epoch 42/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 654ms/step - accuracy: 0.4713 - loss: 0.6944\n","Epoch 43/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 577ms/step - accuracy: 0.4716 - loss: 0.6915\n","Epoch 44/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 720ms/step - accuracy: 0.5303 - loss: 0.7063\n","Epoch 45/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 661ms/step - accuracy: 0.4878 - loss: 0.7038\n","Epoch 46/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 578ms/step - accuracy: 0.5343 - loss: 0.6913\n","Epoch 47/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 809ms/step - accuracy: 0.5142 - loss: 0.6876\n","Epoch 48/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 590ms/step - accuracy: 0.5460 - loss: 0.6842\n","Epoch 49/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 849ms/step - accuracy: 0.5583 - loss: 0.6836\n","Epoch 50/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 585ms/step - accuracy: 0.5432 - loss: 0.6877\n","Epoch 51/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 872ms/step - accuracy: 0.5197 - loss: 0.6846\n","Epoch 52/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 579ms/step - accuracy: 0.5494 - loss: 0.6864\n","Epoch 53/100\n","\u001b[1m4/7\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 944ms/step - accuracy: 0.4857 - loss: 0.6898"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fda461375bec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Step 4: Train on tiny set to overfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    Input, Dense, LayerNormalization, Dropout,\n","    MultiHeadAttention, GlobalAveragePooling1D,\n","    Conv1D, Add\n",")\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","# Step 1: Create synthetic data\n","X = np.random.randn(200, 700, 1).astype(np.float32)  # (200, 700, 1)\n","y = np.random.randint(0, 2, size=(200,))             # binary labels (0 or 1)\n","\n","# Step 2: Build Transformer model\n","def transformer_mini(input_length=700, num_classes=2, d_model=32, num_heads=2, ff_dim=64, dropout=0.0):\n","    inputs = Input(shape=(input_length, 1))\n","\n","    # Project input to d_model\n","    x = Conv1D(d_model, kernel_size=3, padding='same', activation='relu')(inputs)\n","\n","    # Positional encoding (fixed sinusoidal for simplicity)\n","    positions = tf.range(start=0, limit=input_length, delta=1)\n","    pos_enc = tf.keras.layers.Embedding(input_length, d_model)(positions)\n","    pos_enc = tf.expand_dims(pos_enc, 0)  # shape (1, L, D)\n","    x = x + pos_enc  # broadcasting\n","\n","    # Transformer block\n","    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(x, x)\n","    x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n","\n","    ffn_output = Dense(ff_dim, activation='relu')(x)\n","    ffn_output = Dense(d_model)(ffn_output)\n","    x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n","\n","    # Pooling + output\n","    x = GlobalAveragePooling1D()(x)\n","    outputs = Dense(num_classes)(x)\n","\n","    return Model(inputs, outputs)\n","\n","# Step 3: Prepare training\n","model = transformer_mini()\n","model.compile(\n","    optimizer=Adam(1e-3),\n","    loss=SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy']\n",")\n","\n","# Step 4: Train on tiny set to overfit\n","history = model.fit(\n","    X, y,\n","    batch_size=32,\n","    epochs=100,\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1747418090019,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"bwu-O-feEahB","outputId":"cbd9f461-1673-41c7-85f1-d13671059b8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X: (200, 700)\n","Shape of y: (200,)\n","First 5 samples of X:\n"," [[0.94729068 0.83018292 0.94396811 ... 0.71376634 0.14106948 0.05311945]\n"," [0.94706932 0.99742859 0.16382021 ... 0.05274973 0.9926633  0.80645876]\n"," [0.96522497 0.9934532  0.35787745 ... 0.58883575 0.66636795 0.94210572]\n"," [0.14347154 0.70811755 0.40753455 ... 0.07365959 0.7642488  0.94821592]\n"," [0.47506059 0.97611243 0.453808   ... 0.22125511 0.45341534 0.44366963]]\n","First 5 labels of y: [0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0\n"," 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0\n"," 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0\n"," 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n"," 0 1]\n"]},{"output_type":"execute_result","data":{"text/plain":["np.int64(49)"]},"metadata":{},"execution_count":30}],"source":["import numpy as np\n","\n","# Define the number of samples and features\n","num_samples = 200\n","num_features = 700\n","num_classes = 2\n","\n","# Generate random input data (X)\n","X = np.random.rand(num_samples, num_features)\n","\n","# Generate random integer labels (y) for two classes\n","y = np.random.randint(0, num_classes, num_samples)\n","\n","print(\"Shape of X:\", X.shape)\n","print(\"Shape of y:\", y.shape)\n","print(\"First 5 samples of X:\\n\", X[:5])\n","print(\"First 5 labels of y:\", y[:150])\n","\n","np.sum(y[50:150])"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1747409279487,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"1fh3rJiyEkZE"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Attention and Normalization\n","    attn_output = layers.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(inputs, inputs)\n","    attn_output = layers.Dropout(dropout)(attn_output)\n","    out = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n","\n","    # Feed Forward Part\n","    ffn = keras.Sequential([\n","        layers.Dense(ff_dim, activation=\"relu\"),\n","        # The output dimension of the second Dense layer MUST match the input dimension\n","        layers.Dense(inputs.shape[-1])\n","    ])\n","    ffn_output = ffn(out) # Apply the sequential model to the output of the attention block\n","    ffn_output = layers.Dropout(dropout)(ffn_output)\n","\n","    # Add the FFN output back to the skip connection (out)\n","    # Now out and ffn_output have the same shape (None, 700)\n","    return layers.LayerNormalization(epsilon=1e-6)(out + ffn_output)\n","\n","def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = layers.GlobalAveragePooling1D()(x)\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","    outputs = layers.Dense(2, activation=\"softmax\")(x)  # Output layer for two classes\n","    return keras.Model(inputs, outputs)\n","\n","# Model parameters\n","# Ensure X has the correct shape for the transformer (samples, sequence_length, features)\n","# The previous definition of X was (100, 700) which is missing the feature dimension (1)\n","num_samples = 100\n","num_features = 700\n","num_classes = 2\n","\n","# Generate random input data (X) with shape (num_samples, num_features, 1)\n","X = np.random.rand(num_samples, num_features, 1).astype(np.float32)\n","\n","# Generate random integer labels (y) for two classes\n","y = np.random.randint(0, num_classes, num_samples)\n","\n","\n","input_shape = X.shape[1:]  # (700, 1)\n","head_size = 64\n","num_heads = 2\n","ff_dim = 128\n","num_transformer_blocks = 4\n","mlp_units = [128]\n","dropout = 0.1\n","mlp_dropout = 0.1\n","\n","# Build the Transformer model\n","model = build_transformer_model(\n","    input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout, mlp_dropout\n",")\n","\n","# Compile the model\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":879831,"status":"error","timestamp":1747349233105,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"},"user_tz":-120},"id":"VVwehSZ9E-uK","outputId":"fd59366a-6d41-4bdf-8f3f-cc18e36d8048"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5s/step - accuracy: 0.4917 - loss: 0.6931\n","Epoch 2/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.5090 - loss: 0.6931\n","Epoch 3/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5s/step - accuracy: 0.5278 - loss: 0.6929\n","Epoch 4/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.5590 - loss: 0.6925\n","Epoch 5/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.5007 - loss: 0.6932\n","Epoch 6/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.5403 - loss: 0.6925\n","Epoch 7/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5s/step - accuracy: 0.5132 - loss: 0.6929\n","Epoch 8/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5s/step - accuracy: 0.5122 - loss: 0.6929\n","Epoch 9/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5s/step - accuracy: 0.5090 - loss: 0.6930\n","Epoch 10/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.5153 - loss: 0.6928\n","Epoch 11/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.5205 - loss: 0.6927\n","Epoch 12/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.4955 - loss: 0.6934\n","Epoch 13/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.5351 - loss: 0.6923\n","Epoch 14/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.5174 - loss: 0.6928\n","Epoch 15/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.5090 - loss: 0.6930\n","Epoch 16/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.5288 - loss: 0.6925\n","Epoch 17/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.5236 - loss: 0.6926\n","Epoch 18/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5s/step - accuracy: 0.5184 - loss: 0.6927\n","Epoch 19/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5s/step - accuracy: 0.4945 - loss: 0.6934\n","Epoch 20/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.4736 - loss: 0.6940\n","Epoch 21/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5s/step - accuracy: 0.5288 - loss: 0.6924\n","Epoch 22/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.5747 - loss: 0.6908\n","Epoch 23/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.5278 - loss: 0.6923\n","Epoch 24/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4s/step - accuracy: 0.5549 - loss: 0.6911\n","Epoch 25/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.5372 - loss: 0.6918\n","Epoch 26/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5340 - loss: 0.6919\n","Epoch 27/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.4893 - loss: 0.6938\n","Epoch 28/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.5236 - loss: 0.6924\n","Epoch 29/100\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.5340 - loss: 0.6920\n","Epoch 30/100\n","\u001b[1m2/4\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6s/step - accuracy: 0.5469 - loss: 0.6915"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-8ac8bd81ac8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Evaluate the model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train the model\n","epochs = 100  # You can increase this for more overfitting\n","batch_size = 32\n","\n","history = model.fit(X, y, epochs=epochs, batch_size=batch_size)\n","\n","# Evaluate the model on the training data\n","loss, accuracy = model.evaluate(X, y)\n","print(f\"\\nTraining Loss: {loss:.4f}\")\n","print(f\"Training Accuracy: {accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"z6vPPQ0LIWDA"},"source":[]},{"cell_type":"code","source":["def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Attention and Normalization\n","    attn_output = layers.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(inputs, inputs)\n","    attn_output = layers.Dropout(dropout)(attn_output)\n","    out = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n","\n","    # Feed Forward Part\n","    ffn = keras.Sequential([\n","        layers.Dense(ff_dim, activation=\"relu\"),\n","        # The output dimension of the second Dense layer MUST match the input dimension\n","        layers.Dense(inputs.shape[-1])\n","    ])\n","    ffn_output = ffn(out) # Apply the sequential model to the output of the attention block\n","    ffn_output = layers.Dropout(dropout)(ffn_output)\n","\n","    # Add the FFN output back to the skip connection (out)\n","    # Now out and ffn_output have the same shape (None, 700)\n","    return layers.LayerNormalization(epsilon=1e-6)(out + ffn_output)"],"metadata":{"id":"QOYspBD3JdsT","executionInfo":{"status":"ok","timestamp":1747416654980,"user_tz":-120,"elapsed":52,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n","    inputs = keras.Input(shape=input_shape)\n","    # Reshape the input to (batch_size, sequence_length, 1)\n","    reshaped_inputs = layers.Reshape((input_shape[0], 1))(inputs)\n","    x = reshaped_inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = layers.GlobalAveragePooling1D()(x)\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","    outputs = layers.Dense(2, activation=\"softmax\")(x)  # Output layer for two classes\n","    return keras.Model(inputs, outputs)"],"metadata":{"id":"45omdhUxtI4a","executionInfo":{"status":"ok","timestamp":1747416657127,"user_tz":-120,"elapsed":3,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0gN695wIWi6","executionInfo":{"status":"ok","timestamp":1747417739468,"user_tz":-120,"elapsed":30003,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"a68937c6-fb43-4a54-dac8-85500ce5ef80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.4669 - loss: 0.6932\n","Epoch 2/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4287 - loss: 0.6932 \n","Epoch 3/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4724 - loss: 0.6932 \n","Epoch 4/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4863 - loss: 0.6932 \n","Epoch 5/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5191 - loss: 0.6931 \n","Epoch 6/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4442 - loss: 0.6932 \n","Epoch 7/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4685 - loss: 0.6931 \n","Epoch 8/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4848 - loss: 0.6932 \n","Epoch 9/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5351 - loss: 0.6931 \n","Epoch 10/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5029 - loss: 0.6931 \n","Epoch 11/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4324 - loss: 0.6932 \n","Epoch 12/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4722 - loss: 0.6932 \n","Epoch 13/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5305 - loss: 0.6931 \n","Epoch 14/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4957 - loss: 0.6932 \n","Epoch 15/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4064 - loss: 0.6932 \n","Epoch 16/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4742 - loss: 0.6932 \n","Epoch 17/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4545 - loss: 0.6932 \n","Epoch 18/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4817 - loss: 0.6932 \n","Epoch 19/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5111 - loss: 0.6931 \n","Epoch 20/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4900 - loss: 0.6932 \n","Epoch 21/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5110 - loss: 0.6931 \n","Epoch 22/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5373 - loss: 0.6931 \n","Epoch 23/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5156 - loss: 0.6931 \n","Epoch 24/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4990 - loss: 0.6932 \n","Epoch 25/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5320 - loss: 0.6930 \n","Epoch 26/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5202 - loss: 0.6931 \n","Epoch 27/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4949 - loss: 0.6932 \n","Epoch 28/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4816 - loss: 0.6932 \n","Epoch 29/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5126 - loss: 0.6931 \n","Epoch 30/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5254 - loss: 0.6932 \n","Epoch 31/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4746 - loss: 0.6932 \n","Epoch 32/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5132 - loss: 0.6931 \n","Epoch 33/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5062 - loss: 0.6931 \n","Epoch 34/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5137 - loss: 0.6931 \n","Epoch 35/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5339 - loss: 0.6930 \n","Epoch 36/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5268 - loss: 0.6930 \n","Epoch 37/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4790 - loss: 0.6933 \n","Epoch 38/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5177 - loss: 0.6930 \n","Epoch 39/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5229 - loss: 0.6930 \n","Epoch 40/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5209 - loss: 0.6930 \n","Epoch 41/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5165 - loss: 0.6930 \n","Epoch 42/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5159 - loss: 0.6931 \n","Epoch 43/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5128 - loss: 0.6931 \n","Epoch 44/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5024 - loss: 0.6931 \n","Epoch 45/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5025 - loss: 0.6931 \n","Epoch 46/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5135 - loss: 0.6931 \n","Epoch 47/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4999 - loss: 0.6932 \n","Epoch 48/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5145 - loss: 0.6931 \n","Epoch 49/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4563 - loss: 0.6933 \n","Epoch 50/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4861 - loss: 0.6932 \n","Epoch 51/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4370 - loss: 0.6932 \n","Epoch 52/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4938 - loss: 0.6931 \n","Epoch 53/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5144 - loss: 0.6931 \n","Epoch 54/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5041 - loss: 0.6931 \n","Epoch 55/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4611 - loss: 0.6932 \n","Epoch 56/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4940 - loss: 0.6932 \n","Epoch 57/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5146 - loss: 0.6931 \n","Epoch 58/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4809 - loss: 0.6932 \n","Epoch 59/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5256 - loss: 0.6931 \n","Epoch 60/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4509 - loss: 0.6932 \n","Epoch 61/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5042 - loss: 0.6932 \n","Epoch 62/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4783 - loss: 0.6932 \n","Epoch 63/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4753 - loss: 0.6932 \n","Epoch 64/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5252 - loss: 0.6931 \n","Epoch 65/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4908 - loss: 0.6932 \n","Epoch 66/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5070 - loss: 0.6931 \n","Epoch 67/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4988 - loss: 0.6931 \n","Epoch 68/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4612 - loss: 0.6932 \n","Epoch 69/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5422 - loss: 0.6931 \n","Epoch 70/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4788 - loss: 0.6931 \n","Epoch 71/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5106 - loss: 0.6931 \n","Epoch 72/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5082 - loss: 0.6931 \n","Epoch 73/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5015 - loss: 0.6931 \n","Epoch 74/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5021 - loss: 0.6931 \n","Epoch 75/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5219 - loss: 0.6931 \n","Epoch 76/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4905 - loss: 0.6932 \n","Epoch 77/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4704 - loss: 0.6932 \n","Epoch 78/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4794 - loss: 0.6932 \n","Epoch 79/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5142 - loss: 0.6931 \n","Epoch 80/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4880 - loss: 0.6931 \n","Epoch 81/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4941 - loss: 0.6931 \n","Epoch 82/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5165 - loss: 0.6931 \n","Epoch 83/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5092 - loss: 0.6931 \n","Epoch 84/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5060 - loss: 0.6931 \n","Epoch 85/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4712 - loss: 0.6933 \n","Epoch 86/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4936 - loss: 0.6932 \n","Epoch 87/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4862 - loss: 0.6932 \n","Epoch 88/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4456 - loss: 0.6932 \n","Epoch 89/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4788 - loss: 0.6932 \n","Epoch 90/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4771 - loss: 0.6932 \n","Epoch 91/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4906 - loss: 0.6932 \n","Epoch 92/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5015 - loss: 0.6931 \n","Epoch 93/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4872 - loss: 0.6932 \n","Epoch 94/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5023 - loss: 0.6931 \n","Epoch 95/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4991 - loss: 0.6932 \n","Epoch 96/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5161 - loss: 0.6931 \n","Epoch 97/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5270 - loss: 0.6930 \n","Epoch 98/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5032 - loss: 0.6931 \n","Epoch 99/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5155 - loss: 0.6931 \n","Epoch 100/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4730 - loss: 0.6933 \n","Epoch 101/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5027 - loss: 0.6931 \n","Epoch 102/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5314 - loss: 0.6930 \n","Epoch 103/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5253 - loss: 0.6930 \n","Epoch 104/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5232 - loss: 0.6931 \n","Epoch 105/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5181 - loss: 0.6931 \n","Epoch 106/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5473 - loss: 0.6930 \n","Epoch 107/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4970 - loss: 0.6932 \n","Epoch 108/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4836 - loss: 0.6932 \n","Epoch 109/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5355 - loss: 0.6931 \n","Epoch 110/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5268 - loss: 0.6931 \n","Epoch 111/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5334 - loss: 0.6931 \n","Epoch 112/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4906 - loss: 0.6932 \n","Epoch 113/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4997 - loss: 0.6932 \n","Epoch 114/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4486 - loss: 0.6934 \n","Epoch 115/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4971 - loss: 0.6932 \n","Epoch 116/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5307 - loss: 0.6929 \n","Epoch 117/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5025 - loss: 0.6931 \n","Epoch 118/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5190 - loss: 0.6930 \n","Epoch 119/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4840 - loss: 0.6933 \n","Epoch 120/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4704 - loss: 0.6934 \n","Epoch 121/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4969 - loss: 0.6932 \n","Epoch 122/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5221 - loss: 0.6929 \n","Epoch 123/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4847 - loss: 0.6933 \n","Epoch 124/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5219 - loss: 0.6930 \n","Epoch 125/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4822 - loss: 0.6933 \n","Epoch 126/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5100 - loss: 0.6931 \n","Epoch 127/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4782 - loss: 0.6933 \n","Epoch 128/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5286 - loss: 0.6929 \n","Epoch 129/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4902 - loss: 0.6932 \n","Epoch 130/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4822 - loss: 0.6933 \n","Epoch 131/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5194 - loss: 0.6930 \n","Epoch 132/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5343 - loss: 0.6929 \n","Epoch 133/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4981 - loss: 0.6932 \n","Epoch 134/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4913 - loss: 0.6932 \n","Epoch 135/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4954 - loss: 0.6932 \n","Epoch 136/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5027 - loss: 0.6931 \n","Epoch 137/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5199 - loss: 0.6931 \n","Epoch 138/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4913 - loss: 0.6932 \n","Epoch 139/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5115 - loss: 0.6931 \n","Epoch 140/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4776 - loss: 0.6932 \n","Epoch 141/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4712 - loss: 0.6932 \n","Epoch 142/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5207 - loss: 0.6931 \n","Epoch 143/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5012 - loss: 0.6931 \n","Epoch 144/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4998 - loss: 0.6931 \n","Epoch 145/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5008 - loss: 0.6931 \n","Epoch 146/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4821 - loss: 0.6932 \n","Epoch 147/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5303 - loss: 0.6930 \n","Epoch 148/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5055 - loss: 0.6931 \n","Epoch 149/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4755 - loss: 0.6933 \n","Epoch 150/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4984 - loss: 0.6932 \n","Epoch 151/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5009 - loss: 0.6931 \n","Epoch 152/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4964 - loss: 0.6932 \n","Epoch 153/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5083 - loss: 0.6931 \n","Epoch 154/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3986 - loss: 0.6932 \n","Epoch 155/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5193 - loss: 0.6931 \n","Epoch 156/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5285 - loss: 0.6932 \n","Epoch 157/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5180 - loss: 0.6931 \n","Epoch 158/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5030 - loss: 0.6931 \n","Epoch 159/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4894 - loss: 0.6932 \n","Epoch 160/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5147 - loss: 0.6931 \n","Epoch 161/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4975 - loss: 0.6932 \n","Epoch 162/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4939 - loss: 0.6932 \n","Epoch 163/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4788 - loss: 0.6932 \n","Epoch 164/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4547 - loss: 0.6932 \n","Epoch 165/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5142 - loss: 0.6931 \n","Epoch 166/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4909 - loss: 0.6932 \n","Epoch 167/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4765 - loss: 0.6932 \n","Epoch 168/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4898 - loss: 0.6932 \n","Epoch 169/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4883 - loss: 0.6932 \n","Epoch 170/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4727 - loss: 0.6933 \n","Epoch 171/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5038 - loss: 0.6931 \n","Epoch 172/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5551 - loss: 0.6928 \n","Epoch 173/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5322 - loss: 0.6929 \n","Epoch 174/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5189 - loss: 0.6930 \n","Epoch 175/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4867 - loss: 0.6932 \n","Epoch 176/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5150 - loss: 0.6931 \n","Epoch 177/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4790 - loss: 0.6933 \n","Epoch 178/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4517 - loss: 0.6935 \n","Epoch 179/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5249 - loss: 0.6930 \n","Epoch 180/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4896 - loss: 0.6932 \n","Epoch 181/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5357 - loss: 0.6929 \n","Epoch 182/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5234 - loss: 0.6929 \n","Epoch 183/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5041 - loss: 0.6931 \n","Epoch 184/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4762 - loss: 0.6934 \n","Epoch 185/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4969 - loss: 0.6932 \n","Epoch 186/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5057 - loss: 0.6931 \n","Epoch 187/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4932 - loss: 0.6932 \n","Epoch 188/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5083 - loss: 0.6931 \n","Epoch 189/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5029 - loss: 0.6931 \n","Epoch 190/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5002 - loss: 0.6932 \n","Epoch 191/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4883 - loss: 0.6933 \n","Epoch 192/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5365 - loss: 0.6928 \n","Epoch 193/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4946 - loss: 0.6932 \n","Epoch 194/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5057 - loss: 0.6931 \n","Epoch 195/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5226 - loss: 0.6929 \n","Epoch 196/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4655 - loss: 0.6936 \n","Epoch 197/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4809 - loss: 0.6933 \n","Epoch 198/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5393 - loss: 0.6929 \n","Epoch 199/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4692 - loss: 0.6934 \n","Epoch 200/200\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5127 - loss: 0.6931 \n","\n","Training Loss: 0.6932\n","Training Accuracy: 0.5000\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Adjusted model parameters (increased capacity)\n","input_shape = X.shape[1:]  # (700, 1)\n","head_size = 32\n","num_heads = 1\n","ff_dim = 64\n","num_transformer_blocks = 1\n","mlp_units = [64]\n","dropout = 0.1\n","mlp_dropout = 0.1\n","\n","# Re-build the Transformer model with increased capacity\n","model = build_transformer_model(\n","    input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout, mlp_dropout\n",")\n","\n","# Re-compile the model with a potentially different learning rate\n","optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n","model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# Train the model for more epochs\n","epochs = 200\n","batch_size = 32\n","\n","history = model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1) # Set verbose=1 to see epoch-wise progress\n","\n","# Evaluate the model on the training data\n","loss, accuracy = model.evaluate(X, y, verbose=0)\n","print(f\"\\nTraining Loss: {loss:.4f}\")\n","print(f\"Training Accuracy: {accuracy:.4f}\")"]},{"cell_type":"code","source":["def baseline_mlp(input_shape=(700,1)):\n","  from tensorflow.keras.layers import Flatten, Input, Dense\n","  from tensorflow.keras.models import Model\n","  from tensorflow.keras.optimizers import Adam\n","  inputs = Input(shape=input_shape)\n","  x = Flatten()(inputs)\n","  x = Dense(512, activation='relu')(x)\n","  x = Dense(2)(x)\n","  return Model(inputs, x)"],"metadata":{"id":"DukhZILaNs0i","executionInfo":{"status":"ok","timestamp":1747418098561,"user_tz":-120,"elapsed":43,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["model = baseline_mlp()\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","\n","model.fit(X,y, epochs=100, batch_size=32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbm_iO7SOArv","executionInfo":{"status":"ok","timestamp":1747418107051,"user_tz":-120,"elapsed":7569,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"45dcef7f-3e62-43f5-9394-e53914f9807d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.5380 - loss: 1.2610\n","Epoch 2/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5393 - loss: 0.8978 \n","Epoch 3/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5252 - loss: 0.7670 \n","Epoch 4/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5535 - loss: 0.6946 \n","Epoch 5/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5958 - loss: 0.6419 \n","Epoch 6/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7757 - loss: 0.5703 \n","Epoch 7/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7163 - loss: 0.5531 \n","Epoch 8/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8228 - loss: 0.5018 \n","Epoch 9/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7885 - loss: 0.4734 \n","Epoch 10/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9172 - loss: 0.3924 \n","Epoch 11/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6472 - loss: 0.5000 \n","Epoch 12/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9139 - loss: 0.3456 \n","Epoch 13/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9591 - loss: 0.3279 \n","Epoch 14/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.2715 \n","Epoch 15/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9638 - loss: 0.2766 \n","Epoch 16/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9833 - loss: 0.2162 \n","Epoch 17/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9693 - loss: 0.2448 \n","Epoch 18/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9488 - loss: 0.2361 \n","Epoch 19/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.2171 \n","Epoch 20/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1483 \n","Epoch 21/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1391 \n","Epoch 22/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1024 \n","Epoch 23/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1371 \n","Epoch 24/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1044 \n","Epoch 25/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0833 \n","Epoch 26/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9931 - loss: 0.0966 \n","Epoch 27/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0690 \n","Epoch 28/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0616 \n","Epoch 29/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0563 \n","Epoch 30/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0489 \n","Epoch 31/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0406 \n","Epoch 32/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0402 \n","Epoch 33/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0376 \n","Epoch 34/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0326 \n","Epoch 35/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0290 \n","Epoch 36/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0271 \n","Epoch 37/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0264 \n","Epoch 38/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0232 \n","Epoch 39/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0229 \n","Epoch 40/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0208 \n","Epoch 41/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0195 \n","Epoch 42/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0187 \n","Epoch 43/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0169 \n","Epoch 44/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0165 \n","Epoch 45/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0150 \n","Epoch 46/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0144 \n","Epoch 47/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0139 \n","Epoch 48/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0129 \n","Epoch 49/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0124 \n","Epoch 50/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0122 \n","Epoch 51/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0118 \n","Epoch 52/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0105 \n","Epoch 53/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0102 \n","Epoch 54/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0100 \n","Epoch 55/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0096 \n","Epoch 56/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0098 \n","Epoch 57/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0085 \n","Epoch 58/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0087 \n","Epoch 59/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0084 \n","Epoch 60/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0078 \n","Epoch 61/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0077 \n","Epoch 62/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0074 \n","Epoch 63/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0073 \n","Epoch 64/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0069 \n","Epoch 65/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0062 \n","Epoch 66/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0058 \n","Epoch 67/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0058 \n","Epoch 68/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0056 \n","Epoch 69/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0055 \n","Epoch 70/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0053 \n","Epoch 71/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0051 \n","Epoch 72/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0050 \n","Epoch 73/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0049 \n","Epoch 74/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0046 \n","Epoch 75/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0043 \n","Epoch 76/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0043 \n","Epoch 77/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0042 \n","Epoch 78/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0041 \n","Epoch 79/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0038 \n","Epoch 80/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0039 \n","Epoch 81/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0035 \n","Epoch 82/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0036 \n","Epoch 83/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0034 \n","Epoch 84/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0035 \n","Epoch 85/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0033 \n","Epoch 86/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0033 \n","Epoch 87/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0032 \n","Epoch 88/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0031 \n","Epoch 89/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0029 \n","Epoch 90/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0029 \n","Epoch 91/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0028 \n","Epoch 92/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0030 \n","Epoch 93/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0027 \n","Epoch 94/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0027 \n","Epoch 95/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0026 \n","Epoch 96/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0027 \n","Epoch 97/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0025 \n","Epoch 98/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0025 \n","Epoch 99/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0024 \n","Epoch 100/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0023 \n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ba50912c750>"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["class FixedPositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self, length, d_model):\n","        super().__init__()\n","        pos = np.arange(length)[:, None]\n","        i = np.arange(d_model)[None, :]\n","        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","        angle_rads = pos * angle_rates\n","        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n","\n","    def call(self, x):\n","        return x + self.pos_encoding[:, :tf.shape(x)[1], :]"],"metadata":{"id":"Fee_epd5Pl3c","executionInfo":{"status":"ok","timestamp":1747418362637,"user_tz":-120,"elapsed":2,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["def transformer_mlp_baseline(input_length=700, d_model=32):\n","    inputs = tf.keras.layers.Input(shape=(input_length, 1))\n","    x = tf.keras.layers.Conv1D(d_model, kernel_size=5, padding='same', activation='relu')(inputs)\n","\n","    # Positional encoding\n","    x = FixedPositionalEncoding(input_length, d_model)(x)\n","\n","    # Basic Transformer Block\n","    attn = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=d_model//2)(x, x)\n","    x = tf.keras.layers.Add()([x, attn])\n","    x = tf.keras.layers.LayerNormalization()(x)\n","\n","    ffn = tf.keras.layers.Dense(64, activation='relu')(x)\n","    ffn = tf.keras.layers.Dense(d_model)(ffn)\n","    x = tf.keras.layers.Add()([x, ffn])\n","    x = tf.keras.layers.LayerNormalization()(x)\n","\n","    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n","    outputs = tf.keras.layers.Dense(2)(x)\n","\n","    return tf.keras.models.Model(inputs, outputs)"],"metadata":{"id":"C-P0Yt8LPobo","executionInfo":{"status":"ok","timestamp":1747418363950,"user_tz":-120,"elapsed":2,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["model = transformer_mlp_baseline()\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n","model.fit(X, y, epochs=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHdV2MNFPrHW","executionInfo":{"status":"ok","timestamp":1747418416811,"user_tz":-120,"elapsed":29233,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"8f0f413b-6299-42e1-f092-a4ad506164aa"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.5456 - loss: 0.7362\n","Epoch 2/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4968 - loss: 0.7231 \n","Epoch 3/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5038 - loss: 0.7092 \n","Epoch 4/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5168 - loss: 0.6967 \n","Epoch 5/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4126 - loss: 0.7173 \n","Epoch 6/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4887 - loss: 0.7053 \n","Epoch 7/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4878 - loss: 0.6950 \n","Epoch 8/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5246 - loss: 0.7002 \n","Epoch 9/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5288 - loss: 0.6905 \n","Epoch 10/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5004 - loss: 0.7018 \n","Epoch 11/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4679 - loss: 0.6956 \n","Epoch 12/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4915 - loss: 0.6950 \n","Epoch 13/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4780 - loss: 0.6991 \n","Epoch 14/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4671 - loss: 0.6952 \n","Epoch 15/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5278 - loss: 0.6941 \n","Epoch 16/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5350 - loss: 0.6874 \n","Epoch 17/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5192 - loss: 0.7041 \n","Epoch 18/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5268 - loss: 0.6917 \n","Epoch 19/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5207 - loss: 0.7015 \n","Epoch 20/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4791 - loss: 0.7042 \n","Epoch 21/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5470 - loss: 0.6902 \n","Epoch 22/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5020 - loss: 0.7005 \n","Epoch 23/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5088 - loss: 0.7016 \n","Epoch 24/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5054 - loss: 0.6947 \n","Epoch 25/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4811 - loss: 0.6941 \n","Epoch 26/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5005 - loss: 0.6970 \n","Epoch 27/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5230 - loss: 0.6942 \n","Epoch 28/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5028 - loss: 0.6920 \n","Epoch 29/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4602 - loss: 0.7006 \n","Epoch 30/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5053 - loss: 0.6951 \n","Epoch 31/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5101 - loss: 0.6943 \n","Epoch 32/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5140 - loss: 0.6949 \n","Epoch 33/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4858 - loss: 0.6948 \n","Epoch 34/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5065 - loss: 0.6951 \n","Epoch 35/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4945 - loss: 0.7001 \n","Epoch 36/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5023 - loss: 0.6954 \n","Epoch 37/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4750 - loss: 0.7036 \n","Epoch 38/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5032 - loss: 0.6996 \n","Epoch 39/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4176 - loss: 0.6975 \n","Epoch 40/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4776 - loss: 0.7035 \n","Epoch 41/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5185 - loss: 0.6918 \n","Epoch 42/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5237 - loss: 0.6941 \n","Epoch 43/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5395 - loss: 0.6904 \n","Epoch 44/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5040 - loss: 0.6990 \n","Epoch 45/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4648 - loss: 0.7111 \n","Epoch 46/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5046 - loss: 0.6943 \n","Epoch 47/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5228 - loss: 0.6943 \n","Epoch 48/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5289 - loss: 0.6928 \n","Epoch 49/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4881 - loss: 0.6979 \n","Epoch 50/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4840 - loss: 0.7016 \n","Epoch 51/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5276 - loss: 0.6946 \n","Epoch 52/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5056 - loss: 0.6959 \n","Epoch 53/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5209 - loss: 0.6915 \n","Epoch 54/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4571 - loss: 0.6959 \n","Epoch 55/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5567 - loss: 0.6898 \n","Epoch 56/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4946 - loss: 0.6968 \n","Epoch 57/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4715 - loss: 0.6999 \n","Epoch 58/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4804 - loss: 0.6938 \n","Epoch 59/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5121 - loss: 0.6954 \n","Epoch 60/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4914 - loss: 0.6997 \n","Epoch 61/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5427 - loss: 0.6953 \n","Epoch 62/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5605 - loss: 0.6867 \n","Epoch 63/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5010 - loss: 0.6927 \n","Epoch 64/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4808 - loss: 0.6960 \n","Epoch 65/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4839 - loss: 0.6951 \n","Epoch 66/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4974 - loss: 0.6949 \n","Epoch 67/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5221 - loss: 0.6929 \n","Epoch 68/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4905 - loss: 0.6974 \n","Epoch 69/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4699 - loss: 0.7007 \n","Epoch 70/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5103 - loss: 0.6933 \n","Epoch 71/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4922 - loss: 0.6949 \n","Epoch 72/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4882 - loss: 0.6936 \n","Epoch 73/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5129 - loss: 0.6923 \n","Epoch 74/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5050 - loss: 0.6943 \n","Epoch 75/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4599 - loss: 0.6971 \n","Epoch 76/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5133 - loss: 0.6934 \n","Epoch 77/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4880 - loss: 0.6949 \n","Epoch 78/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4579 - loss: 0.6969 \n","Epoch 79/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5075 - loss: 0.6946 \n","Epoch 80/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4538 - loss: 0.6955 \n","Epoch 81/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5127 - loss: 0.6965 \n","Epoch 82/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4740 - loss: 0.6986 \n","Epoch 83/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4807 - loss: 0.6948 \n","Epoch 84/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5171 - loss: 0.6927 \n","Epoch 85/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5342 - loss: 0.6921 \n","Epoch 86/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4397 - loss: 0.6945 \n","Epoch 87/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4769 - loss: 0.6968 \n","Epoch 88/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4719 - loss: 0.6950 \n","Epoch 89/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5255 - loss: 0.6938 \n","Epoch 90/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4606 - loss: 0.6950 \n","Epoch 91/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5039 - loss: 0.6942 \n","Epoch 92/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4919 - loss: 0.6940 \n","Epoch 93/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5401 - loss: 0.6903 \n","Epoch 94/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4877 - loss: 0.6950 \n","Epoch 95/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4760 - loss: 0.6943 \n","Epoch 96/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5194 - loss: 0.6948 \n","Epoch 97/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5189 - loss: 0.6916 \n","Epoch 98/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4672 - loss: 0.6942 \n","Epoch 99/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4824 - loss: 0.6926 \n","Epoch 100/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4704 - loss: 0.6967 \n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ba55d018690>"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Input, Dense, Conv1D, GlobalAveragePooling1D, LayerNormalization, Add, MultiHeadAttention\n","\n","# ==== Fake linearly separable dataset ====\n","X = np.random.randn(200, 700, 1).astype(np.float32)\n","y = np.array([0]*100 + [1]*100)\n","np.random.shuffle(y)\n","\n","# ==== Positional Encoding (sinusoidal) ====\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self, length, d_model):\n","        super().__init__()\n","        pos = np.arange(length)[:, None]\n","        i = np.arange(d_model)[None, :]\n","        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / d_model)\n","        angle_rads = pos * angle_rates\n","        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","        self.encoding = tf.constant(angle_rads[np.newaxis, :, :], dtype=tf.float32)\n","\n","    def call(self, x):\n","        return x + self.encoding[:, :tf.shape(x)[1], :]\n","\n","# ==== Model ====\n","def transformer_overfit(input_len=700, d_model=32):\n","    inputs = Input(shape=(input_len, 1))\n","    x = Conv1D(d_model, kernel_size=3, padding='same', activation='relu')(inputs)\n","\n","    x = PositionalEncoding(input_len, d_model)(x)\n","\n","    attn_out = MultiHeadAttention(num_heads=2, key_dim=d_model // 2)(x, x)\n","    x = Add()([x, attn_out])\n","    x = LayerNormalization()(x)\n","\n","    ffn = Dense(64, activation='relu')(x)\n","    ffn = Dense(d_model)(ffn)\n","    x = Add()([x, ffn])\n","    x = LayerNormalization()(x)\n","\n","    x = GlobalAveragePooling1D()(x)\n","    outputs = Dense(2)(x)  # 2-class output\n","\n","    return Model(inputs, outputs)\n","\n","# ==== Compile and train ====\n","model = transformer_overfit()\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(1e-3),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.fit(X, y, batch_size=32, epochs=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQV7k6llQoxx","executionInfo":{"status":"ok","timestamp":1747418548758,"user_tz":-120,"elapsed":17721,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"5e905a58-8905-421e-9665-6ba592280c72"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 629ms/step - accuracy: 0.5603 - loss: 0.6904\n","Epoch 2/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5199 - loss: 0.7038 \n","Epoch 3/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4559 - loss: 0.7048 \n","Epoch 4/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5014 - loss: 0.6959 \n","Epoch 5/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4587 - loss: 0.7027 \n","Epoch 6/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5295 - loss: 0.6911 \n","Epoch 7/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4873 - loss: 0.6991 \n","Epoch 8/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4993 - loss: 0.6996 \n","Epoch 9/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5281 - loss: 0.6914 \n","Epoch 10/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4790 - loss: 0.6965 \n","Epoch 11/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5089 - loss: 0.6947 \n","Epoch 12/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4821 - loss: 0.6983 \n","Epoch 13/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4884 - loss: 0.6949 \n","Epoch 14/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4882 - loss: 0.6953 \n","Epoch 15/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5036 - loss: 0.7026 \n","Epoch 16/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4877 - loss: 0.7050 \n","Epoch 17/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4838 - loss: 0.6935 \n","Epoch 18/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3823 - loss: 0.6986 \n","Epoch 19/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4693 - loss: 0.6978 \n","Epoch 20/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5034 - loss: 0.6947 \n","Epoch 21/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4818 - loss: 0.6926 \n","Epoch 22/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5143 - loss: 0.6940 \n","Epoch 23/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4876 - loss: 0.6937 \n","Epoch 24/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5308 - loss: 0.6908 \n","Epoch 25/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4982 - loss: 0.6944 \n","Epoch 26/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4880 - loss: 0.6976 \n","Epoch 27/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5189 - loss: 0.6925 \n","Epoch 28/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5246 - loss: 0.6992 \n","Epoch 29/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5128 - loss: 0.6953 \n","Epoch 30/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4914 - loss: 0.6965 \n","Epoch 31/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4951 - loss: 0.7042 \n","Epoch 32/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4967 - loss: 0.6952 \n","Epoch 33/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5011 - loss: 0.6935 \n","Epoch 34/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4901 - loss: 0.6935 \n","Epoch 35/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5139 - loss: 0.6935 \n","Epoch 36/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4733 - loss: 0.6939 \n","Epoch 37/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4986 - loss: 0.6947 \n","Epoch 38/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4962 - loss: 0.6930 \n","Epoch 39/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4967 - loss: 0.6945 \n","Epoch 40/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5264 - loss: 0.6929 \n","Epoch 41/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5048 - loss: 0.6950 \n","Epoch 42/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5163 - loss: 0.6920 \n","Epoch 43/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5311 - loss: 0.6918 \n","Epoch 44/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5266 - loss: 0.6916 \n","Epoch 45/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5143 - loss: 0.6973 \n","Epoch 46/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4821 - loss: 0.7008 \n","Epoch 47/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4941 - loss: 0.6940 \n","Epoch 48/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5627 - loss: 0.6926 \n","Epoch 49/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4720 - loss: 0.7017 \n","Epoch 50/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4827 - loss: 0.6933 \n","Epoch 51/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4643 - loss: 0.6960 \n","Epoch 52/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4749 - loss: 0.6962 \n","Epoch 53/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4995 - loss: 0.6946 \n","Epoch 54/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4912 - loss: 0.6954 \n","Epoch 55/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4677 - loss: 0.6959 \n","Epoch 56/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4804 - loss: 0.6978 \n","Epoch 57/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5059 - loss: 0.6975 \n","Epoch 58/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5363 - loss: 0.6913 \n","Epoch 59/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4791 - loss: 0.6966 \n","Epoch 60/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5071 - loss: 0.6943 \n","Epoch 61/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5400 - loss: 0.6915 \n","Epoch 62/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4795 - loss: 0.6928 \n","Epoch 63/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4930 - loss: 0.6934 \n","Epoch 64/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4461 - loss: 0.6926 \n","Epoch 65/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5023 - loss: 0.6929 \n","Epoch 66/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4606 - loss: 0.6940 \n","Epoch 67/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5176 - loss: 0.6920 \n","Epoch 68/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5382 - loss: 0.6920 \n","Epoch 69/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5520 - loss: 0.6910 \n","Epoch 70/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4909 - loss: 0.6941 \n","Epoch 71/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5135 - loss: 0.6900 \n","Epoch 72/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4958 - loss: 0.6963 \n","Epoch 73/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5109 - loss: 0.6936 \n","Epoch 74/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5085 - loss: 0.6906 \n","Epoch 75/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4732 - loss: 0.6975 \n","Epoch 76/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5256 - loss: 0.6894 \n","Epoch 77/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5160 - loss: 0.6918 \n","Epoch 78/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5399 - loss: 0.6926 \n","Epoch 79/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5248 - loss: 0.6905 \n","Epoch 80/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5134 - loss: 0.6923 \n","Epoch 81/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5021 - loss: 0.6966 \n","Epoch 82/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5578 - loss: 0.6911 \n","Epoch 83/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5484 - loss: 0.6912 \n","Epoch 84/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5036 - loss: 0.6921 \n","Epoch 85/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5892 - loss: 0.6900 \n","Epoch 86/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5505 - loss: 0.6915 \n","Epoch 87/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4725 - loss: 0.6999 \n","Epoch 88/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4223 - loss: 0.7037 \n","Epoch 89/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4887 - loss: 0.6911 \n","Epoch 90/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5351 - loss: 0.6868 \n","Epoch 91/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5458 - loss: 0.6865 \n","Epoch 92/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5187 - loss: 0.6859 \n","Epoch 93/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6304 - loss: 0.6811 \n","Epoch 94/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6234 - loss: 0.6674 \n","Epoch 95/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5514 - loss: 0.6805 \n","Epoch 96/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6551 - loss: 0.6482 \n","Epoch 97/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5896 - loss: 0.6611 \n","Epoch 98/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6692 - loss: 0.6344 \n","Epoch 99/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6414 - loss: 0.6338 \n","Epoch 100/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5604 - loss: 0.6283 \n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ba55ce8ee50>"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Lambda\n","\n","def transformer_debug(input_len=700, d_model=32):\n","    inputs = Input(shape=(input_len, 1))\n","    x = Conv1D(d_model, kernel_size=3, padding='same', activation='relu')(inputs)\n","\n","    # Positional encoding (skip for now)\n","    # x = PositionalEncoding(input_len, d_model)(x)\n","\n","    # REMOVE attention: replace with identity to isolate bug\n","    attn_out = Lambda(lambda z: z)(x)  # identity layer\n","    x = Add()([x, attn_out])\n","    x = LayerNormalization()(x)\n","\n","    ffn = Dense(128, activation='relu')(x)\n","    ffn = Dense(64, activation='relu')(x)\n","    ffn = Dense(d_model)(ffn)\n","    x = Add()([x, ffn])\n","    x = LayerNormalization()(x)\n","\n","    x = GlobalAveragePooling1D()(x)\n","    outputs = Dense(2)(x)\n","    return Model(inputs, outputs)"],"metadata":{"id":"V0sy1CryRAMb","executionInfo":{"status":"ok","timestamp":1747418781635,"user_tz":-120,"elapsed":10,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""],"metadata":{"id":"9wz66L9yRdkO","executionInfo":{"status":"ok","timestamp":1747418752489,"user_tz":-120,"elapsed":4,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# ==== Compile and train ====\n","model = transformer_debug()\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(1e-3),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.fit(X, y, batch_size=32, epochs=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUVkyNQARFEd","executionInfo":{"status":"ok","timestamp":1747418795746,"user_tz":-120,"elapsed":11644,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"c7ca7bd7-ebf6-4837-dcba-2b93aca2ab4b"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318ms/step - accuracy: 0.4675 - loss: 0.7457\n","Epoch 2/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4895 - loss: 0.6986 \n","Epoch 3/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5147 - loss: 0.6934 \n","Epoch 4/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5168 - loss: 0.7058 \n","Epoch 5/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5246 - loss: 0.6938 \n","Epoch 6/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5072 - loss: 0.6964 \n","Epoch 7/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5072 - loss: 0.6989 \n","Epoch 8/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5614 - loss: 0.6945 \n","Epoch 9/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4869 - loss: 0.7003 \n","Epoch 10/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5172 - loss: 0.6943 \n","Epoch 11/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4435 - loss: 0.6957 \n","Epoch 12/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5410 - loss: 0.6923 \n","Epoch 13/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5154 - loss: 0.6950 \n","Epoch 14/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4938 - loss: 0.6932 \n","Epoch 15/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5213 - loss: 0.6924 \n","Epoch 16/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4631 - loss: 0.7028 \n","Epoch 17/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5192 - loss: 0.6959 \n","Epoch 18/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5028 - loss: 0.6940 \n","Epoch 19/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5182 - loss: 0.6957 \n","Epoch 20/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5416 - loss: 0.6953 \n","Epoch 21/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4837 - loss: 0.6996 \n","Epoch 22/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4771 - loss: 0.6960 \n","Epoch 23/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4948 - loss: 0.6976 \n","Epoch 24/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5172 - loss: 0.6919 \n","Epoch 25/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5036 - loss: 0.6938 \n","Epoch 26/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5205 - loss: 0.6915 \n","Epoch 27/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5392 - loss: 0.6892 \n","Epoch 28/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4796 - loss: 0.6947 \n","Epoch 29/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5712 - loss: 0.6909 \n","Epoch 30/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5367 - loss: 0.6921 \n","Epoch 31/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5719 - loss: 0.6903 \n","Epoch 32/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4947 - loss: 0.6965 \n","Epoch 33/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4360 - loss: 0.6979 \n","Epoch 34/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4774 - loss: 0.6980 \n","Epoch 35/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5039 - loss: 0.6920 \n","Epoch 36/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5071 - loss: 0.6929 \n","Epoch 37/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5251 - loss: 0.6918 \n","Epoch 38/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5671 - loss: 0.6876 \n","Epoch 39/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5430 - loss: 0.6903 \n","Epoch 40/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5436 - loss: 0.6898 \n","Epoch 41/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5401 - loss: 0.6899 \n","Epoch 42/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4761 - loss: 0.6962 \n","Epoch 43/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5102 - loss: 0.6945 \n","Epoch 44/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4999 - loss: 0.6932 \n","Epoch 45/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5091 - loss: 0.6927 \n","Epoch 46/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5350 - loss: 0.6855 \n","Epoch 47/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4921 - loss: 0.6927 \n","Epoch 48/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5034 - loss: 0.7000 \n","Epoch 49/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5386 - loss: 0.6909 \n","Epoch 50/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5408 - loss: 0.6897 \n","Epoch 51/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5394 - loss: 0.6927 \n","Epoch 52/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5369 - loss: 0.6889 \n","Epoch 53/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4860 - loss: 0.7022 \n","Epoch 54/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4924 - loss: 0.6949 \n","Epoch 55/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5123 - loss: 0.6944 \n","Epoch 56/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5490 - loss: 0.6903 \n","Epoch 57/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5207 - loss: 0.6875 \n","Epoch 58/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4964 - loss: 0.6929 \n","Epoch 59/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5020 - loss: 0.6913 \n","Epoch 60/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5976 - loss: 0.6898 \n","Epoch 61/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5061 - loss: 0.6914 \n","Epoch 62/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4732 - loss: 0.6928 \n","Epoch 63/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5007 - loss: 0.6948 \n","Epoch 64/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5148 - loss: 0.6871 \n","Epoch 65/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5538 - loss: 0.6940 \n","Epoch 66/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5280 - loss: 0.6898 \n","Epoch 67/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4863 - loss: 0.6932 \n","Epoch 68/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5261 - loss: 0.6915 \n","Epoch 69/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5371 - loss: 0.6914 \n","Epoch 70/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4974 - loss: 0.6943 \n","Epoch 71/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5200 - loss: 0.6898 \n","Epoch 72/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5404 - loss: 0.6902 \n","Epoch 73/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5391 - loss: 0.6869 \n","Epoch 74/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5428 - loss: 0.6881 \n","Epoch 75/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4555 - loss: 0.6924 \n","Epoch 76/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4800 - loss: 0.6973 \n","Epoch 77/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5824 - loss: 0.6880 \n","Epoch 78/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4758 - loss: 0.6952 \n","Epoch 79/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5225 - loss: 0.6934 \n","Epoch 80/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5706 - loss: 0.6888 \n","Epoch 81/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4588 - loss: 0.6975 \n","Epoch 82/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5062 - loss: 0.6952 \n","Epoch 83/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4938 - loss: 0.6946 \n","Epoch 84/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5574 - loss: 0.6875 \n","Epoch 85/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5115 - loss: 0.6893 \n","Epoch 86/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5825 - loss: 0.6888 \n","Epoch 87/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5077 - loss: 0.6891 \n","Epoch 88/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4907 - loss: 0.6898 \n","Epoch 89/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5707 - loss: 0.6889 \n","Epoch 90/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4911 - loss: 0.6940 \n","Epoch 91/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5524 - loss: 0.6895 \n","Epoch 92/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5178 - loss: 0.6899 \n","Epoch 93/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5194 - loss: 0.6918 \n","Epoch 94/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5241 - loss: 0.6860 \n","Epoch 95/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5731 - loss: 0.6882 \n","Epoch 96/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4998 - loss: 0.6938 \n","Epoch 97/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5606 - loss: 0.6876 \n","Epoch 98/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5275 - loss: 0.6860 \n","Epoch 99/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5647 - loss: 0.6894 \n","Epoch 100/100\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5646 - loss: 0.6892 \n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ba55c729290>"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["!pip uninstall -y torch torchvision torchaudio\n","!pip install torch==2.1.0 torchvision torchaudio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cciRmLuhSljk","executionInfo":{"status":"ok","timestamp":1747419164261,"user_tz":-120,"elapsed":128766,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"cadd659e-407d-41fb-c4a8-622885e2d30a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.6.0+cu124\n","Uninstalling torch-2.6.0+cu124:\n","  Successfully uninstalled torch-2.6.0+cu124\n","Found existing installation: torchvision 0.21.0+cu124\n","Uninstalling torchvision-0.21.0+cu124:\n","  Successfully uninstalled torchvision-0.21.0+cu124\n","Found existing installation: torchaudio 2.6.0+cu124\n","Uninstalling torchaudio-2.6.0+cu124:\n","  Successfully uninstalled torchaudio-2.6.0+cu124\n","Collecting torch==2.1.0\n","  Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Collecting torchaudio\n","  Downloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.13.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.1.0 (from torch==2.1.0)\n","  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.5.82)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision\n","  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n","  Downloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n","  Downloading torchvision-0.20.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n","  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n","  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n","  Downloading torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","  Downloading torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.17.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n","  Downloading torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.16.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchaudio\n","  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","INFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n","  Downloading torchaudio-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","  Downloading torchaudio-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading torchaudio-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (5.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2025.4.26)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n","Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchaudio-2.1.0-cp311-cp311-manylinux1_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 torchaudio-2.1.0 torchvision-0.16.0 triton-2.1.0\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class M(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.l = nn.Linear(10, 2)\n","\n","    def forward(self, x):\n","        return self.l(x)\n","\n","m = M()\n","print(list(m.parameters()))  # should show 2 tensors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t534aWDOSwE2","executionInfo":{"status":"ok","timestamp":1747419896032,"user_tz":-120,"elapsed":1579,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"0a4dec85-57e3-4456-fcb6-076ae9ba29a2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n","  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"]},{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[ 0.2451, -0.0769, -0.0372, -0.2523, -0.2028,  0.2316,  0.0084,  0.1611,\n","         -0.3128, -0.0851],\n","        [-0.1445,  0.1622,  0.2241,  0.0041,  0.2379,  0.2644,  0.2776, -0.2513,\n","         -0.0179,  0.2518]], requires_grad=True), Parameter containing:\n","tensor([-0.1439,  0.0338], requires_grad=True)]\n"]}]},{"cell_type":"code","source":["!pip install numpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MMdeXXboWlRa","executionInfo":{"status":"ok","timestamp":1747420083699,"user_tz":-120,"elapsed":2315,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"f03e6782-5583-4057-b94f-d92a57482553"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.22.0)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","\n","# === Device ===\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# === Create linearly separable data ===\n","num_samples = 10000\n","X = np.random.randn(num_samples, 700).astype(np.float32)\n","y = np.array([0]*(num_samples//2) + [1]*(num_samples//2))\n","np.random.shuffle(y)\n","print(X.dtype)\n","\n","X = torch.tensor(X, dtype=torch.float32).unsqueeze(1).to(device)  # shape: (200, 1, 700)\n","y = torch.tensor(y, dtype=torch.long).to(device)\n","\n","dataset = TensorDataset(X, y)\n","loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","# === Positional Encoding ===\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=700):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","# === Transformer-like Model ===\n","class OverfitTransformer(nn.Module):\n","    def __init__(self, input_len=700, d_model=32, num_heads=2):\n","        super().__init__()\n","        self.proj = nn.Conv1d(1, d_model, kernel_size=5, padding=2)\n","        self.pos_enc = PositionalEncoding(d_model, max_len=input_len)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n","        self.pool = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(d_model, 2)\n","\n","    def forward(self, x):  # x: (B, 1, 700)\n","        x = self.proj(x)             # (B, d_model, 700)\n","        x = x.permute(0, 2, 1)       # (B, 700, d_model)\n","        x = self.pos_enc(x)          # (B, 700, d_model)\n","        x = self.transformer(x)      # (B, 700, d_model)\n","        x = x.permute(0, 2, 1)       # (B, d_model, 700)\n","        x = self.pool(x).squeeze(-1) # (B, d_model)\n","        return self.fc(x)            # (B, 2)\n","\n","model = OverfitTransformer().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","# === Training loop ===\n","for epoch in range(100):\n","    model.train()\n","    total_loss, correct = 0, 0\n","\n","    for xb, yb in loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        optimizer.zero_grad()\n","        out = model(xb)\n","        loss = criterion(out, yb)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * xb.size(0)\n","        correct += (out.argmax(1) == yb).sum().item()\n","\n","    acc = correct / len(loader.dataset)\n","    print(f\"Epoch {epoch+1}: Loss={total_loss/len(loader.dataset):.4f}, Acc={acc:.4f}\")\n","    if acc >= 0.99:\n","        print(\"✅ Overfit success.\")\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":592},"id":"irnzFEbASPju","executionInfo":{"status":"error","timestamp":1747421550918,"user_tz":-120,"elapsed":125340,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"2a3f6fe9-eaab-4cf2-fff7-d24106bc62c9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","float32\n","Epoch 1: Loss=0.6975, Acc=0.4997\n","Epoch 2: Loss=0.6941, Acc=0.5028\n","Epoch 3: Loss=0.6943, Acc=0.4976\n","Epoch 4: Loss=0.6941, Acc=0.5004\n","Epoch 5: Loss=0.6940, Acc=0.4955\n","Epoch 6: Loss=0.6936, Acc=0.5033\n","Epoch 7: Loss=0.6936, Acc=0.5024\n","Epoch 8: Loss=0.6936, Acc=0.5050\n","Epoch 9: Loss=0.6936, Acc=0.4960\n","Epoch 10: Loss=0.6935, Acc=0.5004\n","Epoch 11: Loss=0.6932, Acc=0.5077\n","Epoch 12: Loss=0.6935, Acc=0.4972\n","Epoch 13: Loss=0.6934, Acc=0.4970\n","Epoch 14: Loss=0.6933, Acc=0.5014\n","Epoch 15: Loss=0.6934, Acc=0.4973\n","Epoch 16: Loss=0.6933, Acc=0.4978\n","Epoch 17: Loss=0.6933, Acc=0.4980\n","Epoch 18: Loss=0.6933, Acc=0.4930\n","Epoch 19: Loss=0.6934, Acc=0.4981\n","Epoch 20: Loss=0.6933, Acc=0.4993\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-0e539858a845>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","\n","# === Fake linearly separable data ===\n","X = torch.randn(200, 1, 700)\n","y = torch.cat([torch.zeros(100), torch.ones(100)]).long()\n","\n","dataset = TensorDataset(X, y)\n","loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# === Positional Encoding ===\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=700):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div_term)\n","        pe[:, 1::2] = torch.cos(pos * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))  # shape: (1, max_len, d_model)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","# === CNN + Transformer Classifier ===\n","class CNNTransformer(nn.Module):\n","    def __init__(self, input_len=700, d_model=32, num_heads=2):\n","        super().__init__()\n","\n","        # Local CNN feature extractor\n","        self.fe = nn.Sequential(\n","            nn.Conv1d(1, 16, kernel_size=11, padding=5),\n","            nn.ReLU(),\n","            nn.Conv1d(16, d_model, kernel_size=5, padding=2),\n","            nn.ReLU()\n","        )\n","\n","        self.pos_enc = PositionalEncoding(d_model, max_len=input_len)\n","\n","        # One-layer Transformer encoder\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model, nhead=num_heads, batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n","\n","        # Final classifier\n","        self.pool = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(d_model, 2)\n","\n","    def forward(self, x):  # (B, 1, 700)\n","        x = self.fe(x)                 # (B, d_model, 700)\n","        x = x.permute(0, 2, 1)         # (B, 700, d_model)\n","        x = self.pos_enc(x)            # (B, 700, d_model)\n","        x = self.transformer(x)        # (B, 700, d_model)\n","        x = x.permute(0, 2, 1)         # (B, d_model, 700)\n","        x = self.pool(x).squeeze(-1)   # (B, d_model)\n","        return self.fc(x)              # (B, 2)"],"metadata":{"id":"0krjV-iCcNnI","executionInfo":{"status":"ok","timestamp":1747421589103,"user_tz":-120,"elapsed":32,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model = CNNTransformer().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(100):\n","    model.train()\n","    correct = 0\n","    total = 0\n","    for xb, yb in loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","\n","        optimizer.zero_grad()\n","        out = model(xb)\n","        loss = criterion(out, yb)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = out.argmax(1)\n","        correct += (pred == yb).sum().item()\n","        total += yb.size(0)\n","\n","    acc = correct / total\n","    print(f\"Epoch {epoch+1}: Accuracy = {acc:.4f}\")\n","    if acc >= 0.99:\n","        print(\"✅ Overfit success!\")\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PATCawtgcSS3","executionInfo":{"status":"ok","timestamp":1747421607744,"user_tz":-120,"elapsed":12385,"user":{"displayName":"Aleksandar Vracarevic","userId":"13426558860841328181"}},"outputId":"5f41fc5f-76bf-449f-d29f-745e4196d2d0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Accuracy = 0.4600\n","Epoch 2: Accuracy = 0.5200\n","Epoch 3: Accuracy = 0.5000\n","Epoch 4: Accuracy = 0.4300\n","Epoch 5: Accuracy = 0.5000\n","Epoch 6: Accuracy = 0.4800\n","Epoch 7: Accuracy = 0.5000\n","Epoch 8: Accuracy = 0.4600\n","Epoch 9: Accuracy = 0.5000\n","Epoch 10: Accuracy = 0.4700\n","Epoch 11: Accuracy = 0.4600\n","Epoch 12: Accuracy = 0.4400\n","Epoch 13: Accuracy = 0.4800\n","Epoch 14: Accuracy = 0.4400\n","Epoch 15: Accuracy = 0.4950\n","Epoch 16: Accuracy = 0.5000\n","Epoch 17: Accuracy = 0.5000\n","Epoch 18: Accuracy = 0.5000\n","Epoch 19: Accuracy = 0.4800\n","Epoch 20: Accuracy = 0.4850\n","Epoch 21: Accuracy = 0.5000\n","Epoch 22: Accuracy = 0.5300\n","Epoch 23: Accuracy = 0.5000\n","Epoch 24: Accuracy = 0.5000\n","Epoch 25: Accuracy = 0.5000\n","Epoch 26: Accuracy = 0.5300\n","Epoch 27: Accuracy = 0.5000\n","Epoch 28: Accuracy = 0.5000\n","Epoch 29: Accuracy = 0.5050\n","Epoch 30: Accuracy = 0.5000\n","Epoch 31: Accuracy = 0.5000\n","Epoch 32: Accuracy = 0.5100\n","Epoch 33: Accuracy = 0.5000\n","Epoch 34: Accuracy = 0.5000\n","Epoch 35: Accuracy = 0.5000\n","Epoch 36: Accuracy = 0.5000\n","Epoch 37: Accuracy = 0.4800\n","Epoch 38: Accuracy = 0.5000\n","Epoch 39: Accuracy = 0.5000\n","Epoch 40: Accuracy = 0.5000\n","Epoch 41: Accuracy = 0.5000\n","Epoch 42: Accuracy = 0.5000\n","Epoch 43: Accuracy = 0.5000\n","Epoch 44: Accuracy = 0.5000\n","Epoch 45: Accuracy = 0.5000\n","Epoch 46: Accuracy = 0.5000\n","Epoch 47: Accuracy = 0.5000\n","Epoch 48: Accuracy = 0.4850\n","Epoch 49: Accuracy = 0.5000\n","Epoch 50: Accuracy = 0.5000\n","Epoch 51: Accuracy = 0.5000\n","Epoch 52: Accuracy = 0.4850\n","Epoch 53: Accuracy = 0.5000\n","Epoch 54: Accuracy = 0.4900\n","Epoch 55: Accuracy = 0.4900\n","Epoch 56: Accuracy = 0.5000\n","Epoch 57: Accuracy = 0.5000\n","Epoch 58: Accuracy = 0.5000\n","Epoch 59: Accuracy = 0.5000\n","Epoch 60: Accuracy = 0.4700\n","Epoch 61: Accuracy = 0.5000\n","Epoch 62: Accuracy = 0.4600\n","Epoch 63: Accuracy = 0.5000\n","Epoch 64: Accuracy = 0.3950\n","Epoch 65: Accuracy = 0.5000\n","Epoch 66: Accuracy = 0.5300\n","Epoch 67: Accuracy = 0.5000\n","Epoch 68: Accuracy = 0.5000\n","Epoch 69: Accuracy = 0.5400\n","Epoch 70: Accuracy = 0.5000\n","Epoch 71: Accuracy = 0.5000\n","Epoch 72: Accuracy = 0.5000\n","Epoch 73: Accuracy = 0.4450\n","Epoch 74: Accuracy = 0.5050\n","Epoch 75: Accuracy = 0.5000\n","Epoch 76: Accuracy = 0.5400\n","Epoch 77: Accuracy = 0.5000\n","Epoch 78: Accuracy = 0.5000\n","Epoch 79: Accuracy = 0.4300\n","Epoch 80: Accuracy = 0.5000\n","Epoch 81: Accuracy = 0.5050\n","Epoch 82: Accuracy = 0.5000\n","Epoch 83: Accuracy = 0.5000\n","Epoch 84: Accuracy = 0.5350\n","Epoch 85: Accuracy = 0.5250\n","Epoch 86: Accuracy = 0.5000\n","Epoch 87: Accuracy = 0.5200\n","Epoch 88: Accuracy = 0.5000\n","Epoch 89: Accuracy = 0.5350\n","Epoch 90: Accuracy = 0.5000\n","Epoch 91: Accuracy = 0.5000\n","Epoch 92: Accuracy = 0.5250\n","Epoch 93: Accuracy = 0.5000\n","Epoch 94: Accuracy = 0.5000\n","Epoch 95: Accuracy = 0.4900\n","Epoch 96: Accuracy = 0.5350\n","Epoch 97: Accuracy = 0.5100\n","Epoch 98: Accuracy = 0.5000\n","Epoch 99: Accuracy = 0.5050\n","Epoch 100: Accuracy = 0.4950\n"]}]}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1FTxrhimBHspVVst2yrfjI44c-yem-QjR","authorship_tag":"ABX9TyO9QPB/k1Y0sXBVdY2WHDdz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}